@article{10.1007/BF00992696,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  year = {1992},
  month = may,
  journal = {Machine Learning},
  volume = {8},
  number = {3--4},
  pages = {229--256},
  publisher = {Kluwer Academic Publishers},
  address = {USA},
  issn = {0885-6125},
  doi = {10.1007/BF00992696},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  issue_date = {May 1992},
  keywords = {connectionist networks,gradient descent,mathematical analysis,Reinforcement learning},
  file = {/home/jeroen/Zotero/storage/GS6ZMHFU/Williams - 1992 - Simple statistical gradient-following algorithms for connectionist reinforcement learning.pdf}
}

@misc{achiamConstrainedPolicyOptimization2017,
  title = {Constrained {{Policy Optimization}}},
  author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  year = {2017},
  month = may,
  number = {arXiv:1705.10528},
  eprint = {1705.10528},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-12-08},
  abstract = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/LUQLNWJJ/Achiam et al. - 2017 - Constrained Policy Optimization.pdf}
}

@article{AdaptiveSamplingAlgorithm2022,
  title = {An {{Adaptive Sampling Algorithm}} for {{Solving Markov Decision Processes}}},
  year = {2022},
  pages = {15},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/BBJI4IYS/2022 - An Adaptive Sampling Algorithm for Solving Markov .pdf}
}

@article{afsarResolutionJobShopProblem2016,
  title = {Resolution of a {{Job-Shop}} Problem with Transportation Constraints: A Master/Slave Approach},
  shorttitle = {Resolution of a {{Job-Shop}} Problem with Transportation Constraints},
  author = {Afsar, H. M. and Lacomme, P. and Ren, L. and Prodhon, C. and Vigo, D.},
  year = {2016},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {8th {{IFAC Conference}} on {{Manufacturing Modelling}}, {{Management}} and {{Control MIM}} 2016},
  volume = {49},
  number = {12},
  pages = {898--903},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2016.07.889},
  urldate = {2024-11-15},
  abstract = {The aim of this paper is to solve the job-shop scheduling problems with transportation constraints in a flexible manufacturing system. This variant is a generalization of the job-shop where the jobs have to be transported between the machines by a fleet of homogenous vehicles with unit capacity. We propose a framework based on an alternate resolution of the scheduling problem (master) modeled on a disjunctive graph by introducing a time lag (i.e., a delay between operations), and of the routing problem (slave). The objective of this study is to validate the approach as a promising method in solving such type of problems. Computational results are presented for the job-shop Laurence's instances and two set of instances dedicated to the job-shop with transportation constraints form the literature. The results show that our approach cannot in general compete with dedicated methods but provides anyway good quality results, and could be easily adapted to more general variants with capacitated vehicles or when jobs need to be transported between distant machines/sites.},
  keywords = {job-shop,transport},
  file = {/home/jeroen/Zotero/storage/F648CAB8/Afsar et al. - 2016 - Resolution of a Job-Shop problem with transportation constraints a masterslave approach.pdf;/home/jeroen/Zotero/storage/6ELXT8Z4/S2405896316311752.html}
}

@article{agandEcoLightRewardShaping2021,
  title = {{{EcoLight}}: {{Reward Shaping}} in {{Deep Reinforcement Learning}} for {{Ergonomic Traffic Signal Control}}},
  author = {Agand, Pedram and Iskrov, Alexey},
  year = {2021},
  abstract = {Mobility, the environment, and human health are all harmed by sub-optimal control policies in transportation systems. Intersection traffic signal controllers are a crucial part of today's transportation infrastructure, as sub-optimal policies may lead to traffic jams and as a result increased levels of air pollution and wasted time. Many adaptive traffic signal controllers have been proposed in the literature, but research on their relative performance differences is limited. On the other hand, to the best of our knowledge there has been no work that directly targets CO2 emission reduction, even though pollution is currently a critical issue. In this paper, we propose a reward shaping scheme for various RL algorithms that not only produces lowers CO2 emissions, but also produces respectable outcomes in terms of other metrics such as travel time. We compare multiple RL algorithms --- sarsa, and A2C --- as well as diverse scenarios with a mix of different road users emitting varied amounts of pollution.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/JY2AYWIY/Agand and Iskrov - EcoLight Reward Shaping in Deep Reinforcement Lea.pdf}
}

@misc{agarwalProvableBenefitsRepresentational2022,
  title = {Provable {{Benefits}} of {{Representational Transfer}} in {{Reinforcement Learning}}},
  author = {Agarwal, Alekh and Song, Yuda and Sun, Wen and Wang, Kaiwen and Wang, Mengdi and Zhang, Xuezhou},
  year = {2022},
  month = may,
  number = {arXiv:2205.14571},
  eprint = {2205.14571},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-12-08},
  abstract = {We study the problem of representational transfer in RL, where an agent first pretrains in a number of source tasks to discover a shared representation, which is subsequently used to learn a good policy in a target task. We propose a new notion of task relatedness between source and target tasks, and develop a novel approach for representational transfer under this assumption. Concretely, we show that given a generative access to source tasks, we can discover a representation, using which subsequent linear RL techniques quickly converge to a near-optimal policy, with only online access to the target task. The sample complexity is close to knowing the ground truth features in the target task, and comparable to prior representation learning results in the source tasks. We complement our positive results with lower bounds without generative access, and validate our findings with empirical evaluation on rich observation MDPs that require deep exploration.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/5GURUTLZ/Agarwal et al. - 2022 - Provable Benefits of Representational Transfer in .pdf}
}

@misc{agarwalReincarnatingReinforcementLearning2022,
  title = {Reincarnating {{Reinforcement Learning}}: {{Reusing Prior Computation}} to {{Accelerate Progress}}},
  shorttitle = {Reincarnating {{Reinforcement Learning}}},
  author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron and Bellemare, Marc G.},
  year = {2022},
  month = oct,
  number = {arXiv:2206.01626},
  eprint = {2206.01626},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-02-22},
  abstract = {Learning tabula rasa, that is without any prior knowledge, is the prevalent workflow in reinforcement learning (RL) research. However, RL systems, when applied to large-scale settings, rarely operate tabula rasa. Such large-scale systems undergo multiple design or algorithmic changes during their development cycle and use ad hoc approaches for incorporating these changes without re-training from scratch, which would have been prohibitively expensive. Additionally, the inefficiency of deep RL typically excludes researchers without access to industrial-scale resources from tackling computationally-demanding problems. To address these issues, we present reincarnating RL as an alternative workflow or class of problem settings, where prior computational work (e.g., learned policies) is reused or transferred between design iterations of an RL agent, or from one RL agent to another. As a step towards enabling reincarnating RL from any agent to any other agent, we focus on the specific setting of efficiently transferring an existing sub-optimal policy to a standalone value-based RL agent. We find that existing approaches fail in this setting and propose a simple algorithm to address their limitations. Equipped with this algorithm, we demonstrate reincarnating RL's gains over tabula rasa RL on Atari 2600 games, a challenging locomotion task, and the real-world problem of navigating stratospheric balloons. Overall, this work argues for an alternative approach to RL research, which we believe could significantly improve real-world RL adoption and help democratize it further. Open-sourced code and trained agents at https://agarwl.github.io/reincarnating\_rl.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/24AZTNK7/Agarwal et al. - 2022 - Reincarnating Reinforcement Learning Reusing Prio.pdf;/home/jeroen/Zotero/storage/LCR7RCRH/2206.html}
}

@article{agrachevIntroductionOptimalControl,
  title = {Introduction to {{Optimal Control Theory}}},
  author = {Agrachev, Andrei A},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/GNLTMSIT/Agrachev - Introduction to Optimal Control Theory.pdf}
}

@article{agrachevIntroductionOptimalControla,
  title = {Introduction to {{Optimal Control Theory}}},
  author = {Agrachev, Andrei A},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/VXVBBFKE/Agrachev - Introduction to Optimal Control Theory.pdf}
}

@article{agrachevIntroductionOptimalControlb,
  title = {Introduction to {{Optimal Control Theory}}},
  author = {Agrachev, Andrei A},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/ZR8ZIDDX/Agrachev - Introduction to Optimal Control Theory.pdf}
}

@incollection{agrawalDifferentiableConvexOptimization2019,
  title = {Differentiable Convex Optimization Layers},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, J. Zico},
  year = {2019},
  month = dec,
  number = {858},
  pages = {9562--9574},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-11-13},
  abstract = {Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver's solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.},
  file = {/home/jeroen/Zotero/storage/3FVNUACI/Agrawal et al. - 2019 - Differentiable convex optimization layers.pdf}
}

@article{alegreQuantifyingImpactNonstationarity2021,
  title = {Quantifying the Impact of Non-Stationarity in Reinforcement Learning-Based Traffic Signal Control},
  author = {Alegre, Lucas N. and Bazzan, Ana L.C. and {da Silva}, Bruno C.},
  year = {2021},
  month = may,
  journal = {PeerJ Computer Science},
  volume = {7},
  pages = {e575},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.575},
  urldate = {2022-12-21},
  abstract = {In reinforcement learning (RL), dealing with non-stationarity is a challenging issue. However, some domains such as traffic optimization are inherently non-stationary. Causes for and effects of this are manifold. In particular, when dealing with traffic signal controls, addressing non-stationarity is key since traffic conditions change over time and as a function of traffic control decisions taken in other parts of a network. In this paper we analyze the effects that different sources of non-stationarity have in a network of traffic signals, in which each signal is modeled as a learning agent. More precisely, we study both the effects of changing the context in which an agent learns (e.g., a change in flow rates experienced by it), as well as the effects of reducing agent observability of the true environment state. Partial observability may cause distinct states (in which distinct actions are optimal) to be seen as the same by the traffic signal agents. This, in turn, may lead to sub-optimal performance. We show that the lack of suitable sensors to provide a representative observation of the real state seems to affect the performance more drastically than the changes to the underlying traffic patterns.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/JB7J3AXL/Alegre et al. - 2021 - Quantifying the impact of non-stationarity in rein.pdf}
}

@book{altmanConstrainedMarkovDecision2021,
  title = {Constrained {{Markov Decision Processes}}: {{Stochastic Modeling}}},
  shorttitle = {Constrained {{Markov Decision Processes}}},
  author = {Altman, Eitan},
  year = {2021},
  month = dec,
  edition = {1},
  publisher = {Routledge},
  address = {Boca Raton},
  doi = {10.1201/9781315140223},
  urldate = {2022-12-13},
  isbn = {978-1-315-14022-3},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/5EM7MRDE/Altman - 2021 - Constrained Markov Decision Processes Stochastic .pdf}
}

@misc{amosOptNetDifferentiableOptimization2021a,
  title = {{{OptNet}}: {{Differentiable Optimization}} as a {{Layer}} in {{Neural Networks}}},
  shorttitle = {{{OptNet}}},
  author = {Amos, Brandon and Kolter, J. Zico},
  year = {2021},
  month = dec,
  number = {arXiv:1703.00443},
  eprint = {1703.00443},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.00443},
  urldate = {2025-04-09},
  abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. We explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, the method is learns to play mini-Sudoku (4x4) given just input and output games, with no a-priori information about the rules of the game; this highlights the ability of OptNet to learn hard constraints better than other neural architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/SW8GPMF3/Amos and Kolter - 2021 - OptNet Differentiable Optimization as a Layer in Neural Networks.pdf;/home/jeroen/Zotero/storage/FUSI8I48/1703.html}
}

@misc{andrychowiczHindsightExperienceReplay2018,
  title = {Hindsight {{Experience Replay}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  year = {2018},
  month = feb,
  number = {arXiv:1707.01495},
  eprint = {1707.01495},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-22},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/home/jeroen/Zotero/storage/84Y6JMZI/Andrychowicz et al. - 2018 - Hindsight Experience Replay.pdf;/home/jeroen/Zotero/storage/2Y8LA4QK/1707.html}
}

@article{antesInformationUpwardsRecommendation2022,
  title = {Information Upwards, Recommendation Downwards: Reinforcement Learning with Hierarchy for Traffic Signal Control},
  shorttitle = {Information Upwards, Recommendation Downwards},
  author = {Antes, Taylor de O. and Bazzan, Ana L.C. and Tavares, Anderson Rocha},
  year = {2022},
  journal = {Procedia Computer Science},
  volume = {201},
  pages = {24--31},
  issn = {18770509},
  doi = {10.1016/j.procs.2022.03.006},
  urldate = {2022-12-21},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/AN5WPLVM/Antes et al. - 2022 - Information upwards, recommendation downwards rei.pdf}
}

@article{antesInformationUpwardsRecommendation2022a,
  title = {Information Upwards, Recommendation Downwards: Reinforcement Learning with Hierarchy for Traffic Signal Control},
  shorttitle = {Information Upwards, Recommendation Downwards},
  author = {Antes, Taylor de O. and Bazzan, Ana L. C. and Tavares, Anderson Rocha},
  year = {2022},
  month = jan,
  journal = {Procedia Computer Science},
  series = {The 13th {{International Conference}} on {{Ambient Systems}}, {{Networks}} and {{Technologies}} ({{ANT}}) / {{The}} 5th {{International Conference}} on {{Emerging Data}} and {{Industry}} 4.0 ({{EDI40}})},
  volume = {201},
  pages = {24--31},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2022.03.006},
  urldate = {2023-09-26},
  abstract = {Traffic signal control (TSC) is a practical solution to the major problem of congestion in metropolitan areas. Reinforcement Learning (RL) techniques present powerful frameworks for optimizing traffic signal controllers that learn to respond to real-time traffic changes. Multiagent RL (MARL) techniques have been showing better results over centralized techniques (RL-based or not), where local intersection agents have partial observation of and control over the environment. Since in TSC the best decision does not depend only on local information, in the present paper we aim at increasing agents' views by using a hierarchical approach, where information is passed upwards, is then aggregated forming recommendations that are sent downwards. We divide the transportation network into regions, each controlled by a region agent; this is done at different hierarchical levels. The traffic signal controllers, located at the intersections, are the local agents at the hierarchy's bottom. Region agents can supervise intersection agents or other region agents. Evaluation of this approach in a synthetic traffic grid shows that the proposed hierarchical organization outperforms a fixed-time approach and an RL-based approach without hierarchy.},
  keywords = {Intelligent Transportation Systems,Multiagent Systems,Reinforcement Learning,Smart Cities},
  file = {/home/jeroen/Zotero/storage/T5W7ATZD/Antes et al. - 2022 - Information upwards, recommendation downwards rei.pdf;/home/jeroen/Zotero/storage/6IRNMEHS/S1877050922004185.html}
}

@inproceedings{ashtianiMultiIntersectionTrafficManagement2018,
  title = {Multi-{{Intersection Traffic Management}} for {{Autonomous Vehicles}} via {{Distributed Mixed Integer Linear Programming}}},
  booktitle = {2018 {{Annual American Control Conference}} ({{ACC}})},
  author = {Ashtiani, Faraz and Fayazi, S. Alireza and Vahidi, Ardalan},
  year = {2018},
  month = jun,
  eprint = {2007.06639},
  primaryclass = {cs, eess, math},
  pages = {6341--6346},
  doi = {10.23919/ACC.2018.8431656},
  urldate = {2023-09-14},
  abstract = {This paper extends our previous work in [1],[2], on optimal scheduling of autonomous vehicle arrivals at intersections, from one to a grid of intersections. A scalable distributed Mixed Integer Linear Program (MILP) is devised that solves the scheduling problem for a grid of intersections. A computational control node is allocated to each intersection and regularly receives position and velocity information from subscribed vehicles. Each node assigns an intersection access time to every subscribed vehicle by solving a local MILP. Neighboring intersections will coordinate with each other in real-time by sharing their solutions for vehicles' access times with each other. Our proposed approach is applied to a grid of nine intersections and its positive impact on traffic flow and vehicles' fuel economy is demonstrated in comparison to conventional intersection control scenarios.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,MILP},
  file = {/home/jeroen/Zotero/storage/YGWRNS8U/Ashtiani et al. - 2018 - Multi-Intersection Traffic Management for Autonomo.pdf;/home/jeroen/Zotero/storage/3QDH5MYN/2007.html}
}

@article{aultReinforcementLearningBenchmarks,
  title = {Reinforcement {{Learning Benchmarks}} for {{Traffic Signal Control}}},
  author = {Ault, James and Sharon, Guni},
  abstract = {We propose a toolkit for developing and comparing reinforcement learning (RL)based traffic signal controllers. The toolkit includes implementation of state-of-theart deep-RL algorithms for signal control along with benchmark control problems that are based on realistic traffic scenarios. Importantly, the toolkit allows a firstof-its-kind comparison between state-of-the-art RL-based signal controllers while providing benchmarks for future comparisons. Consequently, we compare and report the relative performance of current RL algorithms. The experimental results suggest that previous algorithms are not robust to varying sensing assumptions and non-stylized intersection layouts. When more realistic signal layouts and advanced sensing capabilities are considered, a distributed deep Q-learning approach is shown to outperform previously reported state-of-the-art algorithms in many cases.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/XQ6XB54M/Ault and Sharon - Reinforcement Learning Benchmarks for Traffic Sign.pdf}
}

@article{auMotionPlanningAlgorithms,
  title = {Motion {{Planning Algorithms}} for {{Autonomous Intersection Management}}},
  author = {Au, Tsz-Chiu and Stone, Peter},
  abstract = {The impressive results of the 2007 DARPA Urban Challenge showed that fully autonomous vehicles are technologically feasible with current intelligent vehicle hardware. It is natural to ask how current transportation infrastructure can be improved when most vehicles are driven autonomously in the future. Dresner and Stone proposed a new intersection control mechanism called Autonomous Intersection Management (AIM) and showed in simulation that intersection control can be made more efficient than the traditional control mechanisms such as traffic signals and stop signs. In this paper, we extend the study by examining the relationship between the precision of cars' motion controllers and the efficiency of the intersection controller. We propose a planning-based motion controller that can reduce the chance that autonomous vehicles stop before intersections, and show that this controller can increase the efficiency of the intersection control mechanism.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/4G9JAJ2T/Au and Stone - Motion Planning Algorithms for Autonomous Intersection Management.pdf}
}

@article{avramOptimalControlTwoqueue2006a,
  title = {On the Optimal Control of a Two-Queue Polling Model},
  author = {Avram, F. and {G{\'o}mez-Corral}, A.},
  year = {2006},
  month = may,
  journal = {Operations Research Letters},
  volume = {34},
  number = {3},
  pages = {339--348},
  issn = {0167-6377},
  doi = {10.1016/j.orl.2005.05.005},
  urldate = {2024-02-28},
  abstract = {This paper deals with two M/M/1 queues served by a single server with threshold switching. Our main goal is to solve the Poisson equation and, as a result, give expressions for the long-run expected average cost of holding units and switching actions of the server, and the bias vector.},
  keywords = {Poisson equation,Polling system,Priority queue,Threshold policy},
  file = {/home/jeroen/Zotero/storage/HGEJXUS3/Avram and Gómez-Corral - 2006 - On the optimal control of a two-queue polling mode.pdf;/home/jeroen/Zotero/storage/CSZ4FQHP/S0167637705000581.html}
}

@misc{baiAnalyticsMachineLearning2021,
  title = {Analytics and {{Machine Learning}} in {{Vehicle Routing Research}}},
  author = {Bai, Ruibin and Chen, Xinan and Chen, Zhi-Long and Cui, Tianxiang and Gong, Shuhui and He, Wentao and Jiang, Xiaoping and Jin, Huan and Jin, Jiahuan and Kendall, Graham and Li, Jiawei and Lu, Zheng and Ren, Jianfeng and Weng, Paul and Xue, Ning and Zhang, Huayan},
  year = {2021},
  month = feb,
  number = {arXiv:2102.10012},
  eprint = {2102.10012},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.10012},
  urldate = {2024-11-09},
  abstract = {The Vehicle Routing Problem (VRP) is one of the most intensively studied combinatorial optimisation problems for which numerous models and algorithms have been proposed. To tackle the complexities, uncertainties and dynamics involved in real-world VRP applications, Machine Learning (ML) methods have been used in combination with analytical approaches to enhance problem formulations and algorithmic performance across different problem solving scenarios. However, the relevant papers are scattered in several traditional research fields with very different, sometimes confusing, terminologies. This paper presents a first, comprehensive review of hybrid methods that combine analytical techniques with ML tools in addressing VRP problems. Specifically, we review the emerging research streams on ML-assisted VRP modelling and ML-assisted VRP optimisation. We conclude that ML can be beneficial in enhancing VRP modelling, and improving the performance of algorithms for both online and offline VRP optimisations. Finally, challenges and future opportunities of VRP research are discussed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/jeroen/Zotero/storage/IDQAKHG9/Bai et al. - 2021 - Analytics and Machine Learning in Vehicle Routing Research.pdf;/home/jeroen/Zotero/storage/NGX9APIM/2102.html}
}

@article{baileyQueueingProcessesBulk1954,
  title = {On {{Queueing Processes}} with {{Bulk Service}}},
  author = {Bailey, Norman T. J.},
  year = {1954},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {16},
  number = {1},
  pages = {80--87},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1954.tb00149.x},
  urldate = {2023-01-10},
  langid = {english}
}

@article{balasOneMachineProblemDelayed1995,
  title = {The {{One-Machine Problem}} with {{Delayed Precedence Constraints}} and Its {{Use}} in {{Job Shop Scheduling}}},
  author = {Balas, Egon and Lenstra, Jan Karel and Vazacopoulos, Alkis},
  year = {1995},
  month = jan,
  journal = {Management Science},
  publisher = {INFORMS},
  doi = {10.1287/mnsc.41.1.94},
  urldate = {2023-10-20},
  abstract = {We study the one machine scheduling problem with release and delivery times and the minimum makespan objective, in the presence of constraints that for certain pairs of jobs require a delay between...},
  copyright = {{\copyright} 1995 INFORMS},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/IQSCZJN8/mnsc.41.1.html}
}

@article{baykal-guSEMIMARKOVDECISIONPROCESSES,
  title = {{{SEMI-MARKOV DECISION PROCESSES}}},
  author = {{Baykal-Gu}, M},
  abstract = {Considered are infinite horizon semi-Markov decision processes (SMDPs) with finite state and action spaces. Total expected discounted reward and long-run average expected reward optimality criteria are reviewed. Solution methodology for each criterion is given, constraints and variance sensitivity are also discussed.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/HWSVLEU9/Baykal-Gu - SEMI-MARKOV DECISION PROCESSES.pdf}
}

@article{bazzanOpportunitiesMultiagentSystems2009,
  title = {Opportunities for Multiagent Systems and Multiagent Reinforcement Learning in Traffic Control},
  author = {Bazzan, Ana L. C.},
  year = {2009},
  month = jun,
  journal = {Autonomous Agents and Multi-Agent Systems},
  volume = {18},
  number = {3},
  pages = {342--375},
  issn = {1573-7454},
  doi = {10.1007/s10458-008-9062-9},
  urldate = {2023-09-26},
  abstract = {The increasing demand for mobility in our society poses various challenges to traffic engineering, computer science in general, and artificial intelligence and multiagent systems in particular. As it is often the case, it is not possible to provide additional capacity, so that a more efficient use of the available transportation infrastructure is necessary. This relates closely to multiagent systems as many problems in traffic management and control are inherently distributed. Also, many actors in a transportation system fit very well the concept of autonomous agents: the driver, the pedestrian, the traffic expert; in some cases, also the intersection and the traffic signal controller can be regarded as an autonomous agent. However, the ``agentification'' of a transportation system is associated with some challenging issues: the number of agents is high, typically agents are highly adaptive, they react to changes in the environment at individual level but cause an unpredictable collective pattern, and act in a highly coupled environment. Therefore, this domain poses many challenges for standard techniques from multiagent systems such as coordination and learning. This paper has two main objectives: (i) to present problems, methods, approaches and practices in traffic engineering (especially regarding traffic signal control); and (ii) to highlight open problems and challenges so that future research in multiagent systems can address them.},
  langid = {english},
  keywords = {Coordination of agents,Game-theory,Multiagent learning,Multiagent systems,Reinforcement learning,Traffic signal control},
  file = {/home/jeroen/Zotero/storage/JXBA9ZN2/Bazzan - 2009 - Opportunities for multiagent systems and multiagen.pdf}
}

@book{bdr2023,
  title = {Distributional Reinforcement Learning},
  author = {Bellemare, Marc G. and Dabney, Will and Rowland, Mark},
  year = {2023},
  publisher = {MIT Press}
}

@misc{bellemareDistributionalPerspectiveReinforcement2017,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'e}mi},
  year = {2017},
  month = jul,
  number = {arXiv:1707.06887},
  eprint = {1707.06887},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-02-22},
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/SMQ78JD2/Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf;/home/jeroen/Zotero/storage/3UF7WA75/1707.html}
}

@misc{belloNeuralCombinatorialOptimization2017,
  title = {Neural {{Combinatorial Optimization}} with {{Reinforcement Learning}}},
  author = {Bello, Irwan and Pham, Hieu and Le, Quoc V. and Norouzi, Mohammad and Bengio, Samy},
  year = {2017},
  month = jan,
  number = {arXiv:1611.09940},
  eprint = {1611.09940},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.09940},
  urldate = {2024-10-14},
  abstract = {This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Despite the computational expense, without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/RQNZI5D2/Bello et al. - 2017 - Neural Combinatorial Optimization with Reinforcement Learning.pdf;/home/jeroen/Zotero/storage/Z8KGEYKS/1611.html}
}

@article{belouadahSchedulingReleaseDates1992,
  title = {Scheduling with Release Dates on a Single Machine to Minimize Total Weighted Completion Time},
  author = {Belouadah, H. and Posner, M. E. and Potts, C. N.},
  year = {1992},
  month = may,
  journal = {Discrete Applied Mathematics},
  volume = {36},
  number = {3},
  pages = {213--231},
  issn = {0166-218X},
  doi = {10.1016/0166-218X(92)90255-9},
  urldate = {2023-10-23},
  abstract = {This paper considers the problem of scheduling jobs with release dates on a single machine to minimize the total weighted completion time. A branch and bound algorithm is proposed which incorporates three special features that contribute to its efficiency. Firstly, quickly computed lower bounds are obtained using a procedure which is based on job splitting. The job splitting methodology is shown to be applicable to a range of total weighted completion time scheduling problems. Secondly, the branching rule includes a release date adjustment mechanism which increases release dates at certain nodes of the tree with a view to tightening lower bounds. Thirdly, the branch and bound algorithm includes a new dominance rule for eliminating nodes of the search tree. Computational experience on problems with up to 50 jobs indicates that the proposed algorithm is superior to other known algorithms.},
  file = {/home/jeroen/Zotero/storage/ZBKRIBP9/Belouadah et al. - 1992 - Scheduling with release dates on a single machine .pdf;/home/jeroen/Zotero/storage/NGL4CCBH/0166218X92902559.html}
}

@misc{bengioMachineLearningCombinatorial2020,
  title = {Machine {{Learning}} for {{Combinatorial Optimization}}: A {{Methodological Tour}} d'{{Horizon}}},
  shorttitle = {Machine {{Learning}} for {{Combinatorial Optimization}}},
  author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
  year = {2020},
  month = mar,
  number = {arXiv:1811.06128},
  eprint = {1811.06128},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.06128},
  urldate = {2023-09-24},
  abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/FB883LT7/Bengio et al. - 2020 - Machine Learning for Combinatorial Optimization a.pdf;/home/jeroen/Zotero/storage/2IHTDUK3/1811.html}
}

@article{berterottiereFlexibleJobshopScheduling2024,
  title = {Flexible Job-Shop Scheduling with Transportation Resources},
  author = {Berterotti{\`e}re, Lucas and {Dauz{\`e}re-P{\'e}r{\`e}s}, St{\'e}phane and Yugma, Claude},
  year = {2024},
  month = feb,
  journal = {European Journal of Operational Research},
  volume = {312},
  number = {3},
  pages = {890--909},
  issn = {03772217},
  doi = {10.1016/j.ejor.2023.07.036},
  urldate = {2024-11-15},
  abstract = {This paper addresses an extension of the flexible job-shop scheduling problem where transportation resources are explicitly considered when moving jobs from one machine to another. Operations should be assigned to and scheduled on machines and vehicles and the routes of vehicles should be determined. We extend the classical disjunctive graph model to include transportation operations and exploit the graph in an integrated approach to solve the problem. We propose a metaheuristic using a neighborhood function that allows a large set of moves to be explored. As the exact computation of the makespan of every move is time-consuming, we present a move evaluation procedure that runs in constant time (which does not depend on the size of the instance) to choose a promising move in the neighborhood of a solution. This move evaluation procedure is used in a tabu search framework. Computational results show the efficiency of the proposed approach, the quality of the move evaluation procedure and the relevance of explicitly modeling transportation resources. New benchmark instances are also proposed.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/YIHTLHNE/Berterottière et al. - 2024 - Flexible job-shop scheduling with transportation resources.pdf}
}

@article{bettiolPontryaginMaximumPrinciple2021,
  title = {Pontryagin Maximum Principle for State Constrained Optimal Sampled-Data Control Problems on Time Scales},
  author = {Bettiol, Piernicola and Bourdin, Lo{\"i}c},
  year = {2021},
  journal = {ESAIM: Control, Optimisation and Calculus of Variations},
  volume = {27},
  pages = {51},
  publisher = {EDP Sciences},
  doi = {10.1051/cocv/2021046},
  urldate = {2025-02-02},
  abstract = {In this paper we consider optimal sampled-data control problems on time scales with inequality state constraints. A Pontryagin maximum principle is established, extending to the state constrained case existing results in the time scale literature. The proof is based on the Ekeland variational principle and on the concept of implicit spike variations adapted to the time scale setting. The main result is then applied to continuous-time min-max optimal sampled-data control problems, and a maximal velocity minimization problem for the harmonic oscillator with sampled-data control is numerically solved for illustration.},
  keywords = {Ekeland variational principle,implicit spike variations,min-max optimal control problems,Optimal control,Pontryagin maximum principle,sampled-data control,state constraints,time scales},
  file = {/home/jeroen/Zotero/storage/E5GCQMFT/Bettiol and Bourdin - 2021 - Pontryagin maximum principle for state constrained optimal sampled-data control problems on time sca.pdf}
}

@techreport{BolusaniEtal2024OO,
  type = {Technical {{Report}}},
  title = {The {{SCIP}} Optimization Suite 9.0},
  author = {Bolusani, Suresh and Besan{\c c}on, Mathieu and Bestuzheva, Ksenia and Chmiela, Antonia and Dion{\'i}sio, Jo{\~a}o and Donkiewicz, Tim and {van Doornmalen}, Jasper and Eifler, Leon and Ghannam, Mohammed and Gleixner, Ambros and Graczyk, Christoph and Halbig, Katrin and Hedtke, Ivo and Hoen, Alexander and Hojny, Christopher and {van der Hulst}, Rolf and Kamp, Dominik and Koch, Thorsten and Kofler, Kevin and Lentz, Jurgen and Manns, Julian and Mexi, Gioni and {Erik M{\"u}hmer} and Pfetsch, Marc E. and Schl{\"o}sser, Franziska and Serrano, Felipe and Shinano, Yuji and Turner, Mark and Vigerske, Stefan and Weninger, Dieter and Xu, Lixing},
  year = {2024},
  month = feb,
  institution = {Optimization Online},
  file = {/home/jeroen/Zotero/storage/KYLWAG3H/Bolusani et al. - 2024 - The SCIP optimization suite 9.0.pdf}
}

@techreport{BolusaniEtal2024ZR,
  type = {{{ZIB-Report}}},
  title = {The {{SCIP}} Optimization Suite 9.0},
  author = {Bolusani, Suresh and Besan{\c c}on, Mathieu and Bestuzheva, Ksenia and Chmiela, Antonia and Dion{\'i}sio, Jo{\~a}o and Donkiewicz, Tim and {van Doornmalen}, Jasper and Eifler, Leon and Ghannam, Mohammed and Gleixner, Ambros and Graczyk, Christoph and Halbig, Katrin and Hedtke, Ivo and Hoen, Alexander and Hojny, Christopher and {van der Hulst}, Rolf and Kamp, Dominik and Koch, Thorsten and Kofler, Kevin and Lentz, Jurgen and Manns, Julian and Mexi, Gioni and {Erik M{\"u}hmer} and Pfetsch, Marc E. and Schl{\"o}sser, Franziska and Serrano, Felipe and Shinano, Yuji and Turner, Mark and Vigerske, Stefan and Weninger, Dieter and Xu, Lixing},
  year = {2024},
  month = feb,
  number = {24-02-29},
  institution = {Zuse Institute Berlin}
}

@article{boonApplicationsPollingSystems2011,
  title = {Applications of Polling Systems},
  author = {Boon, M. A. A. and {van der Mei}, R. D. and Winands, E. M. M.},
  year = {2011},
  month = jul,
  journal = {Surveys in Operations Research and Management Science},
  volume = {16},
  number = {2},
  pages = {67--82},
  issn = {1876-7354},
  doi = {10.1016/j.sorms.2011.01.001},
  urldate = {2024-02-28},
  abstract = {Since the first paper on polling systems, written by Mack in 1957, a huge number of papers on this topic has been written. A typical polling system consists of a number of queues, attended by a single server. In several surveys, the most notable ones written by Takagi, detailed and comprehensive descriptions of the mathematical analysis of polling systems are provided. The goal of the present survey paper is to complement these papers by putting the emphasis on applications of polling models. We discuss not only the capabilities, but also the limitations of polling models in representing various applications. The present survey is directed at both academicians and practitioners.},
  file = {/home/jeroen/Zotero/storage/Q3WY9JPS/Boon et al. - 2011 - Applications of polling systems.pdf;/home/jeroen/Zotero/storage/DGCF9YUQ/S187673541100002X.html}
}

@inproceedings{boonCriticallyLoadedKlimited2016,
  title = {Critically Loaded K-Limited Polling Systems},
  booktitle = {Proceedings of the 9th {{EAI International Conference}} on {{Performance Evaluation Methodologies}} and {{Tools}}},
  author = {Boon, Marko and Winands, Erik},
  year = {2016},
  publisher = {ICST},
  address = {Berlin, Germany},
  doi = {10.4108/eai.14-12-2015.2262578},
  urldate = {2025-02-08},
  isbn = {978-1-63190-096-9},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/ECG7CBT4/Boon and Winands - 2016 - Critically loaded k-limited polling systems.pdf}
}

@article{boonNetworksFixedcycleIntersections2018,
  title = {Networks of Fixed-Cycle Intersections},
  author = {Boon, Marko A.A. and Van Leeuwaarden, Johan S.H.},
  year = {2018},
  month = nov,
  journal = {Transportation Research Part B: Methodological},
  volume = {117},
  pages = {254--271},
  issn = {01912615},
  doi = {10.1016/j.trb.2018.08.019},
  urldate = {2023-09-25},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/WMHHVYFC/Boon and Van Leeuwaarden - 2018 - Networks of fixed-cycle intersections.pdf}
}

@article{boonOpenProblemsCritically2022a,
  title = {Open Problems for Critically Loaded K-Limited Polling Systems},
  author = {Boon, Marko A. A. and Winands, Erik M. M.},
  year = {2022},
  month = apr,
  journal = {Queueing Systems},
  volume = {100},
  number = {3},
  pages = {281--283},
  issn = {1572-9443},
  doi = {10.1007/s11134-022-09770-x},
  urldate = {2025-02-08},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/M5WUM5CQ/Boon and Winands - 2022 - Open problems for critically loaded k-limited polling systems.pdf}
}

@misc{boonOptimalCapacityAllocation2022,
  title = {Optimal Capacity Allocation for Heavy-Traffic Fixed-Cycle Traffic-Light Queues and Intersections},
  author = {Boon, Marko and Janssen, Guido and {van Leeuwaarden}, Johan and Timmerman, Rik},
  year = {2022},
  month = aug,
  number = {arXiv:2104.04303},
  eprint = {2104.04303},
  primaryclass = {math},
  publisher = {arXiv},
  urldate = {2022-12-21},
  abstract = {Setting traffic light signals is a classical topic in traffic engineering, and important in heavy-traffic conditions when green times become scarce and longer queues are inevitably formed. For the fixed-cycle traffic-light queue, an elementary queueing model for one traffic light with cyclic signaling, we obtain heavy-traffic limits that capture the long-term queue behavior. We leverage the limit theorems to obtain sharp performance approximations for one queue in heavy traffic. We also consider optimization problems that aim for optimal division of green times among multiple conflicting traffic streams. We show that inserting heavy-traffic approximations leads to tractable optimization problems and close-to-optimal signal prescriptions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Optimization and Control,Mathematics - Probability},
  file = {/home/jeroen/Zotero/storage/NQIN56RM/Boon et al. - 2022 - Optimal capacity allocation for heavy-traffic fixe.pdf}
}

@inproceedings{bouderbaReinforcementLearningQLEARNING2019,
  title = {Reinforcement {{Learning}} ({{Q-LEARNING}}) Traffic Light Controller within Intersection Traffic System},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Big Data}} and {{Internet}} of {{Things}}},
  author = {Bouderba, Saif Islam and Moussa, Najem},
  year = {2019},
  month = oct,
  pages = {1--6},
  publisher = {ACM},
  address = {Rabat Morocco},
  doi = {10.1145/3372938.3372999},
  urldate = {2023-01-20},
  abstract = {In this paper we study the effect of signalized traffic intersection control strategies in a cellular automaton model for transportation in urban networks. Starting with a simple synchronized strategy, then a green wave which gave a surprising result. Finally, a reinforcement learning approach (Q-LEARNING) is presented to learn the traffic light controller how to interact with drivers with different situation. By keeping a belief the level of cooperation drivers, we improved the performance of Q-LEARNING algorithms. We show that our traffic light controller successfully learns how to manage the intersection with less deadlocks than without learning.},
  isbn = {978-1-4503-7240-4},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/7PLTGWMR/Bouderba and Moussa - 2019 - Reinforcement Learning (Q-LEARNING) traffic light .pdf}
}

@article{bourdinOptimalSampleddataControls2022,
  title = {Optimal Sampled-Data Controls with Running Inequality State Constraints: {{Pontryagin}} Maximum Principle and Bouncing Trajectory Phenomenon},
  shorttitle = {Optimal Sampled-Data Controls with Running Inequality State Constraints},
  author = {Bourdin, Lo{\"i}c and Dhar, Gaurav},
  year = {2022},
  month = feb,
  journal = {Mathematical Programming},
  volume = {191},
  number = {2},
  pages = {907--951},
  issn = {1436-4646},
  doi = {10.1007/s10107-020-01574-2},
  urldate = {2025-02-02},
  abstract = {In the present paper we derive a Pontryagin maximum principle for general nonlinear optimal sampled-data control problems in the presence of running inequality state constraints. We obtain, in particular, a nonpositive averaged Hamiltonian gradient condition associated with an adjoint vector being a function of bounded variation. As a well known challenge, theoretical and numerical difficulties may arise due to the possible pathological behavior of the adjoint vector (jumps and singular part lying on parts of the optimal trajectory in contact with the boundary of the restricted state space). However, in our case with sampled-data controls, we prove that, under certain general hypotheses, the optimal trajectory activates the running inequality state constraints at most at the sampling times. Due to this so-called bouncing trajectory phenomenon, the adjoint vector experiences jumps at most at the sampling times (and thus in a finite number and at precise instants) and its singular part vanishes. Taking advantage of these informations, we are able to implement an indirect numerical method which we use to solve three simple examples.},
  langid = {english},
  keywords = {34H05,49M05,93C10,93C57,Ekeland variational principle,Indirect numerical method,Optimal control,Pontryagin maximum principle,Sampled-data control,Shooting method,State constraints},
  file = {/home/jeroen/Zotero/storage/3LVXR22B/Bourdin and Dhar - 2022 - Optimal sampled-data controls with running inequality state constraints Pontryagin maximum principl.pdf}
}

@article{boxmaTwoQueuePollingModels2002,
  title = {Two-{{Queue Polling Models}} with a {{Patient Server}}},
  author = {Boxma, O.J. and Schlegel, S. and Yechiali, U.},
  year = {2002},
  month = apr,
  journal = {Annals of Operations Research},
  volume = {112},
  number = {1},
  pages = {101--121},
  issn = {1572-9338},
  doi = {10.1023/A:1020929021474},
  urldate = {2024-03-15},
  abstract = {We consider two-queue polling models with the special feature that a timer mechanism is employed at Q1: whenever the server polls Q1 and finds it empty, it activates a timer and remains dormant, waiting for the first arrival. If such an arrival occurs before the timer expires, a busy period starts in accordance with Q1's service discipline. However, if the timer is shorter than the interarrival time to Q1, the server does not wait any more and switches back to Q2. We consider three configurations: (i) Q1 is controlled by the 1-limited protocol while Q2 is served exhaustively, (ii) Q1 employs the exhaustive regime while Q2 follows the 1-limited procedure, and (iii) both queues are served exhaustively. In all cases, we assume Poisson arrivals and allow general service and switchover time distributions. Our main results include the queue length distributions at polling instants, the waiting time distributions and the distribution of the total workload in the system.},
  langid = {english},
  keywords = {1-limited,alternating service,exhaustive,patient server,polling,timer,two queues},
  file = {/home/jeroen/Zotero/storage/V6C656VZ/Boxma et al. - 2002 - Two-Queue Polling Models with a Patient Server.pdf}
}

@inproceedings{bradtkeReinforcementLearningMethods1994,
  title = {Reinforcement {{Learning Methods}} for {{Continuous-Time Markov Decision Problems}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bradtke, Steven and Duff, Michael},
  year = {1994},
  volume = {7},
  publisher = {MIT Press},
  urldate = {2024-03-05},
  abstract = {Semi-Markov  Decision  Problems  are  continuous  time  generaliza(cid:173) tions  of discrete  time  Markov  Decision  Problems.  A  number  of  reinforcement  learning  algorithms  have  been  developed  recently  for  the  solution  of Markov  Decision  Problems,  based on  the ideas  of asynchronous dynamic programming and stochastic approxima(cid:173) tion.  Among these  are TD(,x), Q-Iearning,  and Real-time Dynamic  Programming.  After  reviewing  semi-Markov  Decision  Problems  and Bellman's optimality equation in  that context,  we  propose al(cid:173) gorithms similar  to those  named above,  adapted to the solution  of  semi-Markov Decision  Problems.  We demonstrate these algorithms  by  applying  them to the  problem of determining  the  optimal con(cid:173) trol for  a  simple  queueing  system.  We  conclude  with  a  discussion  of circumstances  under which these  algorithms may be usefully  ap(cid:173) plied.},
  file = {/home/jeroen/Zotero/storage/FGKGTEQ2/Bradtke and Duff - 1994 - Reinforcement Learning Methods for Continuous-Time.pdf}
}

@article{bruckerJobshopSchedulingLimited2006,
  title = {Job-Shop Scheduling with Limited Capacity Buffers},
  author = {Brucker, Peter and Heitmann, Silvia and Hurink, Johann and Nieberg, Tim},
  year = {2006},
  month = apr,
  journal = {OR Spectrum},
  volume = {28},
  number = {2},
  pages = {151--176},
  issn = {1436-6304},
  doi = {10.1007/s00291-005-0008-1},
  urldate = {2024-11-14},
  abstract = {In this paper we investigate job-shop problems where limited capacity buffers to store jobs in non-processing periods are present. In such a problem setting, after finishing processing on a machine, a job either directly has to be processed on the following machine or it has to be stored in a prespecified buffer. If the buffer is completely occupied the job may wait on its current machine but blocks this machine for other jobs. Besides a general buffer model, also specific configurations are considered. The aim of this paper is to find a compact representation of solutions for the jobshop problem with buffers. In contrast to the classical job-shop problem, where a solution may be given by the sequences of the jobs on the machines, now also the buffers have to be incorporated in the solution representation. In a first part, two such representations are proposed, one which is achieved by adapting the alternative graph model and a second which is based on the disjunctive graph model. In a second part, it is investigated whether the given solution representation can be simplified for specific buffer configurations. For the general buffer configuration it is shown that an incorporation of the buffers in the solution representation is necessary, whereas for specific buffer configurations possible simplifications are presented.},
  langid = {english},
  keywords = {Alternative graph,Buffer,Disjunctive graph,Job-shop problem},
  file = {/home/jeroen/Zotero/storage/BC2ZXFR9/Brucker et al. - 2006 - Job-shop scheduling with limited capacity buffers.pdf}
}

@article{bruckerSchedulingChainsIdentical2006,
  title = {Scheduling Chains with Identical Jobs and Constant Delays on a Single Machine},
  author = {Brucker, Peter and Knust, Sigrid and O{\u g}uz, Ceyda},
  year = {2006},
  month = feb,
  journal = {Mathematical Methods of Operations Research},
  volume = {63},
  number = {1},
  pages = {63--75},
  issn = {1432-5217},
  doi = {10.1007/s00186-005-0014-8},
  urldate = {2023-10-20},
  abstract = {In this paper we study the single-machine problem 1{\textbar}chains(l), pj= p{\textbar}{$\sum$} Cjin which jobs with constant processing times and generalized precedence constraints in form of chains with constant delays are given. One has to schedule the jobs on a single machine such that all delays between consecutive jobs in a chain are satisfied and the sum of all completion times of the jobs is minimized. We show that this problem is polynomially solvable.},
  langid = {english},
  keywords = {Complexity results,Delays,Scheduling,Time-lags},
  file = {/home/jeroen/Zotero/storage/YIUHYYQ7/Brucker et al. - 2006 - Scheduling chains with identical jobs and constant.pdf}
}

@article{bubeckRegretAnalysisStochastic2012,
  title = {Regret {{Analysis}} of {{Stochastic}} and {{Nonstochastic Multi-armed Bandit Problems}}},
  author = {Bubeck, S{\'e}bastien},
  year = {2012},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {5},
  number = {1},
  pages = {1--122},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000024},
  urldate = {2022-12-08},
  abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration--exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the 1930s, exploration--exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/I589D9ZR/Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf}
}

@article{busoniuComprehensiveSurveyMultiagent2008,
  title = {A {{Comprehensive Survey}} of {{Multiagent Reinforcement Learning}}},
  author = {Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  year = {2008},
  month = mar,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {38},
  number = {2},
  pages = {156--172},
  issn = {1094-6977, 1558-2442},
  doi = {10.1109/TSMCC.2007.913919},
  urldate = {2023-09-27},
  abstract = {Multi-agent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must instead discover a solution on their own, using learning. A significant part of the research on multi-agent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multi-agent reinforcement learning (MARL). A central issue in the field is the formal statement of the multi-agent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents' learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim---either explicitly or implicitly---at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges of MARL are described along with some of the problem domains where MARL techniques have been applied. Finally, an outlook for the field is provided.},
  langid = {english},
  keywords = {MARL},
  file = {/home/jeroen/Zotero/storage/29BVG529/Busoniu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement.pdf}
}

@inproceedings{caiRealTimeBiddingReinforcement2017,
  title = {Real-{{Time Bidding}} by {{Reinforcement Learning}} in {{Display Advertising}}},
  booktitle = {Proceedings of the {{Tenth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Cai, Han and Ren, Kan and Zhang, Weinan and Malialis, Kleanthis and Wang, Jun and Yu, Yong and Guo, Defeng},
  year = {2017},
  month = feb,
  eprint = {1701.02490},
  primaryclass = {cs},
  pages = {661--670},
  doi = {10.1145/3018661.3018702},
  urldate = {2022-12-13},
  abstract = {The majority of online display ads are served through realtime bidding (RTB) --- each ad display impression is auctioned off in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign's real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks. The empirical study on two large-scale real-world datasets and the live A/B testing on a commercial platform have demonstrated the superior performance and high efficiency compared to state-of-the-art methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/8HEL4ZHB/Cai et al. - 2017 - Real-Time Bidding by Reinforcement Learning in Dis.pdf}
}

@misc{CaptumModelInterpretability,
  title = {Captum {$\cdot$} {{Model Interpretability}} for {{PyTorch}}},
  urldate = {2024-10-15},
  abstract = {Model Interpretability for PyTorch},
  howpublished = {https://captum.ai/},
  file = {/home/jeroen/Zotero/storage/FFTIKNRV/captum.ai.html}
}

@misc{catichaLecturesProbabilityEntropy2008,
  title = {Lectures on {{Probability}}, {{Entropy}}, and {{Statistical Physics}}},
  author = {Caticha, Ariel},
  year = {2008},
  month = jul,
  number = {arXiv:0808.0012},
  eprint = {0808.0012},
  primaryclass = {cond-mat, physics:physics, stat},
  publisher = {arXiv},
  urldate = {2023-02-18},
  abstract = {These lectures deal with the problem of inductive inference, that is, the problem of reasoning under conditions of incomplete information. Is there a general method for handling uncertainty? Or, at least, are there rules that could in principle be followed by an ideally rational mind when discussing scientific matters? What makes one statement more plausible than another? How much more plausible? And then, when new information is acquired how do we change our minds? Or, to put it differently, are there rules for learning? Are there rules for processing information that are objective and consistent? Are they unique? And, come to think of it, what, after all, is information? It is clear that data contains or conveys information, but what does this precisely mean? Can information be conveyed in other ways? Is information physical? Can we measure amounts of information? Do we need to? Our goal is to develop the main tools for inductive inference--probability and entropy--from a thoroughly Bayesian point of view and to illustrate their use in physics with examples borrowed from the foundations of classical statistical physics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Statistics Theory,Physics - Data Analysis Statistics and Probability,Physics - General Physics},
  file = {/home/jeroen/Zotero/storage/YN6PZ7FS/Caticha - 2008 - Lectures on Probability, Entropy, and Statistical .pdf;/home/jeroen/Zotero/storage/DRQN8XWU/0808.html}
}

@misc{cesa-bianchiBoltzmannExplorationDone2017,
  title = {Boltzmann {{Exploration Done Right}}},
  author = {{Cesa-Bianchi}, Nicol{\`o} and Gentile, Claudio and Lugosi, G{\'a}bor and Neu, Gergely},
  year = {2017},
  month = nov,
  number = {arXiv:1705.10257},
  eprint = {1705.10257},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-12-09},
  abstract = {Boltzmann exploration is a classic strategy for sequential decision-making under uncertainty, and is one of the most standard tools in Reinforcement Learning (RL). Despite its widespread use, there is virtually no theoretical understanding about the limitations or the actual benefits of this exploration scheme. Does it drive exploration in a meaningful way? Is it prone to misidentifying the optimal actions or spending too much time exploring the suboptimal ones? What is the right tuning for the learning rate? In this paper, we address several of these questions in the classic setup of stochastic multi-armed bandits. One of our main results is showing that the Boltzmann exploration strategy with any monotone learning-rate sequence will induce suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that guarantees near-optimal performance, albeit only when given prior access to key problem parameters that are typically not available in practical situations (like the time horizon \$T\$ and the suboptimality gap \${\textbackslash}Delta\$). More importantly, we propose a novel variant that uses different learning rates for different arms, and achieves a distribution-dependent regret bound of order \${\textbackslash}frac\{K{\textbackslash}log{\textasciicircum}2 T\}\{{\textbackslash}Delta\}\$ and a distribution-independent bound of order \${\textbackslash}sqrt\{KT\}{\textbackslash}log K\$ without requiring such prior knowledge. To demonstrate the flexibility of our technique, we also propose a variant that guarantees the same performance bounds even if the rewards are heavy-tailed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/J9KN3GKJ/Cesa-Bianchi et al. - 2017 - Boltzmann Exploration Done Right.pdf;/home/jeroen/Zotero/storage/A9LIPILZ/1705.html}
}

@incollection{chaudhuriComparativeStudyAlgorithms2022,
  title = {A {{Comparative Study}} of {{Algorithms}} for {{Intelligent Traffic Signal Control}}},
  booktitle = {Machine {{Learning}} and {{Autonomous Systems}}},
  author = {Chaudhuri, Hrishit and Masti, Vibha and Veerendranath, Vishruth and Natarajan, S.},
  editor = {Chen, Joy Iong-Zong and Wang, Haoxiang and Du, Ke-Lin and Suma, V.},
  year = {2022},
  volume = {269},
  pages = {271--287},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-16-7996-4_19},
  urldate = {2023-01-20},
  abstract = {In this paper, methods have been explored to effectively optimise traffic signal control to minimise waiting times and queue lengths, thereby increasing traffic flow. The traffic intersection was first defined as a Markov Decision Process, and a state representation, actions and rewards were chosen. Simulation of Urban MObility (SUMO) was used to simulate an intersection and then compare a Round Robin Scheduler, a Feedback Control mechanism and two Reinforcement Learning techniques - Deep Q Network (DQN) and Advantage Actor-Critic (A2C), as the policy for the traffic signal in the simulation under different scenarios. Finally, the methods were tested on a simulation of a real-world intersection in Bengaluru, India.},
  isbn = {978-981-16-7995-7 978-981-16-7996-4},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/BYK43CTD/Chaudhuri et al. - 2022 - A Comparative Study of Algorithms for Intelligent .pdf}
}

@article{chenCooperativeIntersectionManagement2016,
  title = {Cooperative {{Intersection Management}}: {{A Survey}}},
  shorttitle = {Cooperative {{Intersection Management}}},
  author = {Chen, Lei and Englund, Cristofer},
  year = {2016},
  month = feb,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {17},
  number = {2},
  pages = {570--586},
  issn = {1558-0016},
  doi = {10.1109/TITS.2015.2471812},
  urldate = {2024-01-26},
  abstract = {Intersection management is one of the most challenging problems within the transport system. Traffic light-based methods have been efficient but are not able to deal with the growing mobility and social challenges. On the other hand, the advancements of automation and communications have enabled cooperative intersection management, where road users, infrastructure, and traffic control centers are able to communicate and coordinate the traffic safely and efficiently. Major techniques and solutions for cooperative intersections are surveyed in this paper for both signalized and nonsignalized intersections, whereas focuses are put on the latter. Cooperative methods, including time slots and space reservation, trajectory planning, and virtual traffic lights, are discussed in detail. Vehicle collision warning and avoidance methods are discussed to deal with uncertainties. Concerning vulnerable road users, pedestrian collision avoidance methods are discussed. In addition, an introduction to major projects related to cooperative intersection management is presented. A further discussion of the presented works is given with highlights of future research topics. This paper serves as a comprehensive survey of the field, aiming at stimulating new methods and accelerating the advancement of automated and cooperative intersections.},
  keywords = {C-ITS,collision avoidance,Computer integrated manufacturing,Cooperative intelligent traffic systems,cooperative intersection management,mathematical optimization,motion planning,multi-agent system,Optimization,Planning,Roads,Safety,Trajectory,trajectory planning,V2I,V2P,V2V,V2X,VANET,Vehicles},
  file = {/home/jeroen/Zotero/storage/UADN4VTY/7244203.html}
}

@misc{chenLearningOptimizePrimer2021,
  title = {Learning to {{Optimize}}: {{A Primer}} and {{A Benchmark}}},
  shorttitle = {Learning to {{Optimize}}},
  author = {Chen, Tianlong and Chen, Xiaohan and Chen, Wuyang and Heaton, Howard and Liu, Jialin and Wang, Zhangyang and Yin, Wotao},
  year = {2021},
  month = jul,
  number = {arXiv:2103.12828},
  eprint = {2103.12828},
  publisher = {arXiv},
  urldate = {2024-10-15},
  abstract = {Learning to optimize (L2O) is an emerging approach that leverages machine learning to develop optimization methods, aiming at reducing the laborious iterations of hand engineering. It automates the design of an optimization method based on its performance on a set of training problems. This data-driven procedure generates methods that can efficiently solve problems similar to those in the training. In sharp contrast, the typical and traditional designs of optimization methods are theory-driven, so they obtain performance guarantees over the classes of problems specified by the theory. The difference makes L2O suitable for repeatedly solving a certain type of optimization problems over a specific distribution of data, while it typically fails on out-of-distribution problems. The practicality of L2O depends on the type of target optimization, the chosen architecture of the method to learn, and the training procedure. This new paradigm has motivated a community of researchers to explore L2O and report their findings. This article is poised to be the first comprehensive survey and benchmark of L2O for continuous optimization. We set up taxonomies, categorize existing works and research directions, present insights, and identify open challenges. We also benchmarked many existing L2O approaches on a few but representative optimization problems. For reproducible research and fair benchmarking purposes, we released our software implementation and data in the package Open-L2O at https://github.com/VITA-Group/Open-L2O.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/RVD969RP/Chen et al. - 2021 - Learning to Optimize A Primer and A Benchmark.pdf;/home/jeroen/Zotero/storage/U4PYEEBF/2103.html}
}

@misc{chenS2TNetSpatioTemporalTransformer2022,
  title = {{{S2TNet}}: {{Spatio-Temporal Transformer Networks}} for {{Trajectory Prediction}} in {{Autonomous Driving}}},
  shorttitle = {{{S2TNet}}},
  author = {Chen, Weihuang and Wang, Fangfang and Sun, Hongbin},
  year = {2022},
  month = jun,
  number = {arXiv:2206.10902},
  eprint = {2206.10902},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-01-20},
  abstract = {To safely and rationally participate in dense and heterogeneous traffic, autonomous vehicles require to sufficiently analyze the motion patterns of surrounding traffic-agents and accurately predict their future trajectories. This is challenging because the trajectories of traffic-agents are not only influenced by the traffic-agents themselves but also by spatial interaction with each other. Previous methods usually rely on the sequential stepby-step processing of Long Short-Term Memory networks (LSTMs) and merely extract the interactions between spatial neighbors for single type traffic-agents. We propose the Spatio-Temporal Transformer Networks (S2TNet), which models the spatio-temporal interactions by spatio-temporal Transformer and deals with the temporel sequences by temporal Transformer. We input additional category, shape and heading information into our networks to handle the heterogeneity of traffic-agents. The proposed methods outperforms state-of-the-art methods on ApolloScape Trajectory dataset by more than 7\% on both the weighted sum of Average and Final Displacement Error. Our code is available at https://github.com/chenghuang66/s2tnet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/jeroen/Zotero/storage/TPL7UFL9/Chen et al. - 2022 - S2TNet Spatio-Temporal Transformer Networks for T.pdf}
}

@misc{chenSurveyGraphNeural2024,
  title = {A {{Survey}} on {{Graph Neural Networks}} and {{Graph Transformers}} in {{Computer Vision}}: {{A Task-Oriented Perspective}}},
  shorttitle = {A {{Survey}} on {{Graph Neural Networks}} and {{Graph Transformers}} in {{Computer Vision}}},
  author = {Chen, Chaoqi and Wu, Yushuang and Dai, Qiyuan and Zhou, Hong-Yu and Xu, Mutian and Yang, Sibei and Han, Xiaoguang and Yu, Yizhou},
  year = {2024},
  month = aug,
  number = {arXiv:2209.13232},
  eprint = {2209.13232},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.13232},
  urldate = {2024-12-06},
  abstract = {Graph Neural Networks (GNNs) have gained momentum in graph representation learning and boosted the state of the art in a variety of areas, such as data mining ({\textbackslash}emph\{e.g.,\} social network analysis and recommender systems), computer vision ({\textbackslash}emph\{e.g.,\} object detection and point cloud learning), and natural language processing ({\textbackslash}emph\{e.g.,\} relation extraction and sequence learning), to name a few. With the emergence of Transformers in natural language processing and computer vision, graph Transformers embed a graph structure into the Transformer architecture to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases. In this paper, we present a comprehensive review of GNNs and graph Transformers in computer vision from a task-oriented perspective. Specifically, we divide their applications in computer vision into five categories according to the modality of input data, {\textbackslash}emph\{i.e.,\} 2D natural images, videos, 3D data, vision + language, and medical images. In each category, we further divide the applications according to a set of vision tasks. Such a task-oriented taxonomy allows us to examine how each task is tackled by different GNN-based approaches and how well these approaches perform. Based on the necessary preliminaries, we provide the definitions and challenges of the tasks, in-depth coverage of the representative approaches, as well as discussions regarding insights, limitations, and future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/BB4NE2BA/Chen et al. - 2024 - A Survey on Graph Neural Networks and Graph Transformers in Computer Vision A Task-Oriented Perspec.pdf;/home/jeroen/Zotero/storage/366CQMHQ/2209.html}
}

@article{chouAlgorithmsSingleMachine2009,
  title = {Algorithms for the Single Machine Total Weighted Completion Time Scheduling Problem with Release Times and Sequence-Dependent Setups},
  author = {Chou, Fuh-Der and Wang, Hui-Mei and Chang, Tzu-Yun},
  year = {2009},
  month = aug,
  journal = {The International Journal of Advanced Manufacturing Technology},
  volume = {43},
  number = {7-8},
  pages = {810--821},
  issn = {0268-3768, 1433-3015},
  doi = {10.1007/s00170-008-1762-4},
  urldate = {2023-10-20},
  abstract = {This paper considers the problem of scheduling n jobs on a single machine to minimize the total weighted completion time in the presence of sequence-dependent setup times and release times. To the best of our knowledge, little research has been devoted to this scheduling problem. Therefore, we developed two exact algorithms, including a constraint programming model and a branch-and-bound method for small problems. The obtained optimal solutions can be used as a benchmark for evaluating the performance of heuristics. With the complexity in mind, two heuristics, including a best index dispatch (BID) and a modified weighted shortest processing time (MWSPT) based on non-delay concepts are also proposed for large problems. The time complexities of the two proposed heuristics are O(n4) and O(n3), respectively. The computational results showed that the branch-andbound method could solve most instances with 40 jobs under the time limit of 7,200 s. The BID heuristic is superior to the MWSPT in solution quality, although both can efficiently and effectively obtain near-optimal solutions for large instances.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/9SEVG8S6/Chou et al. - 2009 - Algorithms for the single machine total weighted c.pdf}
}

@article{cireMultivaluedDecisionDiagrams,
  title = {Multivalued {{Decision Diagrams}} for {{Sequencing Problems}}},
  author = {Cire, Andre A and {van Hoeve}, Willem-Jan},
  abstract = {Sequencing problems are among the most prominent problems studied in operations research, with primary application in, e.g., scheduling and routing. We propose a novel approach to solving generic sequencing problems using multivalued decision diagrams (MDDs). Because an MDD representation may grow exponentially large, we apply MDDs of limited size as a discrete relaxation to the problem. We show that MDDs can be used to represent a wide range of sequencing problems with various side constraints and objective functions, and demonstrate how MDDs can be added to existing constraint-based scheduling systems. Our computational results indicate that the additional inference obtained by our MDDs can speed up a state-of-the art solver by several orders of magnitude, for a range of different problem classes.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/VEJBLVRL/Cire and van Hoeve - Multivalued Decision Diagrams for Sequencing Probl.pdf}
}

@mastersthesis{claassenApplicationDeepReinforcement2022,
  title = {Application of {{Deep Reinforcement Learning}} and {{Graph Neural Networks}} to the {{Machine Scheduling Problem}}},
  author = {Claassen, Rob},
  year = {2022},
  month = oct,
  urldate = {2023-11-29},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/F5IFHGUY/application-of-deep-reinforcement-learning-and-graph-neural-netwo.html}
}

@misc{corsiniSelfLabelingJobShop2024,
  title = {Self-{{Labeling}} the {{Job Shop Scheduling Problem}}},
  author = {Corsini, Andrea and Porrello, Angelo and Calderara, Simone and Dell'Amico, Mauro},
  year = {2024},
  month = oct,
  number = {arXiv:2401.11849},
  eprint = {2401.11849},
  publisher = {arXiv},
  urldate = {2024-11-15},
  abstract = {This work proposes a self-supervised training strategy designed for combinatorial problems. An obstacle in applying supervised paradigms to such problems is the need for costly target solutions often produced with exact solvers. Inspired by semi- and self-supervised learning, we show that generative models can be trained by sampling multiple solutions and using the best one according to the problem objective as a pseudo-label. In this way, we iteratively improve the model generation capability by relying only on its self-supervision, eliminating the need for optimality information. We validate this Self-Labeling Improvement Method (SLIM) on the Job Shop Scheduling (JSP), a complex combinatorial problem that is receiving much attention from the neural combinatorial community. We propose a generative model based on the well-known Pointer Network and train it with SLIM. Experiments on popular benchmarks demonstrate the potential of this approach as the resulting models outperform constructive heuristics and state-of-the-art learning proposals for the JSP. Lastly, we prove the robustness of SLIM to various parameters and its generality by applying it to the Traveling Salesman Problem.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Combinatorics},
  file = {/home/jeroen/Zotero/storage/7ZS3IM9B/Corsini et al. - 2024 - Self-Labeling the Job Shop Scheduling Problem.pdf;/home/jeroen/Zotero/storage/R656DNJ3/2401.html}
}

@book{cyganParameterizedAlgorithms2015,
  title = {Parameterized {{Algorithms}}},
  author = {Cygan, Marek and Fomin, Fedor V. and Kowalik, {\L}ukasz and Lokshtanov, Daniel and Marx, D{\'a}niel and Pilipczuk, Marcin and Pilipczuk, Micha{\l} and Saurabh, Saket},
  year = {2015},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-21275-3},
  urldate = {2023-11-15},
  isbn = {978-3-319-21274-6 978-3-319-21275-3},
  langid = {english},
  keywords = {algorithm analysis and problem complexity,Branching,Fixed-parameter tractability (FPT);,Integer linear programming (ILP),Kernels,Matroids,Parameterized algorithms,Parameterized complexity,Treewidth},
  file = {/home/jeroen/Zotero/storage/I3V363VE/Cygan et al. - 2015 - Parameterized Algorithms.pdf}
}

@article{daganzoCellTransmissionModel1994,
  title = {The Cell Transmission Model: {{A}} Dynamic Representation of Highway Traffic Consistent with the Hydrodynamic Theory},
  shorttitle = {The Cell Transmission Model},
  author = {Daganzo, Carlos F.},
  year = {1994},
  month = aug,
  journal = {Transportation Research Part B: Methodological},
  volume = {28},
  number = {4},
  pages = {269--287},
  issn = {0191-2615},
  doi = {10.1016/0191-2615(94)90002-7},
  urldate = {2023-01-20},
  abstract = {This paper presents a simple representation of traffic on a highway with a single entrance and exit. The representation can be used to predict traffic's evolution over time and space, including transient phenomena such as the building, propagation, and dissipation of queues. The easy-to-solve difference equations used to predict traffic's evolution are shown to be the discrete analog of the differential equations arising from a special case of the hydrodynamic model of traffic flow. The proposed method automatically generates appropriate changes in density at locations where the hydrodynamic theory would call for a shockwave; i.e., a jump in density such as those typically seen at the end of every queue. The complex side calculations required by classical methods to keep track of shockwaves are thus eliminated. The paper also shows how the equations can mimic the real-life development of stop-and-go traffic within moving queues.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/9QHLKJCR/Daganzo - 1994 - The cell transmission model A dynamic representat.pdf;/home/jeroen/Zotero/storage/7QWXPRCJ/0191261594900027.html}
}

@article{daganzoCellTransmissionModel1995,
  title = {The Cell Transmission Model, Part {{II}}: {{Network}} Traffic},
  shorttitle = {The Cell Transmission Model, Part {{II}}},
  author = {Daganzo, Carlos F.},
  year = {1995},
  month = apr,
  journal = {Transportation Research Part B: Methodological},
  volume = {29},
  number = {2},
  pages = {79--93},
  issn = {0191-2615},
  doi = {10.1016/0191-2615(94)00022-R},
  urldate = {2023-01-20},
  abstract = {This article shows how the evolution of multi-commodity traffic flows over complex networks can be predicted over time, based on a simple macroscopic computer representation of traffic flow that is consistent with the kinematic wave theory under all traffic conditions. The method does not use ad hoc procedures to treat special situations. After a brief review of the basic model for one link, the article describes how three-legged junctions can be modeled. It then introduces a numerical procedure for networks, assuming that a time-varying origin-destination (O-D) table is given and that the proportion of turns at every junction is known. These assumptions are reasonable for numerical analysis of disaster evacuation plans. The results are then extended to the case where, instead of the turning proportions, the best routes to each destination from every junction are known at all times. For technical reasons explained in the text, the procedure is more complicated in this case, requiring more computer memory and more time for execution. The effort is estimated to be about an order of magnitude greater than for the static traffic assignment problem on a network of the same size. The procedure is ideally suited for parallel computing. It is hoped that the results in the article will lead to more realistic models of freeway flow, disaster evacuations and dynamic traffic assignment for the evening commute.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/BXTINP8V/Daganzo - 1995 - The cell transmission model, part II Network traf.pdf;/home/jeroen/Zotero/storage/2IN935KE/019126159400022R.html}
}

@article{daiQueueingNetworkControls2022,
  title = {Queueing {{Network Controls}} via {{Deep Reinforcement Learning}}},
  author = {Dai, J. G. and Gluzman, Mark},
  year = {2022},
  month = mar,
  journal = {Stochastic Systems},
  volume = {12},
  number = {1},
  pages = {30--67},
  publisher = {INFORMS},
  issn = {1946-5238},
  doi = {10.1287/stsy.2021.0081},
  urldate = {2023-09-24},
  abstract = {Novel advanced policy gradient (APG) methods, such as trust region policy optimization and proximal policy optimization (PPO), have become the dominant reinforcement learning algorithms because of their ease of implementation and good practical performance. A conventional setup for notoriously difficult queueing network control problems is a Markov decision problem (MDP) that has three features: infinite state space, unbounded costs, and long-run average cost objective. We extend the theoretical framework of these APG methods for such MDP problems. The resulting PPO algorithm is tested on a parallel-server system and large-size multiclass queueing networks. The algorithm consistently generates control policies that outperform state-of-art heuristics in literature in a variety of load conditions from light to heavy traffic. These policies are demonstrated to be near optimal when the optimal policy can be computed. A key to the successes of our PPO algorithm is the use of three variance reduction techniques in estimating the relative value function via sampling. First, we use a discounted relative value function as an approximation of the relative value function. Second, we propose regenerative simulation to estimate the discounted relative value function. Finally, we incorporate the approximating martingale-process method into the regenerative estimator.},
  keywords = {control variate,long-run average cost,multiclass queueing network,reinforcement learning},
  file = {/home/jeroen/Zotero/storage/IR7QHQGI/Dai and Gluzman - 2022 - Queueing Network Controls via Deep Reinforcement L.pdf}
}

@article{darrochTrafficlightQueue1964,
  title = {On the {{Traffic-light Queue}}},
  author = {Darroch, J. N.},
  year = {1964},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {35},
  number = {1},
  pages = {380--388},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177703761},
  urldate = {2023-01-10},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/IPWE9LMT/Darroch - 1964 - On the Traffic-light Queue.pdf}
}

@article{dauzere-peresFlexibleJobShop2024,
  title = {The Flexible Job Shop Scheduling Problem: {{A}} Review},
  shorttitle = {The Flexible Job Shop Scheduling Problem},
  author = {{Dauz{\`e}re-P{\'e}r{\`e}s}, St{\'e}phane and Ding, Junwen and Shen, Liji and Tamssaouet, Karim},
  year = {2024},
  month = apr,
  journal = {European Journal of Operational Research},
  volume = {314},
  number = {2},
  pages = {409--432},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2023.05.017},
  urldate = {2024-11-15},
  abstract = {The flexible job shop scheduling problem (FJSP) is an NP-hard combinatorial optimization problem, which has wide applications in the real world. The complexity and relevance of the FJSP have led to numerous research works on its modeling and resolution. This paper reviews some of the research of the past 30 years on the problem, by presenting and classifying the different criteria, constraints, configurations and solution approaches that have been considered. Recent emerging topics on complex shop scheduling, multi-criteria optimization and uncertain and dynamic environments are discussed. Finally, future research opportunities are proposed.},
  keywords = {Constraints,Criteria,Flexible job shop,Scheduling,Survey},
  file = {/home/jeroen/Zotero/storage/6M7GA8Q2/Dauzère-Pérès et al. - 2024 - The flexible job shop scheduling problem A review.pdf;/home/jeroen/Zotero/storage/4KI2NTRS/S037722172300382X.html}
}

@article{dealmeidaMultiagentReinforcementLearning,
  title = {Multiagent {{Reinforcement Learning}} for {{Traffic Signal Control}}: A k-{{Nearest Neighbors Based Approach}}},
  author = {{de Almeida}, Vicente N and Bazzan, Ana L C and Abdoos, Monireh},
  abstract = {The increasing demand for mobility in our society poses various challenges to traffic engineering, computer science in general, and artificial intelligence in particular. As it is often the case, it is not possible to increase the capacity of road networks, therefore a more efficient use of the available transportation infrastructure is required. This relates closely to multiagent systems and to multiagent reinforcement learning, as many problems in traffic management and control are inherently distributed. However, one of the main challenges of this domain is that the state space is large and continuous, which makes it difficult to properly discretize states and also causes many RL algorithms to converge more slowly. To address these issues, a multiagent system with agents learning independently via a temporal difference learning algorithm based on k-nearest neighbors is presented as an option to control traffic signals in real-time. Our results show that the proposed method is both effective (reduces the average waiting time of vehicles in a traffic network) and efficient (the learning task is significantly accelerated), when compared to a baseline reported in the literature.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/AZ4W2BV2/de Almeida et al. - Multiagent Reinforcement Learning for Traffic Sign.pdf}
}

@misc{deasisMultistepReinforcementLearning2018,
  title = {Multi-Step {{Reinforcement Learning}}: {{A Unifying Algorithm}}},
  shorttitle = {Multi-Step {{Reinforcement Learning}}},
  author = {De Asis, Kristopher and {Hernandez-Garcia}, J. Fernando and Holland, G. Zacharias and Sutton, Richard S.},
  year = {2018},
  month = jun,
  number = {arXiv:1703.01327},
  eprint = {1703.01327},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-24},
  abstract = {Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD(\${\textbackslash}lambda\$) elegantly unifies one-step TD prediction with Monte Carlo methods through the use of eligibility traces and the trace-decay parameter \${\textbackslash}lambda\$. Currently, there are a multitude of algorithms that can be used to perform TD control, including Sarsa, \$Q\$-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be extended across multiple time steps to achieve better performance. Each of these algorithms is seemingly distinct, and no one dominates the others for all problems. In this paper, we study a new multi-step action-value algorithm called \$Q({\textbackslash}sigma)\$ which unifies and generalizes these existing algorithms, while subsuming them as special cases. A new parameter, \${\textbackslash}sigma\$, is introduced to allow the degree of sampling performed by the algorithm at each step during its backup to be continuously varied, with Sarsa existing at one extreme (full sampling), and Expected Sarsa existing at the other (pure expectation). \$Q({\textbackslash}sigma)\$ is generally applicable to both on- and off-policy learning, but in this work we focus on experiments in the on-policy case. Our results show that an intermediate value of \${\textbackslash}sigma\$, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/5P5ES2A3/De Asis et al. - 2018 - Multi-step Reinforcement Learning A Unifying Algo.pdf;/home/jeroen/Zotero/storage/INMF7HAF/1703.html}
}

@inproceedings{decamposOptimalLeastRestrictive2015,
  title = {Optimal and Least Restrictive Supervisory Control: {{Safety}} Verification Methods for Human-Driven Vehicles at Traffic Intersections},
  shorttitle = {Optimal and Least Restrictive Supervisory Control},
  booktitle = {2015 54th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {De Campos, Gabriel Rodrigues and Della Rossa, Fabio and Colombo, Alessandro},
  year = {2015},
  month = dec,
  pages = {1707--1712},
  publisher = {IEEE},
  address = {Osaka},
  doi = {10.1109/CDC.2015.7402456},
  urldate = {2024-11-25},
  abstract = {We consider a cooperative conflict resolution problem at traffic intersections. Our goal is to design a least restrictive supervisor able to identify the optimal corrections to a human-decided input with respect to a given performance index, while keeping the system safe. Here, safety is formulated in terms of a maximal safe controlled invariant set. Leveraging results from scheduling theory, we characterize the preorder of the optimal solution set and propose an efficient optimization algorithm providing Pareto optimal solutions. We illustrate the application of the proposed algorithm through simulations in which vehicles crossing an intersection are optimally overridden by the supervisor only when necessary to maintain safety.},
  isbn = {978-1-4799-7886-1},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/TS2226NG/De Campos et al. - 2015 - Optimal and least restrictive supervisory control Safety verification methods for human-driven vehi.pdf}
}

@misc{DecisionModelsOptimization,
  title = {Decision Models and Optimization Solvers {\textbar} {{Nextmv}}},
  urldate = {2024-10-15},
  abstract = {A multi-paradigm solving framework for optimization problems that integrates with OR-Tools and HiGHS for MIP, CP, LP, ALNS, and decision diagram solving.},
  howpublished = {https://www.nextmv.io/solve},
  file = {/home/jeroen/Zotero/storage/K4FNUANX/solve.html}
}

@article{DEMIRKOL1998137,
  title = {Benchmarks for Shop Scheduling Problems},
  author = {Demirkol, Ebru and Mehta, Sanjay and Uzsoy, Reha},
  year = {1998},
  journal = {European Journal of Operational Research},
  volume = {109},
  number = {1},
  pages = {137--141},
  issn = {0377-2217},
  doi = {10.1016/S0377-2217(97)00019-2},
  abstract = {In this paper we present extensive sets of randomly generated test problems for the problems of minimizing makespan (Cmax) and maximum lateness (Lmax) in flow shops and job shops. The 600 problems include three different types of routings, four different due date configurations and a variety of problem sizes. The problems, as well as the best existing solution and a lower bound on the optimal value are available on the world-wide web.},
  keywords = {Benchmarks,Combinatorial optimization,Scheduling}
}

@misc{dewantoAveragerewardModelfreeReinforcement2021,
  title = {Average-Reward Model-Free Reinforcement Learning: A Systematic Review and Literature Mapping},
  shorttitle = {Average-Reward Model-Free Reinforcement Learning},
  author = {Dewanto, Vektor and Dunn, George and Eshragh, Ali and Gallagher, Marcus and Roosta, Fred},
  year = {2021},
  month = aug,
  number = {arXiv:2010.08920},
  eprint = {2010.08920},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-05},
  abstract = {Reinforcement learning is important part of artificial intelligence. In this paper, we review model-free reinforcement learning that utilizes the average reward optimality criterion in the infinite horizon setting. Motivated by the solo survey by Mahadevan (1996a), we provide an updated review of work in this area and extend it to cover policy-iteration and function approximation methods (in addition to the value-iteration and tabular counterparts). We present a comprehensive literature mapping. We also identify and discuss opportunities for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/4ESWTXH3/Dewanto et al. - 2021 - Average-reward model-free reinforcement learning .pdf;/home/jeroen/Zotero/storage/2VLPP7VE/2010.html}
}

@misc{dontiDC3LearningMethod2021,
  title = {{{DC3}}: {{A}} Learning Method for Optimization with Hard Constraints},
  shorttitle = {{{DC3}}},
  author = {Donti, Priya L. and Rolnick, David and Kolter, J. Zico},
  year = {2021},
  month = apr,
  number = {arXiv:2104.12225},
  eprint = {2104.12225},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.12225},
  urldate = {2024-11-13},
  abstract = {Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap "approximate solvers." Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/92JFB2CB/Donti et al. - 2021 - DC3 A learning method for optimization with hard constraints.pdf;/home/jeroen/Zotero/storage/ACJHZWN4/2104.html}
}

@article{dresnerMultiagentApproachAutonomous2008,
  title = {A {{Multiagent Approach}} to {{Autonomous Intersection Management}}},
  author = {Dresner, K. and Stone, P.},
  year = {2008},
  month = mar,
  journal = {Journal of Artificial Intelligence Research},
  volume = {31},
  pages = {591--656},
  issn = {1076-9757},
  doi = {10.1613/jair.2502},
  urldate = {2024-11-28},
  abstract = {Artificial intelligence research is ushering in a new era of sophisticated, mass-market transportation technology. While computers can already fly a passenger jet better than a trained human pilot, people are still faced with the dangerous yet tedious task of driving automobiles. Intelligent Transportation Systems (ITS) is the field that focuses on integrating information technology with vehicles and transportation infrastructure to make transportation safer, cheaper, and more efficient. Recent advances in ITS point to a future in which vehicles themselves handle the vast majority of the driving task. Once autonomous vehicles become popular, autonomous interactions amongst multiple vehicles will be possible. Current methods of vehicle coordination, which are all designed to work with human drivers, will be outdated. The bottleneck for roadway efficiency will no longer be the drivers, but rather the mechanism by which those drivers' actions are coordinated. While open-road driving is a well-studied and more-or-less-solved problem, urban traffic scenarios, especially intersections, are much more challenging.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/YQU3K5SM/Dresner and Stone - 2008 - A Multiagent Approach to Autonomous Intersection Management.pdf}
}

@article{efrosininOptimalSchedulingGeneral2023,
  title = {Optimal {{Scheduling}} in {{General Multi-Queue System}} by {{Combining Simulation}} and {{Neural Network Techniques}}},
  author = {Efrosinin, Dmitry and Vishnevsky, Vladimir and Stepanova, Natalia},
  year = {2023},
  month = jan,
  journal = {Sensors},
  volume = {23},
  number = {12},
  pages = {5479},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s23125479},
  urldate = {2024-02-28},
  abstract = {The problem of optimal scheduling in a system with parallel queues and a single server has been extensively studied in queueing theory. However, such systems have mostly been analysed by assuming homogeneous attributes of arrival and service processes, or Markov queueing models were usually assumed in heterogeneous cases. The calculation of the optimal scheduling policy in such a queueing system with switching costs and arbitrary inter-arrival and service time distributions is not a trivial task. In this paper, we propose to combine simulation and neural network techniques to solve this problem. The scheduling in this system is performed by means of a neural network informing the controller at a service completion epoch on a queue index which has to be serviced next. We adapt the simulated annealing algorithm to optimize the weights and the biases of the multi-layer neural network initially trained on some arbitrary heuristic control policy with the aim to minimize the average cost function which in turn can be calculated only via simulation. To verify the quality of the obtained optimal solutions, the optimal scheduling policy was calculated by solving a Markov decision problem formulated for the corresponding Markovian counterpart. The results of numerical analysis show the effectiveness of this approach to find the optimal deterministic control policy for the routing, scheduling or resource allocation in general queueing systems. Moreover, a comparison of the results obtained for different distributions illustrates statistical insensitivity of the optimal scheduling policy to the shape of inter-arrival and service time distributions for the same first moments.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {heterogeneous queues,Markov decision problem,neural network,optimal scheduling,queue simulation,simulated annealing},
  file = {/home/jeroen/Zotero/storage/WTMKTG4Z/Efrosinin et al. - 2023 - Optimal Scheduling in General Multi-Queue System b.pdf}
}

@article{el-tantawyDesignReinforcementLearning2014,
  title = {Design of {{Reinforcement Learning Parameters}} for {{Seamless Application}} of {{Adaptive Traffic Signal Control}}},
  author = {{El-Tantawy}, Samah and Abdulhai, Baher and Abdelgawad, Hossam},
  year = {2014},
  month = jul,
  journal = {Journal of Intelligent Transportation Systems},
  volume = {18},
  number = {3},
  pages = {227--245},
  issn = {1547-2450, 1547-2442},
  doi = {10.1080/15472450.2013.810991},
  urldate = {2023-01-06},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/34WWS5TM/El-Tantawy et al. - 2014 - Design of Reinforcement Learning Parameters for Se.pdf}
}

@article{el-tantawyMultiagentReinforcementLearning2013,
  title = {Multiagent {{Reinforcement Learning}} for {{Integrated Network}} of {{Adaptive Traffic Signal Controllers}} ({{MARLIN-ATSC}}): {{Methodology}} and {{Large-Scale Application}} on {{Downtown Toronto}}},
  shorttitle = {Multiagent {{Reinforcement Learning}} for {{Integrated Network}} of {{Adaptive Traffic Signal Controllers}} ({{MARLIN-ATSC}})},
  author = {{El-Tantawy}, Samah and Abdulhai, Baher and Abdelgawad, Hossam},
  year = {2013},
  month = sep,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {14},
  number = {3},
  pages = {1140--1150},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2013.2255286},
  urldate = {2023-01-06},
  file = {/home/jeroen/Zotero/storage/7MIEKXXA/El-Tantawy et al. - 2013 - Multiagent Reinforcement Learning for Integrated N.pdf}
}

@misc{emamiLearningPermutationsSinkhorn2018,
  title = {Learning {{Permutations}} with {{Sinkhorn Policy Gradient}}},
  author = {Emami, Patrick and Ranka, Sanjay},
  year = {2018},
  month = may,
  number = {arXiv:1805.07010},
  eprint = {1805.07010},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.07010},
  urldate = {2024-11-12},
  abstract = {Many problems at the intersection of combinatorics and computer science require solving for a permutation that optimally matches, ranks, or sorts some data. These problems usually have a task-specific, often non-differentiable objective function that data-driven algorithms can use as a learning signal. In this paper, we propose the Sinkhorn Policy Gradient (SPG) algorithm for learning policies on permutation matrices. The actor-critic neural network architecture we introduce for SPG uniquely decouples representation learning of the state space from the highly-structured action space of permutations with a temperature-controlled Sinkhorn layer. The Sinkhorn layer produces continuous relaxations of permutation matrices so that the actor-critic architecture can be trained end-to-end. Our empirical results show that agents trained with SPG can perform competitively on sorting, the Euclidean TSP, and matching tasks. We also observe that SPG is significantly more data efficient at the matching task than the baseline methods, which indicates that SPG is conducive to learning representations that are useful for reasoning about permutations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/LMQWDQWD/Emami and Ranka - 2018 - Learning Permutations with Sinkhorn Policy Gradient.pdf;/home/jeroen/Zotero/storage/8WZAU2KG/1805.html}
}

@article{fagnantPreparingNationAutonomous2015,
  title = {Preparing a Nation for Autonomous Vehicles: Opportunities, Barriers and Policy Recommendations},
  shorttitle = {Preparing a Nation for Autonomous Vehicles},
  author = {Fagnant, Daniel J. and Kockelman, Kara},
  year = {2015},
  month = jul,
  journal = {Transportation Research Part A: Policy and Practice},
  volume = {77},
  pages = {167--181},
  issn = {0965-8564},
  doi = {10.1016/j.tra.2015.04.003},
  urldate = {2024-12-08},
  abstract = {Autonomous vehicles (AVs) represent a potentially disruptive yet beneficial change to our transportation system. This new technology has the potential to impact vehicle safety, congestion, and travel behavior. All told, major social AV impacts in the form of crash savings, travel time reduction, fuel efficiency and parking benefits are estimated to approach \$2000 to per year per AV, and may eventually approach nearly \$4000 when comprehensive crash costs are accounted for. Yet barriers to implementation and mass-market penetration remain. Initial costs will likely be unaffordable. Licensing and testing standards in the U.S. are being developed at the state level, rather than nationally, which may lead to inconsistencies across states. Liability details remain undefined, security concerns linger, and without new privacy standards, a default lack of privacy for personal travel may become the norm. The impacts and interactions with other components of the transportation system, as well as implementation details, remain uncertain. To address these concerns, the federal government should expand research in these areas and create a nationally recognized licensing framework for AVs, determining appropriate standards for liability, security, and data privacy.},
  keywords = {Autonomous vehicles,Congestion,Cost-benefit analysis,Market penetration,Safety,Vehicle automation},
  file = {/home/jeroen/Zotero/storage/LXPMXG2P/Fagnant and Kockelman - 2015 - Preparing a nation for autonomous vehicles opportunities, barriers and policy recommendations.pdf;/home/jeroen/Zotero/storage/ADZIJZFE/S0965856415000804.html}
}

@article{fayaziMixedIntegerLinearProgramming2018,
  title = {Mixed-{{Integer Linear Programming}} for {{Optimal Scheduling}} of {{Autonomous Vehicle Intersection Crossing}}},
  author = {Fayazi, Seyed Alireza and Vahidi, Ardalan},
  year = {2018},
  month = sep,
  journal = {IEEE Transactions on Intelligent Vehicles},
  volume = {3},
  number = {3},
  pages = {287--299},
  issn = {2379-8904},
  doi = {10.1109/TIV.2018.2843163},
  urldate = {2024-01-26},
  abstract = {We propose an urban traffic management scheme for an all connected vehicle environment. If all the vehicles are autonomous, for example, in smart city projects or future's dense city centers, then such an environment does not need a physical traffic signal. Instead, an intersection control server processes data streams from approaching vehicles, periodically solves an optimization problem, and assigns to each vehicle an optimal arrival time that ensures safety while significantly reducing number of stops and intersection delays. The scheduling problem is formulated as a mixed-integer linear program (MILP), and is solved by IBM CPLEX optimization package. The optimization outputs (scheduled access/arrival times) are sent to all approaching vehicles. The autonomous vehicles adjust their speed accordingly by a proposed trajectory planning algorithm with the aim of accessing the intersection at their scheduled times. A customized traffic microsimulation environment is developed to determine the potentials of the proposed solution in comparison to two baseline scenarios. In addition, the proposed MILP-based intersection control scheme is modified and simulated for a mixed traffic consisting of autonomous and human-controlled vehicles, all connected through a wireless communication to the intersection controller of a signalized intersection.},
  keywords = {Autonomous vehicles,Intelligent transportation systems connected and autonomous vehicles intersection traffic management mixed integer linear program trajectory planning traffic simulation and modeling,Optimal scheduling,Safety,Servers,Timing,Trajectory},
  file = {/home/jeroen/Zotero/storage/CGTGJGNR/8370718.html}
}

@inproceedings{fayaziOptimalSchedulingAutonomous2017,
  title = {Optimal Scheduling of Autonomous Vehicle Arrivals at Intelligent Intersections via {{MILP}}},
  booktitle = {2017 {{American Control Conference}} ({{ACC}})},
  author = {Fayazi, S. Alireza and Vahidi, Ardalan and Luckow, Andre},
  year = {2017},
  month = may,
  pages = {4920--4925},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.23919/ACC.2017.7963717},
  urldate = {2024-01-26},
  abstract = {We propose optimal scheduling of autonomous vehicle arrivals at intersections, eliminating the need for physical traffic signals. The proposed intersection control algorithm is assumed to have bi-directional communication links to approaching vehicles. After receiving subscription requests and status of approaching vehicles, the intersection control node calculates an arrival schedule that ensures safety while significantly reducing number of stops and intersection delays. The vehicleintersection coordination problem is formulated as a MixedInteger Linear Program (MILP). A case study is presented and a customized traffic microsimulation environment is developed to demonstrate the effectiveness of the proposed intersection control scheme.},
  isbn = {978-1-5090-5992-8},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/NEQUSM5Q/Fayazi et al. - 2017 - Optimal scheduling of autonomous vehicle arrivals .pdf}
}

@article{fengSpatiotemporalIntersectionControl2018,
  title = {Spatiotemporal Intersection Control in a Connected and Automated Vehicle Environment},
  author = {Feng, Yiheng and Yu, Chunhui and Liu, Henry X.},
  year = {2018},
  month = apr,
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {89},
  pages = {364--383},
  issn = {0968-090X},
  doi = {10.1016/j.trc.2018.02.001},
  urldate = {2024-01-26},
  abstract = {Current research on traffic control has focused on the optimization of either traffic signals or vehicle trajectories. With the rapid development of connected and automated vehicle (CAV) technologies, vehicles equipped with dedicated short-range communications (DSRC) can communicate not only with other CAVs but also with infrastructure. Joint control of vehicle trajectories and traffic signals becomes feasible and may achieve greater benefits regarding system efficiency and environmental sustainability. Traffic control framework is expected to be extended from one dimension (either spatial or temporal) to two dimensions (spatiotemporal). This paper investigates a joint control framework for isolated intersections. The control framework is modeled as a two-stage optimization problem with signal optimization at the first stage and vehicle trajectory control at the second stage. The signal optimization is modeled as a dynamic programming (DP) problem with the objective to minimize vehicle delay. Optimal control theory is applied to the vehicle trajectory control problem with the objective to minimize fuel consumption and emissions. A simplified objective function is adopted to get analytical solutions to the optimal control problem so that the two-stage model is solved efficiently. Simulation results show that the proposed joint control framework is able to reduce both vehicle delay and emissions under a variety of demand levels compared to fixed-time and adaptive signal control when vehicle trajectories are not optimized. The reduced vehicle delay and CO2 emissions can be as much as 24.0\% and 13.8\%, respectively for a simple two-phase intersection. Sensitivity analysis suggests that maximum acceleration and deceleration rates have a significant impact on the performance regarding both vehicle delay and emission reduction. Further extension to a full eight-phase intersection shows a similar pattern of delay and emission reduction by the joint control framework.},
  keywords = {Connected and automated vehicle,Delay and emission reduction,Traffic signal control,Vehicle trajectory control},
  file = {/home/jeroen/Zotero/storage/HXACNKZJ/Feng et al. - 2018 - Spatiotemporal intersection control in a connected and automated vehicle environment.pdf;/home/jeroen/Zotero/storage/UIUYKK85/S0968090X1830144X.html}
}

@article{fleurenOptimizingPretimedControl2017,
  title = {Optimizing Pre-Timed Control at Isolated Intersections},
  author = {Fleuren, Stijn},
  year = {2017},
  urldate = {2023-01-06},
  keywords = {MILP},
  file = {/home/jeroen/Zotero/storage/JFULKAUB/20170119_Fleuren.pdf}
}

@book{FoundationsBilevelProgramming2002,
  title = {Foundations of {{Bilevel Programming}}},
  year = {2002},
  series = {Nonconvex {{Optimization}} and {{Its Applications}}},
  volume = {61},
  publisher = {Kluwer Academic Publishers},
  address = {Boston},
  doi = {10.1007/b101970},
  urldate = {2024-12-19},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-1-4020-0631-9},
  langid = {english},
  keywords = {algorithms,linear optimization,modeling,nonlinear optimization,operations research,Optimality Conditions,optimization,programming},
  file = {/home/jeroen/Zotero/storage/KLMYBRHC/2002 - Foundations of Bilevel Programming.pdf}
}

@misc{frankowskaOptimalControlState,
  title = {Optimal {{Control}} under {{State Constraints}}},
  author = {Frankowska},
  file = {/home/jeroen/Zotero/storage/QE9YTNLS/_.pdf}
}

@article{garciaComprehensiveSurveySafe,
  title = {A {{Comprehensive Survey}} on {{Safe Reinforcement Learning}}},
  author = {Garc{\i}a, Javier and Fernandez, Fernando},
  pages = {44},
  abstract = {Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/H467LGLD/Garcıa and Fernandez - A Comprehensive Survey on Safe Reinforcement Learn.pdf}
}

@misc{garmendiaNeuralCombinatorialOptimization2022,
  title = {Neural {{Combinatorial Optimization}}: A {{New Player}} in the {{Field}}},
  shorttitle = {Neural {{Combinatorial Optimization}}},
  author = {Garmendia, Andoni I. and Ceberio, Josu and Mendiburu, Alexander},
  year = {2022},
  month = may,
  number = {arXiv:2205.01356},
  eprint = {2205.01356},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.01356},
  urldate = {2025-01-16},
  abstract = {Neural Combinatorial Optimization attempts to learn good heuristics for solving a set of problems using Neural Network models and Reinforcement Learning. Recently, its good performance has encouraged many practitioners to develop neural architectures for a wide variety of combinatorial problems. However, the incorporation of such algorithms in the conventional optimization framework has raised many questions related to their performance and the experimental comparison with other methods such as exact algorithms, heuristics and metaheuristics. This paper presents a critical analysis on the incorporation of algorithms based on neural networks into the classical combinatorial optimization framework. Subsequently, a comprehensive study is carried out to analyse the fundamental aspects of such algorithms, including performance, transferability, computational cost and generalization to larger-sized instances. To that end, we select the Linear Ordering Problem as a case of study, an NP-hard problem, and develop a Neural Combinatorial Optimization model to optimize it. Finally, we discuss how the analysed aspects apply to a general learning framework, and suggest new directions for future work in the area of Neural Combinatorial Optimization algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Discrete Mathematics},
  file = {/home/jeroen/Zotero/storage/J8ZPPXX9/Garmendia et al. - 2022 - Neural Combinatorial Optimization a New Player in the Field.pdf;/home/jeroen/Zotero/storage/JAPF9L2L/2205.html}
}

@article{ghanadbashiUsingOntologyGuide2022,
  title = {Using Ontology to Guide Reinforcement Learning Agents in Unseen Situations: {{A}} Traffic Signal Control System Case Study},
  shorttitle = {Using Ontology to Guide Reinforcement Learning Agents in Unseen Situations},
  author = {Ghanadbashi, Saeedeh and Golpayegani, Fatemeh},
  year = {2022},
  month = jan,
  journal = {Applied Intelligence},
  volume = {52},
  number = {2},
  pages = {1808--1824},
  issn = {0924-669X, 1573-7497},
  doi = {10.1007/s10489-021-02449-5},
  urldate = {2022-12-21},
  abstract = {In multi-agent systems, goal achievement is challenging when agents operate in ever-changing environments and face unseen situations, where not all the goals are known or predefined. In such cases, agents need to identify the changes and adapt their behaviour, by evolving their goals or even generating new goals to address the emerging requirements. Learning and practical reasoning techniques have been used to enable agents with limited knowledge to adapt to new circumstances. However, they depend on the availability of large amounts of data, require long exploration periods, and cannot help agents to set new goals. Furthermore, the accuracy of agents' actions is improved by introducing added intelligence through integrating conceptual features extracted from ontologies. However, the concerns related to taking suitable actions when unseen situations occur are not addressed. This paper proposes a new Automatic Goal Generation Model (AGGM) that enables agents to create new goals to handle unseen situations and to adapt to their ever-changing environment on a real-time basis. AGGM is compared to Q-learning, SARSA, and Deep Q Network in a Traffic Signal Control System case study. The results show that AGGM outperforms the baseline algorithms in unseen situations while handling the seen situations as well as the baseline algorithms.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/ABJVYHG8/Ghanadbashi and Golpayegani - 2022 - Using ontology to guide reinforcement learning age.pdf}
}

@article{ghavamzadehBayesianReinforcementLearning2015,
  title = {Bayesian {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Bayesian {{Reinforcement Learning}}},
  author = {Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
  year = {2015},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {8},
  number = {5-6},
  eprint = {1609.04436},
  primaryclass = {cs, stat},
  pages = {359--483},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000049},
  urldate = {2022-12-08},
  abstract = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/8MG2AF4R/Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf}
}

@article{gholamhosseinianComprehensiveSurveyCooperative,
  title = {A {{Comprehensive Survey}} on {{Cooperative Intersection Management}} for {{Heterogeneous Connected Vehicles}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  author = {Gholamhosseinian, Ashkan and Seitz, Jochen},
  journal = {IEEE Access},
  volume = {Volume 10},
  urldate = {2024-01-26},
  file = {/home/jeroen/Zotero/storage/64HIUTRM/9678327.html}
}

@inproceedings{ghoshOfflineRLPolicies2022,
  title = {Offline {{RL Policies Should Be Trained}} to Be {{Adaptive}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Ghosh, Dibya and Ajay, Anurag and Agrawal, Pulkit and Levine, Sergey},
  year = {2022},
  month = jun,
  pages = {7513--7530},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-02-23},
  abstract = {Offline RL algorithms must account for the fact that the dataset they are provided may leave many facets of the environment unknown. The most common way to approach this challenge is to employ pessimistic or conservative methods, which avoid behaviors that are too dissimilar from those in the training dataset. However, relying exclusively on conservatism has drawbacks: performance is sensitive to the exact degree of conservatism, and conservative objectives can recover highly suboptimal policies. In this work, we propose that offline RL methods should instead be adaptive in the presence of uncertainty. We show that acting optimally in offline RL in a Bayesian sense involves solving an implicit POMDP. As a result, optimal policies for offline RL must be adaptive, depending not just on the current state but rather all the transitions seen so far during evaluation. We present a model-free algorithm for approximating this optimal adaptive policy, and demonstrate the efficacy of learning such adaptive policies in offline RL benchmarks.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/GLY9GR5W/Ghosh et al. - 2022 - Offline RL Policies Should Be Trained to be Adapti.pdf}
}

@article{glaubiusRealTimeSchedulingReinforcement,
  title = {Real-{{Time Scheduling}} via {{Reinforcement Learning}}},
  author = {Glaubius, Robert and Tidwell, Terry and Gill, Christopher and Smart, William D},
  abstract = {Cyber-physical systems, such as mobile robots, must respond adaptively to dynamic operating conditions. Effective operation of these systems requires that sensing and actuation tasks are performed in a timely manner. Additionally, execution of mission specific tasks such as imaging a room must be balanced against the need to perform more general tasks such as obstacle avoidance. This problem has been addressed by maintaining relative utilization of shared resources among tasks near a user-specified target level. Producing optimal scheduling strategies requires complete prior knowledge of task behavior, which is unlikely to be available in practice. Instead, suitable scheduling strategies must be learned online through interaction with the system. We consider the sample complexity of reinforcement learning in this domain, and demonstrate that while the problem state space is countably infinite, we may leverage the problem's structure to guarantee efficient learning.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/GXTLU4S6/Glaubius et al. - Real-Time Scheduling via Reinforcement Learning.pdf}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2025-01-05},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/PA82TCX5/Glorot and Bengio - 2010 - Understanding the difficulty of training deep feedforward neural networks.pdf}
}

@book{gosaviSimulationBasedOptimizationParametric2015,
  title = {Simulation-{{Based Optimization}}: {{Parametric Optimization Techniques}} and {{Reinforcement Learning}}},
  shorttitle = {Simulation-{{Based Optimization}}},
  author = {Gosavi, Abhijit},
  year = {2015},
  series = {Operations {{Research}}/{{Computer Science Interfaces Series}}},
  volume = {55},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4899-7491-4},
  urldate = {2024-03-04},
  isbn = {978-1-4899-7490-7 978-1-4899-7491-4},
  langid = {english},
  keywords = {Computational Operations Research,Operations Research,Optimization,Reinforcement Learning,Simulation},
  file = {/home/jeroen/Zotero/storage/8YUUVDKH/Gosavi - 2015 - Simulation-Based Optimization Parametric Optimiza.pdf}
}

@article{gosaviTargetsensitiveControlMarkov2011,
  title = {Target-Sensitive Control of {{Markov}} and Semi-{{Markov}} Processes},
  author = {Gosavi, Abhijit},
  year = {2011},
  month = oct,
  journal = {International Journal of Control, Automation and Systems},
  volume = {9},
  number = {5},
  pages = {941--951},
  issn = {1598-6446, 2005-4092},
  doi = {10.1007/s12555-011-0515-6},
  urldate = {2024-03-05},
  abstract = {We develop the theory for Markov and semi-Markov control using dynamic programming and reinforcement learning in which a form of semi-variance which computes the variability of rewards below a pre-specified target is penalized. The objective is to optimize a function of the rewards and risk where risk is penalized. Penalizing variance, which is popular in the literature, has some drawbacks that can be avoided with semi-variance.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/6EYBLEUC/Gosavi - 2011 - Target-sensitive control of Markov and semi-Markov.pdf}
}

@incollection{grahamOptimizationApproximationDeterministic1979,
  title = {Optimization and {{Approximation}} in {{Deterministic Sequencing}} and {{Scheduling}}: A {{Survey}}},
  shorttitle = {Optimization and {{Approximation}} in {{Deterministic Sequencing}} and {{Scheduling}}},
  booktitle = {Annals of {{Discrete Mathematics}}},
  author = {Graham, R. L. and Lawler, E. L. and Lenstra, J. K. and Kan, A. H. G. Rinnooy},
  editor = {Hammer, P. L. and Johnson, E. L. and Korte, B. H.},
  year = {1979},
  month = jan,
  series = {Discrete {{Optimization II}}},
  volume = {5},
  pages = {287--326},
  publisher = {Elsevier},
  doi = {10.1016/S0167-5060(08)70356-X},
  urldate = {2023-10-23},
  abstract = {The theory of deterministic sequencing and scheduling has expanded rapidly during the past years. In this paper we survey the state of the art with respect to optimization and approximation algorithms and interpret these in terms of computational complexity theory. Special cases considered are single machine scheduling, identical, uniform and unrelated parallel machine scheduling, and open shop, flow shop and job shop scheduling. We indicate some problems for future research and include a selective bibliography.},
  file = {/home/jeroen/Zotero/storage/6INJSUCP/Graham et al. - 1979 - Optimization and Approximation in Deterministic Se.pdf}
}

@misc{gravesNeuralTuringMachines2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = dec,
  number = {arXiv:1410.5401},
  eprint = {1410.5401},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1410.5401},
  urldate = {2025-01-21},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/jeroen/Zotero/storage/U4RM22UC/Graves et al. - 2014 - Neural Turing Machines.pdf;/home/jeroen/Zotero/storage/MNXR9GEL/1410.html}
}

@book{greeneEconometricAnalysis2018,
  title = {Econometric Analysis},
  author = {Greene, William},
  year = {2018},
  edition = {Eighth edition},
  publisher = {Pearson},
  address = {New York, NY},
  isbn = {978-0-13-446136-6},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/MPE3LFKI/8th_econometric_analysis_by_William_H_greene_8th.pdf}
}

@article{greenshields1934a,
  title = {The Photographic Method of Studying Traffic Behavior},
  author = {Greenshields, B.D.},
  year = {1934},
  journal = {Proceedings of the 13th annual meeting of the highway research board},
  pages = {382--399},
  langid = {english}
}

@article{greenshields1935a,
  title = {A Study of Traffic Capacity},
  author = {Greenshields, B.D. and Bibbins, J.R. and Channing, W.S. and Miller, H.H.},
  year = {1935},
  journal = {Proceedings of the 14th annual meeting of the highway research board},
  pages = {448--477},
  langid = {english}
}

@article{grootenDeepReinforcementLearning2021,
  title = {Deep {{Reinforcement Learning}} for the Cooperative Card Game {{Hanabi}}},
  author = {Grooten, Bram},
  year = {2021},
  month = sep,
  langid = {english},
  file = {/home/jeroen/Zotero/storage/H8M7CX2Y/Grooten - Deep Reinforcement Learning for the cooperative ca.pdf}
}

@inproceedings{guerreiroOnlineSchedulingSurvey2023,
  title = {Online {{Scheduling}}: {{A Survey}}},
  shorttitle = {Online {{Scheduling}}},
  booktitle = {2023 18th {{Iberian Conference}} on {{Information Systems}} and {{Technologies}} ({{CISTI}})},
  author = {Guerreiro, Rita and Santos, Andr{\'e} S. and Tereso, Anabela},
  year = {2023},
  month = jun,
  pages = {1--6},
  issn = {2166-0727},
  doi = {10.23919/CISTI58278.2023.10211826},
  urldate = {2024-01-26},
  abstract = {In this article a deep search of the literature of online scheduling is conducted. This paper intends to assess the developments and solutions found for online scheduling problems. Online scheduling is a very important topic since most of the real scheduling problems have dynamic characteristics. First, it was developed a literature review about scheduling problems, dividing them in stochastic and deterministic problems as well as in online and offline problems. Then, a bibliometric analysis was performed. Finally, some case studies in the field of online scheduling were analyzed. Online Scheduling is mostly explored in industry and health areas. In some articles explored there is a rescheduling, and the sequence of task may change due to the arrival of new tasks. In other cases, the new tasks are introduced in blocks of time that do not affect the previous schedule. This last technique is limited, since, with the arrival of new tasks, the schedule is not re-evaluated. Therefore, it is thought that, in future work, within the scope of online scheduling, when new tasks or other significant changes enter the system, the system should be evaluated, allowing the necessary changes to be made to the existing schedule. The Industry 4.0 and the evolution of Internet of Things (IoT), Deep Learning and Machine Learning favours a continuous and real-time flow of information, which allows the implementation of real-time online scheduling. This is a branch that should be explored in future works.},
  keywords = {Bibliometrics,Deep learning,Dynamic scheduling,heuristics,Industries,internet of things,Job shop scheduling,machine learning,online scheduling,Real-time systems,Schedules},
  file = {/home/jeroen/Zotero/storage/9FBJZKNU/10211826.html}
}

@misc{gurobi,
  title = {Gurobi Optimizer Reference Manual},
  author = {{Gurobi Optimization, LLC}},
  year = {2024}
}

@misc{hanResearchAdaptiveJob,
  title = {Research on {{Adaptive Job Shop Scheduling Problems Based}} on {{Dueling Double DQN}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  author = {Han, Bao-An and Yang, Jian-Jun},
  urldate = {2023-10-14},
  howpublished = {https://ieeexplore.ieee.org/document/9218934?denied=}
}

@article{hartlSurveyMaximumPrinciples1995,
  title = {A {{Survey}} of the {{Maximum Principles}} for {{Optimal Control Problems}} with {{State Constraints}}},
  author = {Hartl, Richard F. and Sethi, Suresh P. and Vickson, Raymond G.},
  year = {1995},
  journal = {SIAM Review},
  volume = {37},
  number = {2},
  eprint = {2132823},
  eprinttype = {jstor},
  pages = {181--218},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  urldate = {2025-02-02},
  abstract = {This paper gives a survey of the various forms of Pontryagin's maximum principle for optimal control problems with state variable inequality constraints. The relations between the different sets of optimality conditions arising in these forms are shown. Furthermore, the application of these maximum principle conditions is demonstrated by solving some illustrative examples.},
  file = {/home/jeroen/Zotero/storage/PUBR6IM8/Hartl et al. - 1995 - A Survey of the Maximum Principles for Optimal Control Problems with State Constraints.pdf}
}

@article{hatzackOperationalTrafficControl,
  title = {The {{Operational Traffic Control Problem}}: {{Computational Complexity}} and {{Solutions}}},
  author = {Hatzack, Wolfgang and Nebel, Bernhard},
  abstract = {The operational traffic control problem comes up in a number of different contexts. It involves the coordinated movement of a set of vehicles and has by and large the flavor of a scheduling problem. In trying to apply scheduling techniques to the problem, one notes that this is a job-shop scheduling problem with blocking, a type of scheduling problem that is quite unusual. In particular, we will highlight a condition necessary to guarantee that job-shop schedules can be executed in the presences of the blocking constraint. Based on the insight that the traffic problem is a scheduling problem, we can derive the computational complexity of the operational traffic control problem and can design some algorithms to deal with this problem. In particular, we will specify a very simple method that works well in fast-time simulation contexts.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/ZUV6QKPM/Hatzack and Nebel - The Operational Traffic Control Problem Computational Complexity and Solutions.pdf}
}

@article{hausknechtAutonomousIntersectionManagement,
  title = {Autonomous {{Intersection Management}}: {{Multi-Intersection Optimization}}},
  author = {Hausknecht, Matthew and Au, Tsz-Chiu and Stone, Peter},
  abstract = {Advances in autonomous vehicles and intelligent transportation systems indicate a rapidly approaching future in which intelligent vehicles will automatically handle the process of driving. However, increasing the efficiency of today's transportation infrastructure will require intelligent traffic control mechanisms that work hand in hand with intelligent vehicles. To this end, Dresner and Stone proposed a new intersection control mechanism called Autonomous Intersection Management (AIM) and showed in simulation that by studying the problem from a multiagent perspective, intersection control can be made more efficient than existing control mechanisms such as traffic signals and stop signs. We extend their study beyond the case of an individual intersection and examine the unique implications and abilities afforded by using AIM-based agents to control a network of interconnected intersections. We examine different navigation policies by which autonomous vehicles can dynamically alter their planned paths, observe an instance of Braess' paradox, and explore the new possibility of dynamically reversing the flow of traffic along lanes in response to minute-by-minute traffic conditions. Studying this multiagent system in simulation, we quantify the substantial improvements in efficiency imparted by these agent-based traffic control methods.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/VKELRGRC/Hausknecht et al. - Autonomous Intersection Management Multi-Intersection Optimization.pdf}
}

@article{haydariDeepReinforcementLearning2022,
  title = {Deep {{Reinforcement Learning}} for {{Intelligent Transportation Systems}}: {{A Survey}}},
  shorttitle = {Deep {{Reinforcement Learning}} for {{Intelligent Transportation Systems}}},
  author = {Haydari, Ammar and Yilmaz, Yasin},
  year = {2022},
  month = jan,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {1},
  pages = {11--32},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2020.3008612},
  urldate = {2023-01-20},
  abstract = {Latest technological improvements increased the quality of transportation. New data-driven approaches bring out a new research direction for all control-based systems, e.g., in transportation, robotics, IoT and power systems. Combining data-driven applications with transportation systems plays a key role in recent transportation applications. In this paper, the latest deep reinforcement learning (RL) based traffic control applications are surveyed. Specifically, traffic signal control (TSC) applications based on (deep) RL, which have been studied extensively in the literature, are discussed in detail. Different problem formulations, RL parameters, and simulation environments for TSC are discussed comprehensively. In the literature, there are also several autonomous driving applications studied with deep RL models. Our survey extensively summarizes existing works in this field by categorizing them with respect to application types, control models and studied algorithms. In the end, we discuss the challenges and open questions regarding deep RL-based transportation applications.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/ZAD9J8I2/Haydari and Yilmaz - 2022 - Deep Reinforcement Learning for Intelligent Transp.pdf}
}

@article{hegemanandIntersectionControlFuture,
  title = {Intersection {{Control}} of the Future},
  author = {Hegemanand, Toon},
  langid = {english},
  keywords = {MILP},
  file = {/home/jeroen/Zotero/storage/AU58ZIKR/Hegemanand - Intersection Control of the future.pdf}
}

@phdthesis{heitmannJobshopSchedulingLimited2007,
  title = {Job-Shop Scheduling with Limited Buffer Capacities},
  author = {Heitmann, Silvia},
  year = {2007},
  month = mar,
  school = {Osnabr{\"u}ck University},
  file = {/home/jeroen/Zotero/storage/S9ZHP4ZT/34.pdf}
}

@misc{heOnlinePlanningPOMDPs2022,
  title = {Online {{Planning}} in {{POMDPs}} with {{Self-Improving Simulators}}},
  author = {He, Jinke and Suau, Miguel and Baier, Hendrik and Kaisers, Michael and Oliehoek, Frans A.},
  year = {2022},
  month = dec,
  number = {arXiv:2201.11404},
  eprint = {2201.11404},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.11404},
  urldate = {2023-09-27},
  abstract = {How can we plan efficiently in a large and complex environment when the time budget is limited? Given the original simulator of the environment, which may be computationally very demanding, we propose to learn online an approximate but much faster simulator that improves over time. To plan reliably and efficiently while the approximate simulator is learning, we develop a method that adaptively decides which simulator to use for every simulation, based on a statistic that measures the accuracy of the approximate simulator. This allows us to use the approximate simulator to replace the original simulator for faster simulations when it is accurate enough under the current context, thus trading off simulation speed and accuracy. Experimental results in two large domains show that when integrated with POMCP, our approach allows to plan with improving efficiency over time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/jeroen/Zotero/storage/6CY9E44N/He et al. - 2022 - Online Planning in POMDPs with Self-Improving Simu.pdf;/home/jeroen/Zotero/storage/427D6ZVR/2201.html}
}

@article{hePAMSCODPlatoonbasedArterial2012,
  title = {{{PAMSCOD}}: {{Platoon-based}} Arterial Multi-Modal Signal Control with Online Data},
  shorttitle = {{{PAMSCOD}}},
  author = {He, Qing and Head, K. Larry and Ding, Jun},
  year = {2012},
  month = feb,
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {20},
  number = {1},
  pages = {164--184},
  issn = {0968090X},
  doi = {10.1016/j.trc.2011.05.007},
  urldate = {2023-01-20},
  abstract = {A unified platoon-based mathematical formulation called PAMSCOD is presented to perform arterial (network) traffic signal control while considering multiple travel modes in a vehicle-to-infrastructure communications environment. First, a headway-based platoon recognition algorithm is developed to identify pseudo-platoons given probe vehicles' online information. It is assumed that passenger vehicles constitute a significant majority of the vehicles in the network. This algorithm identifies existing queues and significant platoons approaching each intersection. Second, a mixed-integer linear program (MILP) is solved to determine future optimal signal plans based on the current traffic controller status, online platoon data and priority requests from special vehicles, such as transit buses. Deviating from the traditional common network cycle length, PAMSCOD aims to provide multi-modal dynamical progression (MDP) on the arterial based on the probe information. Microscopic simulation using VISSIM shows that PAMSCOD can easily handle two common traffic modes, transit buses and automobiles, and significantly reduce delays for both modes under both non-saturated and oversaturated traffic conditions as compared to traditional state-of-practice coordinated-actuated signal control with timings optimized by SYNCHRO.},
  langid = {english},
  keywords = {MILP},
  file = {/home/jeroen/Zotero/storage/FNICQCLU/He et al. - 2012 - PAMSCOD Platoon-based arterial multi-modal signal.pdf}
}

@article{hofriOptimalControlTwo1987,
  title = {On the {{Optimal Control}} of {{Two Queues}} with {{Server Setup Times}} and {{Its Analysis}}},
  author = {Hofri, Micha and Ross, Keith W.},
  year = {1987},
  month = apr,
  journal = {SIAM Journal on Computing},
  volume = {16},
  number = {2},
  pages = {399--420},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/0216029},
  urldate = {2024-02-28},
  abstract = {A generalization of methods given by Kemperman [6] is used to obtain the generating function of a transform of the joint distribution of the waiting time \${\textbackslash}tau \_n \$ and the arrival time \$t\_n \$ of the nth customer to join a single server queue. The service time of the nth customer may depend on the inter-arrival time \$t\_\{n + 1\}  - t\_n \$. A relation between a transform of the virtual waiting time and our generating function is obtained as well as generating functions for the busy period and the probability that virtual waiting time is zero.},
  file = {/home/jeroen/Zotero/storage/2IA7ECN6/Hofri and Ross - 1987 - On the Optimal Control of Two Queues with Server S.pdf}
}

@article{hoogendoornModelbasedStochasticControl,
  title = {Model-Based {{Stochastic Control}} of {{Traffic Networks}}},
  author = {Hoogendoorn, S P and Knoop, V L and {van Zuylen}, H J},
  abstract = {Uncertainty of traffic network operations has been a subject of lively debate in the last decade. However, little efforts have been put in developing control frameworks that are not only aimed at improving the mean performance of the system, but also at improving the system robustness and reliability. In fact, it can be argued that most of the current control approaches are only aimed at improving the efficiency, which can even be counterproductive from a robustness point of view.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/LZMGC6NL/Hoogendoorn et al. - Model-based Stochastic Control of Traffic Networks.pdf}
}

@article{huang2022cleanrl,
  title = {{{CleanRL}}: {{High-quality}} Single-File Implementations of Deep Reinforcement Learning Algorithms},
  author = {Huang, Shengyi and Dossa, Rousslan Fernand Julien and Ye, Chang and Braga, Jeff and Chakraborty, Dipam and Mehta, Kinal and Ara{\'u}jo, Jo{\~a}o G.M.},
  year = {2022},
  journal = {Journal of Machine Learning Research},
  volume = {23},
  number = {274},
  pages = {1--18}
}

@article{hullermeierAleatoricEpistemicUncertainty2021,
  title = {Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods},
  shorttitle = {Aleatoric and Epistemic Uncertainty in Machine Learning},
  author = {H{\"u}llermeier, Eyke and Waegeman, Willem},
  year = {2021},
  month = mar,
  journal = {Machine Learning},
  volume = {110},
  number = {3},
  pages = {457--506},
  issn = {1573-0565},
  doi = {10.1007/s10994-021-05946-3},
  urldate = {2023-10-13},
  abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
  langid = {english},
  keywords = {Bayesian inference,Calibration,Conformal prediction,Credal sets and classifiers,Deep neural networks,Ensembles,Epistemic uncertainty,Gaussian processes,Generative models,Likelihood-based methods,Probability,Set-valued prediction,Uncertainty,Version space learning},
  file = {/home/jeroen/Zotero/storage/JUPP7XHF/Hüllermeier and Waegeman - 2021 - Aleatoric and epistemic uncertainty in machine lea.pdf}
}

@inproceedings{hultApproximateSolutionOptimal2015,
  title = {An Approximate Solution to the Optimal Coordination Problem for Autonomous Vehicles at Intersections},
  booktitle = {2015 {{American Control Conference}} ({{ACC}})},
  author = {Hult, Robert and Campos, Gabriel R. and Falcone, Paolo and Wymeersch, Henk},
  year = {2015},
  month = jul,
  pages = {763--768},
  publisher = {IEEE},
  address = {Chicago, IL, USA},
  doi = {10.1109/ACC.2015.7170826},
  urldate = {2024-04-23},
  abstract = {In this paper, we address the problem of optimal and safe coordination of autonomous vehicles through a traffic intersection. We state the problem as a finite time, constrained optimal control problem, a combinatorial optimization problem that is difficult to solve in real-time. A low complexity computational scheme is proposed, based on a hierarchical decomposition of the original optimal control formulation, where a central coordination problem is solved together with a number of local optimal control problems for each vehicle. We show how the proposed decomposition allows a reduction of the complexity of the central problem, provided that approximated parametric solutions of the local problems are available beforehand. We derive conditions for the construction of the parametric approximations and demonstrate the method with a numerical example.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-4799-8684-2},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/WAVDXAAT/Hult et al. - 2015 - An approximate solution to the optimal coordinatio.pdf}
}

@article{hultCoordinationCooperativeAutonomous2016,
  title = {Coordination of {{Cooperative Autonomous Vehicles}}: {{Toward}} Safer and More Efficient Road Transportation},
  shorttitle = {Coordination of {{Cooperative Autonomous Vehicles}}},
  author = {Hult, Robert and Campos, Gabriel R. and Steinmetz, Erik and Hammarstrand, Lars and Falcone, Paolo and Wymeersch, Henk},
  year = {2016},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {33},
  number = {6},
  pages = {74--84},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2016.2602005},
  urldate = {2024-11-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/LLU9SXQM/Hult et al. - 2016 - Coordination of Cooperative Autonomous Vehicles Toward safer and more efficient road transportation.pdf}
}

@inproceedings{hultOptimalCoordinationAutomated2019,
  title = {Optimal {{Coordination}} of {{Automated Vehicles}} at {{Intersections}} with {{Turns}}},
  booktitle = {2019 18th {{European Control Conference}} ({{ECC}})},
  author = {Hult, Robert and Zanon, Mario and Gros, Sebastien and Falcone, Paolo},
  year = {2019},
  month = jun,
  pages = {225--230},
  publisher = {IEEE},
  address = {Naples, Italy},
  doi = {10.23919/ECC.2019.8795770},
  urldate = {2025-02-02},
  abstract = {In this paper we address the problem of coordinating automated vehicles at intersections by means of optimal control, and extend earlier work to include vehicles that turns inside the intersection. Turning vehicles requires rear-end collision avoidance relations that are turned on and off as a function of the vehicle state, to capture the departure from one lane and the merger onto another. Such binary decisions are difficult to handle with continuous optimization tools, and typically requires the introduction of integer variables. To keep the problem in the continuous optimization domain, we introduce a smooth, approximate representation of the binary on-off decision. Moreover, for both safety and comfort reasons, turning vehicles are required to limit their velocity while the turn is being negotiated We therefore introduce curvature-based acceleration constraints, which implicitly limits the velocity of the vehicle during the turn, and a comfort promoting term in the objective function. We discuss how the problem is transcribed to a nonlinear program and present simulation results which illustrates our approach. We demonstrate that for most practical problem instances the proposed approximation is exact, and that for problem instances where it isn't, the induced conservativeness is small.},
  isbn = {978-3-907144-00-8},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/QSPNVA7A/Hult et al. - 2019 - Optimal Coordination of Automated Vehicles at Intersections with Turns.pdf}
}

@article{hultOptimisationbasedCoordinationConnected2020,
  title = {Optimisation-Based Coordination of Connected, Automated Vehicles at Intersections},
  author = {Hult, Robert and Zanon, Mario and Gros, S{\'e}bastien and Wymeersch, Henk and Falcone, Paolo},
  year = {2020},
  month = may,
  journal = {Vehicle System Dynamics},
  volume = {58},
  number = {5},
  pages = {726--747},
  issn = {0042-3114, 1744-5159},
  doi = {10.1080/00423114.2020.1755446},
  urldate = {2025-02-02},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/BJNRZ7RD/Hult et al. - 2020 - Optimisation-based coordination of connected, automated vehicles at intersections.pdf}
}

@book{hultOptimizationBasedCoordination2019,
  title = {Optimization Based Coordination Strategies for Connected and Autonomous Vehicles},
  author = {Hult, Robert},
  year = {2019},
  publisher = {Chalmers University of Technology},
  address = {G{\"o}teborg},
  isbn = {978-91-7905-108-2},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/2KWPTM6K/Hult - 2019 - Optimization based coordination strategies for connected and autonomous vehicles.pdf}
}

@article{hultTechnicalReportApproximate,
  title = {Technical {{Report}}: {{Approximate}} Solution to the Optimal Coordination Problem for Autonomous Vehicles at Intersections},
  author = {Hult, Robert and Campos, Gabriel R and Falcone, Paolo and Wymeersch, Henk},
  abstract = {In this report, we address the problem of optimal and safe coordination of autonomous vehicles through a traffic intersection. We state the problem as a finite time, constrained optimal control problem, which result into a combinatorial optimization problem that might be difficult to solve in real-time. A low complexity computational scheme is proposed instead, based on a hierarchical decomposition of the optimal control formulation, where a central coordination problem is solved together with a number of local optimal control problems for each vehicle. We show how the proposed decomposition allows a drastic reduction of the complexity of the central problem, provided that approximated solutions of the local problems are available beforehand.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/PZ2HAPBC/Hult et al. - Technical Report Approximate solution to the optimal coordination problem for autonomous vehicles a.pdf}
}

@article{i.garmendiaApplicabilityNeuralCombinatorial2024,
  title = {Applicability of {{Neural Combinatorial Optimization}}: {{A Critical View}}},
  shorttitle = {Applicability of {{Neural Combinatorial Optimization}}},
  author = {I. Garmendia, Andoni and Ceberio, Josu and Mendiburu, Alexander},
  year = {2024},
  month = sep,
  journal = {ACM Transactions on Evolutionary Learning and Optimization},
  volume = {4},
  number = {3},
  pages = {1--26},
  issn = {2688-299X, 2688-3007},
  doi = {10.1145/3647644},
  urldate = {2025-01-16},
  abstract = {Neural Combinatorial Optimization has emerged as a new paradigm in the optimization area. It attempts to solve optimization problems by means of neural networks and reinforcement learning. In the past few years, due to their novelty and presumably good performance, many research papers have been published introducing new neural architectures for a variety of combinatorial problems. However, the incorporation of such models in the conventional optimization portfolio raises many questions related to their performance compared to other existing methods, such as exact algorithms, heuristics, or metaheuristics. This article aims to present a critical view of these new proposals, discussing their benefits and drawbacks with respect to the tools and algorithms already present in the optimization field. For this purpose, a comprehensive study is carried out to analyze the fundamental aspects of such methods, including performance, computational cost, transferability, and reusability of the trained model. Moreover, this discussion is accompanied by the design and validation of a new neural combinatorial optimization algorithm on two well-known combinatorial problems: the Linear Ordering Problem and the Permutation Flowshop Scheduling Problem. Finally, new directions for future work in the area of Neural Combinatorial Optimization algorithms are suggested.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/U9BYUB7T/I. Garmendia et al. - 2024 - Applicability of Neural Combinatorial Optimization A Critical View.pdf}
}

@inproceedings{iklassovStudyCurriculumLearning2023,
  title = {On the {{Study}} of {{Curriculum Learning}} for {{Inferring Dispatching Policies}} on the {{Job Shop Scheduling}}},
  booktitle = {Proceedings of the {{Thirty-Second International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Iklassov, Zangir and Medvedev, Dmitrii and Solozabal Ochoa De Retana, Ruben and Takac, Martin},
  year = {2023},
  month = aug,
  pages = {5350--5358},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Macau, SAR China},
  doi = {10.24963/ijcai.2023/594},
  urldate = {2023-10-28},
  abstract = {This paper studies the use of Curriculum Learning on Reinforcement Learning (RL) to improve the performance of the dispatching policies learned on the Job-shop Scheduling Problem (JSP). Current works in the literature present a large optimality gap when learning end-to-end solutions on this problem. In this regard, we identify the difficulty for RL to learn directly on large instances as part of the issue and use Curriculum Learning (CL) to mitigate this effect. Particularly, CL sequences the learning process in a curriculum of increasing complexity tasks, which allows learning on large instances that otherwise would be impossible to learn from scratch. In this paper, we present a size-agnostic model that enables us to demonstrate that current curriculum strategies have a major impact on the quality of the solution inferred. In addition, we introduce a novel Reinforced Adaptive Staircase Curriculum Learning (RASCL) strategy, which adjusts the difficulty level during the learning process by revisiting the worstperforming instances. Conducted experiments on Taillard's and Demirkol's datasets show that the presented approach significantly improves the current state-of-the-art models on the JSP. It reduces the average optimality gap from 19.35\% to 10.46\% on Taillard's instances and from 38.43\% to 18.85\% on Demirkol's instances.},
  isbn = {978-1-956792-03-4},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/EFP8YQ2B/Iklassov et al. - 2023 - On the Study of Curriculum Learning for Inferring .pdf}
}

@article{ingimundardottirDiscoveringDispatchingRules2018,
  title = {Discovering Dispatching Rules from Data Using Imitation Learning: {{A}} Case Study for the Job-Shop Problem},
  shorttitle = {Discovering Dispatching Rules from Data Using Imitation Learning},
  author = {Ingimundardottir, Helga and Runarsson, Thomas Philip},
  year = {2018},
  month = aug,
  journal = {Journal of Scheduling},
  volume = {21},
  number = {4},
  pages = {413--428},
  issn = {1099-1425},
  doi = {10.1007/s10951-017-0534-0},
  urldate = {2023-11-01},
  abstract = {Dispatching rules can be automatically generated from scheduling data. This paper will demonstrate that the key to learning an effective dispatching rule is through the careful construction of the training data, \$\${\textbackslash}\{{\textbackslash}mathbf \{x\}\_i(k),y\_i(k){\textbackslash}\}\_\{k=1\}{\textasciicircum}K{\textbackslash}in \{{\textbackslash}mathscr \{D\}\}\$\$, where (i) features of partially constructed schedules \$\${\textbackslash}mathbf \{x\}\_i\$\$should necessarily reflect the induced data distribution \$\$\{{\textbackslash}mathscr \{D\}\}\$\$for when the rule is applied. This is achieved by updating the learned model in an active imitation learning fashion; (ii) \$\$y\_i\$\$is labelled optimally using a MIP solver; and (iii) data need to be balanced, as the set is unbalanced with respect to the dispatching step k. Using the guidelines set by our framework the design of custom dispatching rules, for a particular scheduling application, will become more effective. In the study presented three different distributions of the job-shop will be considered. The machine learning approach considered is based on preference learning, i.e. which dispatch (post-decision state) is preferable to another.},
  langid = {english},
  keywords = {Composite dispatching rules,DAgger,Imitation learning,Performance analysis,Preference learning,Scheduling},
  file = {/home/jeroen/Zotero/storage/ZSDVWWJ3/Ingimundardottir and Runarsson - 2018 - Discovering dispatching rules from data using imit.pdf}
}

@article{jainEligibilityTracesOptions2018,
  title = {Eligibility {{Traces}} for {{Options}}},
  author = {Jain, Ayush and Precup, Doina},
  year = {2018},
  abstract = {Temporally extended actions not only represent knowledge in the hierarchical setup in reinforcement learning, they also improve exploration while reducing the complexity of choosing actions. The option framework provides a concrete way to implement and reason about temporal abstraction. This work attempts to test the utility of eligibility traces with options and find good ways of doing multi-step intra-option updates. Three algorithms, based on offpolicy methods - importance sampling, tree-backup and retrace, are proposed for using eligibility traces with options.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/BVEUDP8Z/Jain and Precup - 2018 - Eligibility Traces for Options.pdf}
}

@misc{jannerSequenceModelingSolutions2021,
  title = {Sequence {{Modeling Solutions}} for {{Reinforcement Learning Problems}}},
  author = {Janner, Michael},
  year = {2021},
  month = nov
}

@book{jaynes2003probability,
  title = {Probability Theory: {{The}} Logic of Science},
  author = {Jaynes, Edwin T},
  year = {2003},
  publisher = {Cambridge university press}
}

@inproceedings{jinCommonStructuresResource2019,
  title = {Common {{Structures}} in {{Resource Management}} as {{Driver}} for {{Reinforcement Learning}}: {{A Survey}} and {{Research Tracks}}},
  shorttitle = {Common {{Structures}} in {{Resource Management}} as {{Driver}} for {{Reinforcement Learning}}},
  booktitle = {Machine {{Learning}} for {{Networking}}},
  author = {Jin, Yue and Kostadinov, Dimitre and Bouzid, Makram and Aghasaryan, Armen},
  editor = {Renault, {\'E}ric and M{\"u}hlethaler, Paul and Boumerdassi, Selma},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {117--132},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-19945-6_8},
  abstract = {In the era of growing digitalization, dynamic resource management becomes one of the critical problems in many application fields where, due to the permanently evolving environment, the trade-off between cost and system performance needs to be continuously adapted. While traditional approaches based on prior system specification or model learning are challenged by the complexity and the dynamicity of these systems, a new paradigm of learning in interaction brings a strong promise - based on the toolset of model-free Reinforcement Learning (RL) and its great success stories in various domains. However, current RL methods still struggle to learn rapidly in incremental, online settings, which is a barrier to deal with many practical problems. To address the slow convergence issue, one approach consists in exploiting the system's structural properties, instead of acting in full model-free mode. In this paper, we review the existing resource management systems and unveil their common structural properties. We propose a meta-model and discuss the tracks on how these properties can enhance general purpose RL algorithms.},
  isbn = {978-3-030-19945-6},
  langid = {english},
  keywords = {Capacity management,Learning through interactions,Resource management,RL},
  file = {/home/jeroen/Zotero/storage/TBHHD5TB/Jin et al. - 2019 - Common Structures in Resource Management as Driver.pdf}
}

@misc{kaelblingReinforcementLearningSurvey1996,
  title = {Reinforcement {{Learning}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning}}},
  author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
  year = {1996},
  month = apr,
  number = {arXiv:cs/9605103},
  eprint = {cs/9605103},
  publisher = {arXiv},
  urldate = {2023-09-27},
  abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/jeroen/Zotero/storage/L7PTKLKD/Kaelbling et al. - 1996 - Reinforcement Learning A Survey.pdf;/home/jeroen/Zotero/storage/LASUXHLE/9605103.html}
}

@article{kellyCitygenInteractiveSystem,
  title = {Citygen: {{An Interactive System}} for {{Procedural City Generation}}},
  author = {Kelly, George and McCabe, Hugh},
  abstract = {Contemporary 3D games are often situated within large urban environments. This necessitates a time-consuming and expensive content creation process involving the modelling of vast amounts of geometric detail: including terrain, roads, buildings, and other associated features. We present a system called Citygen that aims to automate as much of this as possible by employing procedural generation methods to rapidly create the urban geometry typical of a modern city. Procedural methods have long been used within the graphics and game development communities to generate natural phenomena such as plants and trees. We employ these methods to generate the underlying road networks that form the structure of cities and urban neighbourhoods. These road networks are automatically mapped to any supplied terrain model, and adapt themselves to the specific geometry of the underlying terrain. Building footprints are automatically extracted from the resulting model and buildings can then be inserted either procedurally or by hand. Our system is unique in that it is designed to allow developers hands-on interactive control over the generation process. We achieve this by providing an interface allowing the user to directly manipulate geometric elements such as road intersection nodes, and to directly control and specify many aspects of the procedural generation. The results are updated in real time, thus facilitating an interactive design process.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/ME84A9YC/Kelly and McCabe - Citygen An Interactive System for Procedural City.pdf}
}

@article{kellySurveyProceduralTechniques2006a,
  title = {A {{Survey}} of {{Procedural Techniques}} for {{City Generation}}},
  author = {Kelly, George},
  year = {2006},
  publisher = {Dublin Institute of Technology},
  doi = {10.21427/D76M9P},
  urldate = {2023-02-11},
  abstract = {The computer game industry requires a skilled workforce and this combined with the complexity of modern games, means that production costs are extremely high. One of the most time consuming aspects is the creation of game geometry, the virtual world which the players inhabit. Procedural techniques have been used within computer graphics to create natural textures, simulate special effects and generate complex natural models including trees and waterfalls. It is these procedural techniques that we intend to harness to generate geometry and textures suitable for a game situated in an urban environment. Procedural techniques can provide many benefits for computer graphics applications when the correct algorithm is used. An overview of several commonly used procedural techniques including fractals, L-systems, Perlin noise, tiling systems and cellular basis is provided. The function of each technique and the resulting output they create are discussed to better understand their characteristics, benefits and relevance to the city generation problem. City generation is the creation of an urban area which necessitates the creation of buildings, situated along streets and arranged in appropriate patterns. Some research has already taken place into recreating road network patterns and generating buildings that can vary in function and architectural style. We will study the main body of existing research into procedural city generation and provide an overview of their implementations and a critique of their functionality and results. Finally we present areas in which further research into the generation of cities is required and outline our research goals for city generation.},
  copyright = {Creative Commons Attribution-Noncommercial-Share Alike 3.0 License},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/S3ILDLCP/Kelly - 2006 - A Survey of Procedural Techniques for City Generat.pdf}
}

@article{kellySurveyProceduralTechniques2006c,
  title = {A {{Survey}} of {{Procedural Techniques}} for {{City Generation}}},
  author = {Kelly, George and McCabe, Hugh},
  year = {2006},
  month = jan,
  volume = {14},
  doi = {10.21427/D76M9P},
  abstract = {The computer game industry requires a skilled workforce and this combined with the complexity of modern games, means that production costs are extremely high. One of the most time consuming aspects is the creation of game geometry, the virtual world which the players inhabit. Procedural techniques have been used within computer graphics to create natural textures, simulate special effects and generate complex natural models including trees and waterfalls. It is these procedural techniques that we intend to harness to generate geometry and textures suitable for a game situated in an urban environment. Procedural techniques can provide many benefits for computer graphics applications when the correct algorithm is used. An overview of several commonly used procedural techniques including fractals, L-systems, Perlin noise, tiling systems and cellular basis is provided. The function of each technique and the resulting output they create are discussed to better understand their characteristics, benefits and relevance to the city generation problem. City generation is the creation of an urban area which necessitates the creation of buildings, situated along streets and arranged in appropriate patterns. Some research has already taken place into recreating road network patterns and generating buildings that can vary in function and architectural style. We will study the main body of existing research into procedural city generation and provide an overview of their implementations and a critique of their functionality and results. Finally we present areas in which further research into the generation of cities is required and outline our research goals for city generation.}
}

@article{khayatianCrossroadsTimeawareApproach2020,
  title = {Crossroads+: {{A Time-aware Approach}} for {{Intersection Management}} of {{Connected Autonomous Vehicles}}},
  shorttitle = {Crossroads+},
  author = {Khayatian, Mohammad and Lou, Yingyan and Mehrabian, Mohammadreza and Shirvastava, Aviral},
  year = {2020},
  month = apr,
  journal = {ACM Transactions on Cyber-Physical Systems},
  volume = {4},
  number = {2},
  pages = {1--28},
  issn = {2378-962X, 2378-9638},
  doi = {10.1145/3364182},
  urldate = {2024-11-25},
  abstract = {As vehicles become autonomous and connected, intelligent management techniques can be utilized to operate an intersection without a traffic light. When a Connected Autonomous Vehicle (CAV) approaches an intersection, it shares its status and intended direction with the Intersection Manager (IM), and the IM checks the status of other CAVs and assigns a target velocity/reference trajectory for it to maintain. In practice, however, there is an unknown delay between the time a CAV sends a request to the IM and the moment it receives back the response, namely, the Round-Trip Delay (RTD). As a result, the CAV will start tracking the target velocity/reference trajectory later than when the IM expects, which may lead to accidents. In this article, we present a time-aware approach, Crossroads+, that makes CAVs' behaviors deterministic despite the existence of the unknown RTD. In Crossroads+, we use timestamping and synchronization to ensure that both the IM and the CAVs have the same notion of time. The IM will also set a fixed start time to track the target velocity/reference trajectory for each CAV. The effectiveness of the proposed Crossroads+ technique is illustrated by experiments on a 1/10 scale model of an intersection with CAVs. We also built a simulator to demonstrate the scalability of Crossroads+ for multi-lane intersections. Results from our experiments indicate that our approach can reduce the position uncertainty by 15\% in comparison with conventional techniques and achieve up to 36\% better throughputs.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/6UDJ6J9C/Khayatian et al. - 2020 - Crossroads+ A Time-aware Approach for Intersection Management of Connected Autonomous Vehicles.pdf}
}

@article{khayatianSurveyIntersectionManagement2020,
  title = {A {{Survey}} on {{Intersection Management}} of {{Connected Autonomous Vehicles}}},
  author = {Khayatian, Mohammad and Mehrabian, Mohammadreza and Andert, Edward and Dedinsky, Rachel and Choudhary, Sarthake and Lou, Yingyan and Shirvastava, Aviral},
  year = {2020},
  month = oct,
  journal = {ACM Transactions on Cyber-Physical Systems},
  volume = {4},
  number = {4},
  pages = {1--27},
  issn = {2378-962X, 2378-9638},
  doi = {10.1145/3407903},
  urldate = {2024-11-25},
  abstract = {Intersection management of Connected Autonomous Vehicles (CAVs) has the potential to improve safety and mobility. CAVs approaching an intersection can exchange information with the infrastructure or each other to schedule their cross times. By avoiding unnecessary stops, scheduling CAVs can increase traffic throughput, reduce energy consumption, and most importantly, minimize the number of accidents that happen in intersection areas due to human errors. We study existing intersection management approaches from following key perspectives: (1) intersection management interface, (2) scheduling policy, (3) existing wireless technologies, (4) existing vehicle models used by researchers and their impact, (5) conflict detection, (6) extension to multi-intersection management, (7) challenges of supporting human-driven vehicles, (8) safety and robustness required for real-life deployment, (9) graceful degradation and recovery for emergency scenarios, (10) security concerns and attack models, and (11) evaluation methods. We then discuss the effectiveness and limitations of each approach with respect to the aforementioned aspects and conclude with a discussion on tradeoffs and further research directions.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/4456ZXGY/Khayatian et al. - 2020 - A Survey on Intersection Management of Connected Autonomous Vehicles.pdf}
}

@misc{kidambiMOReLModelBasedOffline2021,
  title = {{{MOReL}} : {{Model-Based Offline Reinforcement Learning}}},
  shorttitle = {{{MOReL}}},
  author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  year = {2021},
  month = mar,
  number = {arXiv:2005.05951},
  eprint = {2005.05951},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-12-13},
  abstract = {In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. The ability to train RL policies offline can greatly expand the applicability of RL, its data efficiency, and its experimental velocity. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; and (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP. This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied offline RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g. generative modeling, uncertainty estimation, planning etc.) to directly translate into advances for offline RL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/IV2THQ4G/Kidambi et al. - 2021 - MOReL  Model-Based Offline Reinforcement Learning.pdf;/home/jeroen/Zotero/storage/SGVCJAZB/2005.html}
}

@article{kimProceduralCityGeneration2018,
  title = {Procedural City Generation beyond Game Development},
  author = {Kim, Joon-Seok and Kavak, Hamdi and Crooks, Andrew},
  year = {2018},
  month = nov,
  journal = {SIGSPATIAL Special},
  volume = {10},
  number = {2},
  pages = {34--41},
  doi = {10.1145/3292390.3292397},
  urldate = {2023-02-11},
  abstract = {The common trend in the scientific inquiry of urban areas and their populations is to use real-world geographic and population data to understand, explain, and predict urban phenomena. We argue that this trend limits our understanding of urban areas as dealing with arbitrarily collected geographic data requires technical expertise to process; moreover, population data is often aggregated, sparsified, or anonymized for privacy reasons. We believe synthetic urban areas generated via procedural city generation, which is a technique mostly used in the gaming area, could help improve the state-of-the-art in many disciplines which study urban areas. In this paper, we describe a selection of research areas that could benefit from such synthetic urban data and show that the current research in procedurally generated cities needs to address specific issues (e.g., plausibility) to sufficiently capture real-world cities and thus take such data beyond gaming.},
  file = {/home/jeroen/Zotero/storage/ZGQAJ62E/Kim et al. - 2018 - Procedural city generation beyond game development.pdf}
}

@misc{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  number = {arXiv:1609.02907},
  eprint = {1609.02907},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.02907},
  urldate = {2025-01-05},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/3XF6LGNR/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolutional Networks.pdf;/home/jeroen/Zotero/storage/QIZWH859/1609.html}
}

@misc{koolAttentionLearnSolve2019,
  title = {Attention, {{Learn}} to {{Solve Routing Problems}}!},
  author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
  year = {2019},
  month = feb,
  number = {arXiv:1803.08475},
  eprint = {1803.08475},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.08475},
  urldate = {2025-04-01},
  abstract = {The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/HC4STPSE/Kool et al. - 2019 - Attention, Learn to Solve Routing Problems!.pdf;/home/jeroen/Zotero/storage/XMLK5SB7/1803.html}
}

@article{koolLearningOptimizationCombinatorial,
  title = {Learning and {{Optimization}} in {{Combinatorial Spaces}}},
  author = {Kool, Wouter},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/TMPCSYV4/Kool - Learning and Optimization in Combinatorial Spaces.pdf}
}

@misc{krauss1998a,
  type = {{Tech. rept.}},
  title = {{Microscopic modeling of traffic flow: investigation of collision free vehicle dynamics}},
  author = {Krauss, S.},
  year = {1998},
  pages = {98--08},
  publisher = {DLR Deutsches Zentrum fur Luft-- und Raumfahrt},
  langid = {ngerman}
}

@book{kreyszigIntroductoryFunctionalAnalysis1989,
  title = {Introductory Functional Analysis with Applications},
  author = {Kreyszig, Erwin},
  year = {1989},
  series = {Wiley Classics Library},
  edition = {Wiley classics library ed},
  publisher = {Wiley},
  address = {New York},
  isbn = {978-0-471-50731-4 978-0-471-50459-7},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/KB32HABR/Kreyszig - 1989 - Introductory functional analysis with applications.pdf}
}

@inproceedings{kuyerMultiagentReinforcementLearning2008,
  title = {Multiagent {{Reinforcement Learning}} for {{Urban Traffic Control Using Coordination Graphs}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Kuyer, Lior and Whiteson, Shimon and Bakker, Bram and Vlassis, Nikos},
  editor = {Daelemans, Walter and Goethals, Bart and Morik, Katharina},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {656--671},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-87479-9_61},
  abstract = {Since traffic jams are ubiquitous in the modern world, optimizing the behavior of traffic lights for efficient traffic flow is a critically important goal. Though most current traffic lights use simple heuristic protocols, more efficient controllers can be discovered automatically via multiagent reinforcement learning, where each agent controls a single traffic light. However, in previous work on this approach, agents select only locally optimal actions without coordinating their behavior. This paper extends this approach to include explicit coordination between neighboring traffic lights. Coordination is achieved using the max-plus algorithm, which estimates the optimal joint action by sending locally optimized messages among connected agents. This paper presents the first application of max-plus to a large-scale problem and thus verifies its efficacy in realistic settings. It also provides empirical evidence that max-plus performs well on cyclic graphs, though it has been proven to converge only for tree-structured graphs. Furthermore, it provides a new understanding of the properties a traffic network must have for such coordination to be beneficial and shows that max-plus outperforms previous methods on networks that possess those properties.},
  isbn = {978-3-540-87479-9},
  langid = {english},
  keywords = {coordination graphs,max-plus,multiagent systems,reinforcement learning,traffic control},
  file = {/home/jeroen/Zotero/storage/IXPYC9HM/Kuyer et al. - 2008 - Multiagent Reinforcement Learning for Urban Traffic Control Using Coordination Graphs.pdf}
}

@article{laemmer2008a,
  title = {Self-Control of Traffic Lights and Vehicle Flows in Urban Road Networks},
  author = {L{\"a}mmer, S. and Helbing, D.},
  year = {2008},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2008},
  number = {04},
  langid = {english}
}

@article{lamorgeseExactDecompositionApproach2015,
  title = {An {{Exact Decomposition Approach}} for the {{Real-Time Train Dispatching Problem}}},
  author = {Lamorgese, Leonardo and Mannino, Carlo},
  year = {2015},
  month = feb,
  journal = {Operations Research},
  volume = {63},
  number = {1},
  pages = {48--64},
  publisher = {INFORMS},
  issn = {0030-364X},
  doi = {10.1287/opre.2014.1327},
  urldate = {2023-10-16},
  abstract = {Trains' movements on a railway network are regulated by official timetables. Deviations and delays occur quite often in practice, demanding fast rescheduling and rerouting decisions in order to avoid conflicts and minimize overall delay. This is the real-time train dispatching problem. In contrast with the classic ``holistic'' approach, we show how to decompose the problem into smaller subproblems associated with the line and the stations. This decomposition is the basis for a master-slave solution algorithm, in which the master problem is associated with the line and the slave problem is associated with the stations. The two subproblems are modeled as mixed integer linear programs, with specific sets of variables and constraints. Similarly to the classical Benders' decomposition approach, slave and master communicate through suitable feasibility cuts in the variables of the master. Extensive tests on real-life instances from single and double-track lines in Italy showed significant improvements over current dispatching performances. A decision support system based on this exact approach has been in operation in Norway since February 2014 and represents one of the first operative applications of mathematical optimization to train dispatching.},
  keywords = {discrete mathematics,railway optimization,train dispatching},
  file = {/home/jeroen/Zotero/storage/K7SJ2Z57/Lamorgese and Mannino - 2015 - An Exact Decomposition Approach for the Real-Time .pdf}
}

@article{lamorgeseNoncompactFormulationJobShop2019,
  title = {A {{Noncompact Formulation}} for {{Job-Shop Scheduling Problems}} in {{Traffic Management}}},
  author = {Lamorgese, Leonardo and Mannino, Carlo},
  year = {2019},
  month = nov,
  journal = {Operations Research},
  volume = {67},
  number = {6},
  pages = {1586--1609},
  publisher = {INFORMS},
  issn = {0030-364X},
  doi = {10.1287/opre.2018.1837},
  urldate = {2023-10-12},
  abstract = {A central problem in traffic management is that of scheduling the movements of vehicles so as to minimize the cost of the schedule. It arises in important applications such as train timetabling, rescheduling, delay and disruption management, airplane surface routing, runway scheduling, air-traffic control, and more. This problem can be modeled as a job-shop scheduling problem. We introduce a new mixed-integer linear program (MILP) formulation for job-shop scheduling, which is an alternative to classical approaches, namely, big-M and time-indexed formulations. It does not make use of artificially large coefficients, and its constraints correspond to basic graph structures, such as paths, cycles, and trees. The new formulation can be obtained by strengthening and lifting the constraints of a classical Benders' reformulation. Tests on a large set of real-life instances from train rescheduling show that the new approach performs on average better than our previous approaches based on big-M formulations and particularly better on a class of instances with nonconvex costs very common in the practice.},
  keywords = {algorithms,Benders' decomposition,Integer programming,scheduling,Transportation,Transportation technology,transportation: scheduling vehicles},
  file = {/home/jeroen/Zotero/storage/HASPKTYU/Lamorgese and Mannino - 2019 - A Noncompact Formulation for Job-Shop Scheduling P.pdf}
}

@article{lamorgeseOptimalTrainDispatching2016,
  title = {Optimal {{Train Dispatching}} by {{Benders}}'-{{Like Reformulation}}},
  author = {Lamorgese, Leonardo and Mannino, Carlo and Piacentini, Mauro},
  year = {2016},
  month = aug,
  journal = {Transportation Science},
  volume = {50},
  number = {3},
  pages = {910--925},
  publisher = {INFORMS},
  issn = {0041-1655},
  doi = {10.1287/trsc.2015.0605},
  urldate = {2023-10-16},
  abstract = {Train movements on railway lines are generally controlled by human dispatchers. Because disruptions often occur, dispatchers make real-time scheduling and routing decisions in an attempt to minimize deviations from the official timetable. This optimization problem is called train dispatching. We represent it as a mixed integer linear programming model, and solve it with a Benders'-like decomposition within a suitable master/slave scheme. Interestingly, the master and the slave problems correspond to a macroscopic and microscopic representation of the railway, recently exploited in heuristic approaches to the problem. The decomposition, along with some new modeling ideas, allowed us to solve real-life instances of practical interest to optimality. Automatic dispatching systems based on our macro/micro decomposition---in which both master and slave are solved heuristically---have been in operation in several Italian lines since 2011. The exact approach described in this paper outperforms such systems on our test bed of real-life instances. Furthermore, a system based on another version of the exact decomposition approach has been in operation since February 2014 on a line in Norway.},
  keywords = {integer programming,logic Benders' decomposition,railway optimization},
  file = {/home/jeroen/Zotero/storage/9NLH7JB7/Lamorgese et al. - 2016 - Optimal Train Dispatching by Benders’-Like Reformu.pdf}
}

@misc{leBatchPolicyLearning2019,
  title = {Batch {{Policy Learning}} under {{Constraints}}},
  author = {Le, Hoang M. and Voloshin, Cameron and Yue, Yisong},
  year = {2019},
  month = mar,
  number = {arXiv:1903.08738},
  eprint = {1903.08738},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2022-12-13},
  abstract = {When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. To certify constraint satisfaction, we propose a new and simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/5XG8FNW3/Le et al. - 2019 - Batch Policy Learning under Constraints.pdf}
}

@book{lemmonOptimalControlEE,
  title = {Optimal {{Control}} ({{EE}} 60565)},
  author = {Lemmon, Michael},
  file = {/home/jeroen/Zotero/storage/92V95V63/module4.pdf;/home/jeroen/Zotero/storage/ACTNQXWJ/module2.pdf;/home/jeroen/Zotero/storage/JW2JTW4Z/module3.pdf;/home/jeroen/Zotero/storage/K4XJ3QUQ/module5.pdf;/home/jeroen/Zotero/storage/TXMGI93I/module1.pdf}
}

@incollection{lenstraComplexityMachineScheduling1977,
  title = {Complexity of {{Machine Scheduling Problems}}},
  booktitle = {Annals of {{Discrete Mathematics}}},
  author = {Lenstra, J. K. and Rinnooy Kan, A. H. G. and Brucker, P.},
  editor = {Hammer, P. L. and Johnson, E. L. and Korte, B. H. and Nemhauser, G. L.},
  year = {1977},
  month = jan,
  series = {Studies in {{Integer Programming}}},
  volume = {1},
  pages = {343--362},
  publisher = {Elsevier},
  doi = {10.1016/S0167-5060(08)70743-X},
  urldate = {2023-10-23},
  abstract = {We survey and extend the results on the complexity of machine scheduling problems. After a brief review of the central concept of NP-completeness we give a classification of scheduling problems on single, different and identical machines and study the influence of various parameters on their complexity. The problems for which a polynomial-bounded algorithm is available are listed and NP-completeness is established for a large number of other machine scheduling problems. We finally discuss some questions that remain unanswered.},
  file = {/home/jeroen/Zotero/storage/NPNFKNAU/Lenstra et al. - 1977 - Complexity of Machine Scheduling Problems.pdf;/home/jeroen/Zotero/storage/6NLB2NZ4/S016750600870743X.html}
}

@misc{levineOfflineReinforcementLearning2020,
  title = {Offline {{Reinforcement Learning}}: {{Tutorial}}, {{Review}}, and {{Perspectives}} on {{Open Problems}}},
  shorttitle = {Offline {{Reinforcement Learning}}},
  author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  year = {2020},
  month = nov,
  number = {arXiv:2005.01643},
  eprint = {2005.01643},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-12-08},
  abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/JQVHKDUG/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf}
}

@book{liberzonCalculusVariationsOptimal,
  title = {Calculus of {{Variations}} and {{Optimal Control Theory}}},
  author = {Liberzon, Daniel},
  file = {/home/jeroen/Zotero/storage/3YZVET2I/Calculus of Variations and Optimal Control Theory.pdf}
}

@article{liConnectedVehicleBasedTraffic2020,
  title = {Connected {{Vehicle-Based Traffic Signal Coordination}}},
  author = {Li, Wan and Ban, Xuegang},
  year = {2020},
  month = dec,
  journal = {Engineering},
  volume = {6},
  number = {12},
  pages = {1463--1472},
  issn = {2095-8099},
  doi = {10.1016/j.eng.2020.10.009},
  urldate = {2023-09-25},
  abstract = {This study presents a connected vehicles (CVs)-based traffic signal optimization framework for a coordinated arterial corridor. The signal optimization and coordination problem are first formulated in a centralized scheme as a mixed-integer nonlinear program (MINLP). The optimal phase durations and offsets are solved together by minimizing fuel consumption and travel time considering an individual vehicle's trajectories. Due to the complexity of the model, we decompose the problem into two levels: an intersection level to optimize phase durations using dynamic programming (DP), and a corridor level to optimize the offsets of all intersections. In order to solve the two-level model, a prediction-based solution technique is developed. The proposed models are tested using traffic simulation under various scenarios. Compared with the traditional actuated signal timing and coordination plan, the signal timing plans generated by solving the MINLP and the two-level model can reasonably improve the signal control performance. When considering varies vehicle types under high demand levels, the proposed two-level model reduced the total system cost by 3.8\% comparing to baseline actuated plan. MINLP reduced the system cost by 5.9\%. It also suggested that coordination scheme was beneficial to corridors with relatively high demand levels. For intersections with major and minor street, coordination conducted for major street had little impacts on the vehicles at the minor street.},
  keywords = {Connected vehicles,Dynamic programming,MILP,Mixed-integer nonlinear program,Traffic signal coordination,Two-level optimization},
  file = {/home/jeroen/Zotero/storage/2EAQX3JD/Li and Ban - 2020 - Connected Vehicle-Based Traffic Signal Coordinatio.pdf}
}

@article{liConnectedVehiclesBased2019,
  title = {Connected {{Vehicles Based Traffic Signal Timing Optimization}}},
  author = {Li, Wan and Ban, Xuegang},
  year = {2019},
  month = dec,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {20},
  number = {12},
  pages = {4354--4366},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2018.2883572},
  urldate = {2023-01-20},
  abstract = {We study the traffic signal control problem with connected vehicles by assuming a fixed cycle length so that the proposed model can be extended readily for the coordination of multiple signals. The problem can be first formulated as a mixed-integer nonlinear program, by considering the information of individual vehicle's trajectories (i.e., second-by-second vehicle locations and speeds) and their realistic driving/car-following behaviors. The objective function is to minimize the weighted sum of total fuel consumption and travel time. Due to the large dimension of the problem and the complexity of the nonlinear car-following model, solving the nonlinear program directly is challenging. We then reformulate the problem as a dynamic programming model by dividing the timing decisions into stages (one stage for a signal phase) and approximating the fuel consumption and travel time of a stage as functions of the state and decision variables of the stage. We also propose a twostep method to make sure that the obtained optimal solution can lead to the fixed cycle length. Numerical experiments are provided to test the performance of the proposed model using data generated by traffic simulation.},
  langid = {english},
  keywords = {MILP},
  file = {/home/jeroen/Zotero/storage/CG5SBAC6/Li and Ban - 2019 - Connected Vehicles Based Traffic Signal Timing Opt.pdf}
}

@article{lighthill1955a,
  title = {On Kinematic Waves {{II}}. {{A}} Theory of Traffic Flow on Long Crowded Roads},
  author = {Lighthill, M.J. and Whitham, G.B.},
  year = {1955},
  journal = {Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences},
  volume = {229},
  number = {1178},
  pages = {317--345},
  langid = {english}
}

@misc{liLearningOptimize2016,
  title = {Learning to {{Optimize}}},
  author = {Li, Ke and Malik, Jitendra},
  year = {2016},
  month = jun,
  number = {arXiv:1606.01885},
  eprint = {1606.01885},
  publisher = {arXiv},
  urldate = {2024-10-15},
  abstract = {Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/7AJDDB9Y/Li and Malik - 2016 - Learning to Optimize.pdf;/home/jeroen/Zotero/storage/BSCYYEBX/1606.html}
}

@misc{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  month = jul,
  number = {arXiv:1509.02971},
  eprint = {1509.02971},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1509.02971},
  urldate = {2023-01-20},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/AT28MPL2/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf;/home/jeroen/Zotero/storage/FNSFETWC/1509.html}
}

@phdthesis{limpensOnlinePlatoonForming2023,
  type = {Bachelor},
  title = {Online {{Platoon Forming Algorithms}} for Automated Vehicles: {{A}} More Efficient Approach},
  author = {Limpens, Matthijs},
  year = {2023},
  month = sep,
  school = {Eindhoven University of Technology}
}

@article{linSingleMachineScheduling2022,
  title = {Single Machine Scheduling Problems with Sequence-Dependent Setup Times and Precedence Delays},
  author = {Lin, Shih-Wei and Ying, Kuo-Ching},
  year = {2022},
  month = jun,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {9430},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-13278-y},
  urldate = {2023-10-20},
  abstract = {Sequence-dependent setup times and precedence delays occur frequently in various production environments. This study investigates the single machine scheduling problem with setup times and precedence delays that occur in an amplifier assembly company. This study proposes a novel mixed-integer linear programming model and a lean iterated greedy algorithm to minimize the makespan for this problem. Based on the property of delayed precedence constraints, the lean iterated greedy (LIG) algorithm uses a simple but effective lean construction mechanism that can discard infeasible solutions to reduce the waste of unnecessary searches and quickly converge to the (near) global optimum. The computational results show that LIG significantly outperforms the state-of-the-art algorithm in terms of solution quality and computational efficiency. This study mainly contributes to providing a simple, effective, and efficient algorithm that can facilitate industrial applications and serve as a new benchmark approach for future research.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Engineering,Mathematics and computing},
  file = {/home/jeroen/Zotero/storage/5X5QHDPC/Lin and Ying - 2022 - Single machine scheduling problems with sequence-d.pdf}
}

@article{liPOINTPartiallyObservable2022,
  title = {{{POINT}}: {{Partially Observable Imitation Network}} for {{Traffic Signal Control}}},
  shorttitle = {{{POINT}}},
  author = {Li, Wan and Wang, Boyu and Liu, Zhanlin and Li, Qiang and Qi, Guo-Jun},
  year = {2022},
  month = jan,
  journal = {Sustainable Cities and Society},
  volume = {76},
  pages = {103461},
  issn = {2210-6707},
  doi = {10.1016/j.scs.2021.103461},
  urldate = {2023-01-20},
  abstract = {Smart traffic signals bring together transportation infrastructure and advance technologies to improve the mobility and efficiency of urban transportation network. Adaptive traffic signal control studies can be categorized into modeling-based approaches and learning-based approaches. In order to take advantages of these two systems, this study developed an offline-online combined Partial Observable Imitation Network for Traffic signal control (POINT). In the offline system, the traffic signal timing optimization problem was formulated as a Mixed Integer Nonlinear Programming (MINLP) given complete traffic information, i.e., second-by-second speeds and locations of all vehicles. The objective of MINLP is to minimize total travel delays considering individual vehicle trajectories under Connected Vehicle (CV) environment. The calculated optimal solutions under various traffic conditions were considered as the ''expert'' decisions. In the online system, an imitation neural network model was developed to learn the ''expert'' signal plans generated from offline system. Given partial observable traffic conditions in real time, e.g., the aggregate-level of traffic volume, the POINT model can compute the signal timing parameters in the online system. The numerical results demonstrated that the proposed method outperformed other state-of-the-art signal control method under high and unbalanced traffic demand levels in terms of reducing travel delays and queue length.},
  langid = {english},
  keywords = {Adaptive traffic signal control system,Connected vehicle,Imitation network,immitation learning,MILP,Vehicle trajectories},
  file = {/home/jeroen/Zotero/storage/LXKSI7DN/Li et al. - 2022 - POINT Partially Observable Imitation Network for .pdf;/home/jeroen/Zotero/storage/LRPEMQ3X/S2210670721007344.html}
}

@article{liTemporalspatialDimensionExtensionbased2019,
  title = {Temporal-Spatial Dimension Extension-Based Intersection Control Formulation for Connected and Autonomous Vehicle Systems},
  author = {Li, Zhenning and Wu, Qiong and Yu, Hao and Chen, Cong and Zhang, Guohui and Tian, Zong Z. and Prevedouros, Panos D.},
  year = {2019},
  month = jul,
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {104},
  pages = {234--248},
  issn = {0968-090X},
  doi = {10.1016/j.trc.2019.05.003},
  urldate = {2024-01-26},
  abstract = {Traffic congestion has become a serious issue worldwide due to the rapid increase in population and traffic demands. Advances in connected automated vehicle (CAV) technology demonstrate the potential to improve traffic mobility and safety performance at intersections. An advanced intersection control system is proposed in this study to coordinate vehicle trajectories and ensure safety and operation efficiency at intersections. A temporal-spatial dimension extension-based trajectory coordination model is developed by formulating all possible trajectories of vehicles at the intersection. Correspondingly, according to the trajectory coordination model, two signal-free control algorithms, including the priority-based algorithm and the Discrete Forward-Rolling Optimal Control (DFROC) algorithm are proposed in this study to manage vehicles at the intersection. These two algorithms, together with the FCFS policy, are compared with the conventional signal control method in a SUMO-based simulation platform. Experimental results indicate that the proposed algorithms outperform the signal control method in terms of reducing total traffic delays at intersections and increasing intersection capacity and operation efficiency.},
  keywords = {Connected automated vehicles,Intersection management,Tabu search,Vehicle trajectories},
  file = {/home/jeroen/Zotero/storage/LPAJRKJK/S0968090X18305436.html}
}

@article{liTemporalspatialDimensionExtensionbased2019a,
  title = {Temporal-Spatial Dimension Extension-Based Intersection Control Formulation for Connected and Autonomous Vehicle Systems},
  author = {Li, Zhenning and Wu, Qiong and Yu, Hao and Chen, Cong and Zhang, Guohui and Tian, Zong Z. and Prevedouros, Panos D.},
  year = {2019},
  month = jul,
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {104},
  pages = {234--248},
  issn = {0968-090X},
  doi = {10.1016/j.trc.2019.05.003},
  urldate = {2024-11-25},
  abstract = {Traffic congestion has become a serious issue worldwide due to the rapid increase in population and traffic demands. Advances in connected automated vehicle (CAV) technology demonstrate the potential to improve traffic mobility and safety performance at intersections. An advanced intersection control system is proposed in this study to coordinate vehicle trajectories and ensure safety and operation efficiency at intersections. A temporal-spatial dimension extension-based trajectory coordination model is developed by formulating all possible trajectories of vehicles at the intersection. Correspondingly, according to the trajectory coordination model, two signal-free control algorithms, including the priority-based algorithm and the Discrete Forward-Rolling Optimal Control (DFROC) algorithm are proposed in this study to manage vehicles at the intersection. These two algorithms, together with the FCFS policy, are compared with the conventional signal control method in a SUMO-based simulation platform. Experimental results indicate that the proposed algorithms outperform the signal control method in terms of reducing total traffic delays at intersections and increasing intersection capacity and operation efficiency.},
  keywords = {Connected automated vehicles,Intersection management,Tabu search,Vehicle trajectories},
  file = {/home/jeroen/Zotero/storage/9BEHFXFM/Li et al. - 2019 - Temporal-spatial dimension extension-based intersection control formulation for connected and autono.pdf}
}

@article{littleVersatileProgramSetting,
  title = {A {{Versatile Program}} for {{Setting Signals}} on {{Arteries}} and {{Triangular Networks}}},
  author = {Little, John D C and Kelson, Mark D and Gartner, Nathan H},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/5UZLGK52/Little et al. - A Versatile Program for Setting Signals on Arterie.pdf}
}

@article{liuOptimalPollingPolicies1992,
  title = {On Optimal Polling Policies},
  author = {Liu, Zhen and Nain, Philippe and Towsley, Don},
  year = {1992},
  month = mar,
  journal = {Queueing Systems},
  volume = {11},
  number = {1},
  pages = {59--83},
  issn = {1572-9443},
  doi = {10.1007/BF01159287},
  urldate = {2024-02-28},
  abstract = {In a single-server polling system, the server visits the queues according to a routing policy and while at a queue, serves some or all of the customers there according to a service policy. A polling (or scheduling) policy is a sequence of decisions on whether to serve a customer, idle the server, or switch the server to another queue. The goal of this paper is to find polling policies that stochastically minimize the unfinished work and the number of customers in the system at all times. This optimization problem is decomposed into three subproblems: determine the optimal action (i.e., serve, switch, idle) when the server is at a nonempty queue; determine the optimal action (i.e., switch, idle) when the server empties a queue; determine the optimal routing (i.e., choice of the queue) when the server decides to switch. Under fairly general assumptions, we show for the first subproblem that optimal policies are greedy and exhaustive, i.e., the server should neither idle nor switch when it is at a nonempty queue. For the second subproblem, we prove that in symmetric polling systems patient policies are optimal, i.e., the server should stay idling at the last visited queue whenever the system is empty. When the system is slotted, we further prove that non-idling and impatient policies are optimal. For the third subproblem, we establish that in symmetric polling systems optimal policies belong to the class of Stochastically Largest Queue (SLQ) policies. An SLQ policy is one that never routes the server to a queue known to have a queue length that is stochastically smaller than that of another queue. This result implies, in particular, that the policy that routes the server to the queue with the largest queue length is optimal when all queue lengths are known and that the cyclic routing policy is optimal in the case that the only information available is the previous decisions.},
  langid = {english},
  keywords = {coupling,optimal stochastic scheduling,polling policy,Polling system,stochastic ordering},
  file = {/home/jeroen/Zotero/storage/WL8NE5IR/Liu et al. - 1992 - On optimal polling policies.pdf}
}

@inproceedings{liuPolicyLearningConstraints2021,
  title = {Policy {{Learning}} with {{Constraints}} in {{Model-free Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Policy {{Learning}} with {{Constraints}} in {{Model-free Reinforcement Learning}}},
  booktitle = {Proceedings of the {{Thirtieth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Liu, Yongshuai and Halev, Avishai and Liu, Xin},
  year = {2021},
  month = aug,
  pages = {4508--4515},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Montreal, Canada},
  doi = {10.24963/ijcai.2021/614},
  urldate = {2022-12-08},
  abstract = {Reinforcement Learning (RL) algorithms have had tremendous success in simulated domains. These algorithms, however, often cannot be directly applied to physical systems, especially in cases where there are constraints to satisfy (e.g. to ensure safety or limit resource consumption). In standard RL, the agent is incentivized to explore any policy with the sole goal of maximizing reward; in the real world, however, ensuring satisfaction of certain constraints in the process is also necessary and essential. In this article, we overview existing approaches addressing constraints in model-free reinforcement learning. We model the problem of learning with constraints as a Constrained Markov Decision Process and consider two main types of constraints: cumulative and instantaneous. We summarize existing approaches and discuss their pros and cons. To evaluate policy performance under constraints, we introduce a set of standard benchmarks and metrics. We also summarize limitations of current methods and present open questions for future research.},
  isbn = {978-0-9992411-9-6},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/NLYBQNWC/Liu et al. - 2021 - Policy Learning with Constraints in Model-free Rei.pdf}
}

@book{locatelliOptimalControlDouble2017,
  title = {Optimal {{Control}} of a {{Double Integrator}}},
  author = {Locatelli, Arturo},
  year = {2017},
  series = {Studies in {{Systems}}, {{Decision}} and {{Control}}},
  volume = {68},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-42126-1},
  urldate = {2025-05-27},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-319-42125-4 978-3-319-42126-1},
  keywords = {Continuous-time Dynamical Systems,Finite-dimensional Dynamical Systems,First Order Variational Methods,Integral Constraints,Optimal Control Theory,Pontryagin Maximum Principle,Punctual Global Constraints,Punctual Isolated Constrains,Singular Arcs,Time-invariant Dynamical System},
  file = {/home/jeroen/Zotero/storage/APUHTMBL/Locatelli - 2017 - Optimal Control of a Double Integrator.pdf}
}

@article{lodiLearningBranchingSurvey2017,
  title = {On Learning and Branching: A Survey},
  shorttitle = {On Learning and Branching},
  author = {Lodi, Andrea and Zarpellon, Giulia},
  year = {2017},
  month = jul,
  journal = {TOP},
  volume = {25},
  number = {2},
  pages = {207--236},
  issn = {1863-8279},
  doi = {10.1007/s11750-017-0451-6},
  urldate = {2024-11-09},
  abstract = {This paper surveys learning techniques to deal with the two most crucial decisions in the branch-and-bound algorithm for Mixed-Integer Linear Programming, namely variable and node selections. Because of the lack of deep mathematical understanding on those decisions, the classical and vast literature in the field is inherently based on computational studies and heuristic, often problem-specific, strategies. We will both interpret some of those early contributions in the light of modern (machine) learning techniques, and give the details of the recent algorithms that instead explicitly incorporate machine learning paradigms.},
  langid = {english},
  keywords = {68R-02,68T-02,90-02,Branch and bound,Machine learning},
  file = {/home/jeroen/Zotero/storage/QBSDUE55/Lodi and Zarpellon - 2017 - On learning and branching a survey.pdf}
}

@inproceedings{lopezMicroscopicTrafficSimulation2018,
  title = {Microscopic {{Traffic Simulation}} Using {{SUMO}}},
  booktitle = {2018 21st {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Lopez, Pablo Alvarez and Wiessner, Evamarie and Behrisch, Michael and {Bieker-Walz}, Laura and Erdmann, Jakob and Flotterod, Yun-Pang and Hilbrich, Robert and Lucken, Leonhard and Rummel, Johannes and Wagner, Peter},
  year = {2018},
  month = nov,
  pages = {2575--2582},
  publisher = {IEEE},
  address = {Maui, HI},
  doi = {10.1109/ITSC.2018.8569938},
  urldate = {2023-01-10},
  isbn = {978-1-7281-0321-1 978-1-7281-0323-5},
  file = {/home/jeroen/Zotero/storage/KV3FY2AH/Lopez et al. - 2018 - Microscopic Traffic Simulation using SUMO.pdf}
}

@book{luenbergerOptimizationVectorSpace1969,
  title = {Optimization by Vector Space Methods},
  author = {Luenberger, David G.},
  year = {1969},
  series = {Series in Decision and Control},
  publisher = {J. Wiley},
  address = {New York},
  isbn = {978-0-471-18117-0},
  langid = {english},
  lccn = {519.6},
  keywords = {optimization},
  file = {/home/jeroen/Zotero/storage/EQHPYZ3M/Luenberger - 1969 - Optimization by vector space methods.pdf}
}

@book{mackiIntroductionOptimalControl1982,
  title = {Introduction to {{Optimal Control Theory}}},
  author = {Macki, Jack and Strauss, Aaron},
  editor = {Ewing, J. H. and Gehring, F. W. and Halmos, P. R.},
  year = {1982},
  series = {Undergraduate {{Texts}} in {{Mathematics}}},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-5671-7},
  urldate = {2025-02-02},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-1-4612-5673-1 978-1-4612-5671-7},
  keywords = {control,control theory,equation,mathematics,Optimal control,Optimale Regelung,Pontryagin-Type,proof,theorem},
  file = {/home/jeroen/Zotero/storage/JSTU5E74/Macki and Strauss - 1982 - Introduction to Optimal Control Theory.pdf}
}

@article{manninoPathCycleFormulation2018,
  title = {The {{Path}}\&{{Cycle Formulation}} for the {{Hotspot Problem}} in {{Air Traffic Management}}},
  author = {Mannino, Carlo and Sartor, Giorgio},
  year = {2018},
  pages = {11 pages},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany},
  doi = {10.4230/OASICS.ATMOS.2018.14},
  urldate = {2023-10-12},
  abstract = {The Hotspot Problem in Air Traffic Management consists of optimally rescheduling a set of airplanes that are forecast to occupy an overcrowded region of the airspace, should they follow their original schedule. We first provide a MILP model for the Hotspot Problem using a standard big-M formulation. Then, we present a novel MILP model that gets rid of the big-M coefficients. The new formulation contains only simple combinatorial constraints, corresponding to paths and cycles in an associated disjunctive graph. We report computational results on a set of randomly generated instances. In the experiments, the new formulation consistently outperforms the big-M formulation, both in terms of running times and number of branching nodes.},
  collaborator = {Wagner, Michael},
  copyright = {Creative Commons Attribution 3.0 Unported license (CC-BY 3.0)},
  langid = {english},
  keywords = {000 Computer science knowledge general works,Computer Science},
  file = {/home/jeroen/Zotero/storage/PKMK754N/Mannino and Sartor - 2018 - The Path&amp;Cycle Formulation for the Hotspot Pro.pdf}
}

@article{marcucciGraphsConvexSets,
  title = {Graphs of {{Convex Sets}} with {{Applications}} to {{Optimal Control}} and {{Motion Planning}}},
  author = {Marcucci, Tobia},
  abstract = {This thesis introduces a new class of problems at the interface of combinatorial and convex optimization. We consider graphs where each vertex is paired with a convex program, and each edge couples two programs through additional convex costs and constraints. We call such a graph a Graph of Convex Sets (GCS). Over a GCS we can formulate any optimization problem that we can formulate over an ordinary weighted graph, with scalar costs on the vertices and edges. In fact, for any fixed choice of the variables in the convex programs, a GCS reduces to a weighted graph where we can seek, e.g., a path, a matching, a tour, or a spanning tree of minimum cost. The challenge in a GCS problem lies in solving the discrete and the continuous components of the problem jointly.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/G46RPYTY/Marcucci - Graphs of Convex Sets with Applications to Optimal Control and Motion Planning.pdf}
}

@misc{marcucciShortestPathsGraphs2023,
  title = {Shortest {{Paths}} in {{Graphs}} of {{Convex Sets}}},
  author = {Marcucci, Tobia and Umenberger, Jack and Parrilo, Pablo A. and Tedrake, Russ},
  year = {2023},
  month = jul,
  number = {arXiv:2101.11565},
  eprint = {2101.11565},
  publisher = {arXiv},
  urldate = {2024-10-15},
  abstract = {Given a graph, the shortest-path problem requires finding a sequence of edges with minimum cumulative length that connects a source vertex to a target vertex. We consider a variant of this classical problem in which the position of each vertex in the graph is a continuous decision variable constrained in a convex set, and the length of an edge is a convex function of the position of its endpoints. Problems of this form arise naturally in many areas, from motion planning of autonomous vehicles to optimal control of hybrid systems. The price for such a wide applicability is the complexity of this problem, which is easily seen to be NP-hard. Our main contribution is a strong and lightweight mixed-integer convex formulation based on perspective operators, that makes it possible to efficiently find globally optimal paths in large graphs and in high-dimensional spaces.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Discrete Mathematics,Mathematics - Optimization and Control},
  file = {/home/jeroen/Zotero/storage/BNE7P5KH/Marcucci et al. - 2023 - Shortest Paths in Graphs of Convex Sets.pdf;/home/jeroen/Zotero/storage/M2T63AEX/2101.html}
}

@article{marianiCoordinationAutonomousVehicles2022,
  title = {Coordination of {{Autonomous Vehicles}}: {{Taxonomy}} and {{Survey}}},
  shorttitle = {Coordination of {{Autonomous Vehicles}}},
  author = {Mariani, Stefano and Cabri, Giacomo and Zambonelli, Franco},
  year = {2022},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {1},
  pages = {1--33},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3431231},
  urldate = {2023-09-24},
  abstract = {In the near future, our streets will be populated by myriads of autonomous self-driving vehicles to serve our diverse mobility needs. This will raise the need to coordinate their movements in order to properly handle both access to shared resources (e.g., intersections and parking slots) and the execution of mobility tasks (e.g., platooning and ramp merging). The aim of this article is to provide a global view of the coordination issues and the related solutions in the field of autonomous vehicles. To this end, we firstly introduce the general problems associated with coordination of autonomous vehicles by identifying and framing the key classes of coordination problems. Then, we overview the different approaches that can be adopted to deal with such problems by classifying them in terms of the degree of autonomy in decision making that is left to autonomous vehicles during the coordination process. Finally, we overview some further research challenges to address before autonomous coordinated vehicles can safely hit our streets.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/TLS9NWCD/Mariani et al. - 2022 - Coordination of Autonomous Vehicles Taxonomy and .pdf}
}

@article{mascisJobshopSchedulingBlocking2002,
  title = {Job-Shop Scheduling with Blocking and No-Wait Constraints},
  author = {Mascis, Alessandro and Pacciarelli, Dario},
  year = {2002},
  month = dec,
  journal = {European Journal of Operational Research},
  volume = {143},
  number = {3},
  pages = {498--517},
  issn = {0377-2217},
  doi = {10.1016/S0377-2217(01)00338-1},
  urldate = {2024-11-15},
  abstract = {In this paper, we study the job-shop scheduling problem with blocking and/or no-wait constraints. A blocking constraint models the absence of storage capacity between machines, while a no-wait constraint occurs when two consecutive operations in a job must be processed without any interruption. We formulate the problem by means of a generalization of the disjunctive graph of Roy and Sussman, that we call an alternative graph, and investigate the applicability to the blocking and no-wait cases of some of the most effective ideas from the literature on the job shop with unlimited buffers. We show that several key properties, used to design heuristic procedures, do not hold in the blocking and no-wait cases, while some of the most effective ideas used to develop branch and bound algorithms can be easily extended. We presents several complexity results and solution procedures. Computational results for fast heuristics and exact algorithms are also reported.},
  keywords = {Blocking,Job shop,No-wait,Scheduling},
  file = {/home/jeroen/Zotero/storage/NQ2NDJWB/Mascis and Pacciarelli - 2002 - Job-shop scheduling with blocking and no-wait constraints.pdf;/home/jeroen/Zotero/storage/3Y4AF73X/S0377221701003381.html}
}

@article{maurerTutorialControlState,
  title = {Tutorial on {{Control}} and {{State Constrained Optimal Control Problems}} -- {{PART I}} : {{Examples}}},
  author = {Maurer, Helmut},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/MTX3ZC9Z/Maurer - Tutorial on Control and State Constrained Optimal Control Problems – PART I  Examples.pdf}
}

@article{maurerTutorialControlStatea,
  title = {Tutorial on {{Control}} and {{State Constrained Optimal Control Problems}} and {{Applications}} -- {{Part}} 3 : {{Pure State Constraints}}},
  author = {Maurer, Helmut},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/TE4KFGCN/Maurer - Tutorial on Control and State Constrained Optimal Control Problems and Applications – Part 3  Pure.pdf}
}

@misc{mazyavkinaReinforcementLearningCombinatorial2020,
  title = {Reinforcement {{Learning}} for {{Combinatorial Optimization}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning}} for {{Combinatorial Optimization}}},
  author = {Mazyavkina, Nina and Sviridov, Sergey and Ivanov, Sergei and Burnaev, Evgeny},
  year = {2020},
  month = dec,
  number = {arXiv:2003.03600},
  eprint = {2003.03600},
  publisher = {arXiv},
  urldate = {2024-11-13},
  abstract = {Many traditional algorithms for solving combinatorial optimization problems involve using hand-crafted heuristics that sequentially construct a solution. Such heuristics are designed by domain experts and may often be suboptimal due to the hard nature of the problems. Reinforcement learning (RL) proposes a good alternative to automate the search of these heuristics by training an agent in a supervised or self-supervised manner. In this survey, we explore the recent advancements of applying RL frameworks to hard combinatorial problems. Our survey provides the necessary background for operations research and machine learning communities and showcases the works that are moving the field forward. We juxtapose recently proposed RL methods, laying out the timeline of the improvements for each problem, as well as we make a comparison with traditional algorithms, indicating that RL models can become a promising direction for solving combinatorial problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Combinatorics,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/9Z8WVIRZ/Mazyavkina et al. - 2020 - Reinforcement Learning for Combinatorial Optimization A Survey.pdf}
}

@book{mcshaneTrafficEngineering1990,
  title = {Traffic Engineering},
  author = {McShane, William R. and Roess, Roger P.},
  year = {1990},
  series = {Prentice {{Hall}} Polytechnic Series in Traffic Engineering},
  publisher = {Prentice-Hall},
  address = {Englewood Cliffs, N.J},
  isbn = {978-0-13-926148-0},
  lccn = {HE355 .M43 1990},
  keywords = {Traffic engineering,United States}
}

@misc{miculescuPollingsystemsbasedAutonomousVehicle2016,
  title = {Polling-Systems-Based {{Autonomous Vehicle Coordination}} in {{Traffic Intersections}} with {{No Traffic Signals}}},
  author = {Miculescu, David and Karaman, Sertac},
  year = {2016},
  month = jul,
  number = {arXiv:1607.07896},
  eprint = {1607.07896},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2023-12-05},
  abstract = {The rapid development of autonomous vehicles spurred a careful investigation of the potential benefits of all-autonomous transportation networks. Most studies conclude that autonomous systems can enable drastic improvements in performance. A widely studied concept is all-autonomous, collision-free intersections, where vehicles arriving in a traffic intersection with no traffic light adjust their speeds to cross safely through the intersection as quickly as possible. In this paper, we propose a coordination control algorithm for this problem, assuming stochastic models for the arrival times of the vehicles. The proposed algorithm provides provable guarantees on safety and performance. More precisely, it is shown that no collisions occur surely, and moreover a rigorous upper bound is provided for the expected wait time. The algorithm is also demonstrated in simulations. The proposed algorithms are inspired by polling systems. In fact, the problem studied in this paper leads to a new polling system where customers are subject to differential constraints, which may be interesting in its own right.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  file = {/home/jeroen/Zotero/storage/IPVPFI5B/Miculescu and Karaman - 2016 - Polling-systems-based Autonomous Vehicle Coordinat.pdf;/home/jeroen/Zotero/storage/7VMJSZ9T/1607.html}
}

@misc{minHardConstrainedNeuralNetworks2024,
  title = {Hard-{{Constrained Neural Networks}} with {{Universal Approximation Guarantees}}},
  author = {Min, Youngjae and Sonar, Anoopkumar and Azizan, Navid},
  year = {2024},
  month = oct,
  number = {arXiv:2410.10807},
  eprint = {2410.10807},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10807},
  urldate = {2024-11-13},
  abstract = {Incorporating prior knowledge or specifications of input-output relationships into machine learning models has gained significant attention, as it enhances generalization from limited data and leads to conforming outputs. However, most existing approaches use soft constraints by penalizing violations through regularization, which offers no guarantee of constraint satisfaction -- an essential requirement in safety-critical applications. On the other hand, imposing hard constraints on neural networks may hinder their representational power, adversely affecting performance. To address this, we propose HardNet, a practical framework for constructing neural networks that inherently satisfy hard constraints without sacrificing model capacity. Specifically, we encode affine and convex hard constraints, dependent on both inputs and outputs, by appending a differentiable projection layer to the network's output. This architecture allows unconstrained optimization of the network parameters using standard algorithms while ensuring constraint satisfaction by construction. Furthermore, we show that HardNet retains the universal approximation capabilities of neural networks. We demonstrate the versatility and effectiveness of HardNet across various applications: fitting functions under constraints, learning optimization solvers, optimizing control policies in safety-critical systems, and learning safe decision logic for aircraft systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/Q379HGR8/Min et al. - 2024 - Hard-Constrained Neural Networks with Universal Approximation Guarantees.pdf;/home/jeroen/Zotero/storage/WRZS7MFY/2410.html}
}

@misc{miryoosefiSimpleRewardfreeApproach2021,
  title = {A {{Simple Reward-free Approach}} to {{Constrained Reinforcement Learning}}},
  author = {Miryoosefi, Sobhan and Jin, Chi},
  year = {2021},
  month = jul,
  number = {arXiv:2107.05216},
  eprint = {2107.05216},
  publisher = {arXiv},
  urldate = {2024-11-15},
  abstract = {In constrained reinforcement learning (RL), a learning agent seeks to not only optimize the overall reward but also satisfy the additional safety, diversity, or budget constraints. Consequently, existing constrained RL solutions require several new algorithmic ingredients that are notably different from standard RL. On the other hand, reward-free RL is independently developed in the unconstrained literature, which learns the transition dynamics without using the reward information, and thus naturally capable of addressing RL with multiple objectives under the common dynamics. This paper bridges reward-free RL and constrained RL. Particularly, we propose a simple meta-algorithm such that given any reward-free RL oracle, the approachability and constrained RL problems can be directly solved with negligible overheads in sample complexity. Utilizing the existing reward-free RL solvers, our framework provides sharp sample complexity results for constrained RL in the tabular MDP setting, matching the best existing results up to a factor of horizon dependence; our framework directly extends to a setting of tabular two-player Markov games, and gives a new result for constrained RL with linear function approximation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/67EVUVNW/Miryoosefi and Jin - 2021 - A Simple Reward-free Approach to Constrained Reinforcement Learning.pdf;/home/jeroen/Zotero/storage/7WL3FMKA/2107.html}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2024-01-16},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science},
  file = {/home/jeroen/Zotero/storage/5HYMQIMI/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf}
}

@misc{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  number = {arXiv:1312.5602},
  eprint = {1312.5602},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-01-10},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/9KEM54CE/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/jeroen/Zotero/storage/THQC2DX6/1312.html}
}

@misc{moldovanSafeExplorationMarkov2012,
  title = {Safe {{Exploration}} in {{Markov Decision Processes}}},
  author = {Moldovan, Teodor Mihai and Abbeel, Pieter},
  year = {2012},
  month = jul,
  number = {arXiv:1205.4810},
  eprint = {1205.4810},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-12-08},
  abstract = {In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/MBKV6JAT/Moldovan and Abbeel - 2012 - Safe Exploration in Markov Decision Processes.pdf}
}

@article{monchDistributedShiftingBottleneck2005,
  title = {A Distributed Shifting Bottleneck Heuristic for Complex Job Shops},
  author = {M{\"o}nch, Lars and Drie{\ss}el, Ren{\'e}},
  year = {2005},
  month = nov,
  journal = {Computers \& Industrial Engineering},
  volume = {49},
  number = {3},
  pages = {363--380},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2005.06.004},
  urldate = {2023-10-12},
  abstract = {In this paper, we consider distributed versions of a modified shifting bottleneck heuristic for complex job shops. The considered job shop environment contains parallel batching machines, machines with sequence-dependent setup times and reentrant process flows. Semiconductor wafer fabrication facilities are typical examples for manufacturing systems with these characteristics. The used performance measure is total weighted tardiness (TWT). We suggest a two-layer hierarchical approach in order to decompose the overall scheduling problem. The upper (or top) layer works on an aggregated model. Based on appropriately aggregated routes it determines start dates and planned due dates for the jobs within each single work area, where a work area is defined as a set of parallel machine groups. The lower (or base) layer uses the start dates and planned due dates in order to apply shifting bottleneck heuristic type solution approaches for the jobs in each single work area. We conduct simulation experiments in a dynamic job shop environment in order to assess the performance of the heuristic. It turns out that the suggested approach outperforms a pure First In First Out (FIFO) dispatching scheme and provides a similar solution quality as the original modified shifting bottleneck heuristic.},
  keywords = {Complex job shops,Computational experiments,Distributed scheduling,Hierarchical production control,Shifting bottleneck heuristic},
  file = {/home/jeroen/Zotero/storage/EESQXWLU/Mönch and Drießel - 2005 - A distributed shifting bottleneck heuristic for co.pdf;/home/jeroen/Zotero/storage/RPCP6CZG/S0360835205000793.html}
}

@article{morimuraNonparametricReturnDistribution,
  title = {Nonparametric {{Return Distribution Approximation}}  for {{Reinforcement Learning}}},
  author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
  pages = {8},
  abstract = {Standard Reinforcement Learning (RL) aims to optimize decision-making rules in terms of the expected return. However, especially for risk-management purposes, other criteria such as the expected shortfall are sometimes preferred. Here, we describe a method of approximating the distribution of returns, which allows us to derive various kinds of information about the returns. We first show that the Bellman equation, which is a recursive formula for the expected return, can be extended to the cumulative return distribution. Then we derive a nonparametric return distribution estimator with particle smoothing based on this extended Bellman equation. A key aspect of the proposed algorithm is to represent the recursion relation in the extended Bellman equation by a simple replacement procedure of particles associated with a state by using those of the successor state. We show that our algorithm leads to a risksensitive RL paradigm. The usefulness of the proposed approach is demonstrated through numerical experiments.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/ZF9RBJ2V/Morimura et al. - Nonparametric Return Distribution Approximation  f.pdf}
}

@misc{morimuraParametricReturnDensity2012,
  title = {Parametric {{Return Density Estimation}} for {{Reinforcement Learning}}},
  author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
  year = {2012},
  month = mar,
  number = {arXiv:1203.3497},
  eprint = {1203.3497},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-12-09},
  abstract = {Most conventional Reinforcement Learning (RL) algorithms aim to optimize decision-making rules in terms of the expected returns. However, especially for risk management purposes, other risk-sensitive criteria such as the value-at-risk or the expected shortfall are sometimes preferred in real applications. Here, we describe a parametric method for estimating density of the returns, which allows us to handle various criteria in a unified manner. We first extend the Bellman equation for the conditional expected return to cover a conditional probability density of the returns. Then we derive an extension of the TD-learning algorithm for estimating the return densities in an unknown environment. As test instances, several parametric density estimation algorithms are presented for the Gaussian, Laplace, and skewed Laplace distributions. We show that these algorithms lead to risk-sensitive as well as robust RL paradigms through numerical experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/RABI4SWB/Morimura et al. - 2012 - Parametric Return Density Estimation for Reinforce.pdf;/home/jeroen/Zotero/storage/YFDU5G5F/1203.html}
}

@misc{MultiAgentReinforcementLearning,
  title = {Multi-{{Agent Reinforcement Learning}}: {{Foundations}} and {{Modern Approaches}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  urldate = {2023-09-27},
  abstract = {Textbook published by MIT Press},
  howpublished = {https://www.marl-book.com/},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/HXKEIUWE/www.marl-book.com.html}
}

@book{MultiAgentReinforcementLearninga,
  title = {Multi-{{Agent Reinforcement Learning}}: {{Foundations}} and {{Modern Approaches}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  urldate = {2023-09-25},
  abstract = {Textbook published by MIT Press},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/W2JIF25C/Multi-Agent Reinforcement Learning Foundations and Modern Approaches.pdf;/home/jeroen/Zotero/storage/6787IHI8/www.marl-book.com.html}
}

@inproceedings{murgovskiConvexModelingConflict2015,
  title = {Convex Modeling of Conflict Resolution at Traffic Intersections},
  booktitle = {2015 54th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Murgovski, Nikolce and De Campos, Gabriel Rodrigues and Sjoberg, Jonas},
  year = {2015},
  month = dec,
  pages = {4708--4713},
  publisher = {IEEE},
  address = {Osaka},
  doi = {10.1109/CDC.2015.7402953},
  urldate = {2024-11-25},
  abstract = {We stud the problem of optimally controlling autonomous vehicles to safely cross an intersection. The problem is approached by solving an optimal control subproblem for all permutations of crossing sequences. For a chosen crossing sequence, we show that the subproblem of optimal longitudinal vehicle control, subject to collision avoidance constraints, can be formulated as a convex program. The proposed method transforms the problem from the original time domain to a space domain, and introduces a change of optimization variables by replacing vehicles' speed with its inverse. A case study is provided showing the effectiveness of the proposed method.},
  isbn = {978-1-4799-7886-1},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/N9W5YGZ2/Murgovski et al. - 2015 - Convex modeling of conflict resolution at traffic intersections.pdf}
}

@article{nagelCellularAutomatonModel1992,
  title = {A Cellular Automaton Model for Freeway Traffic},
  author = {Nagel, Kai and Schreckenberg, Michael},
  year = {1992},
  month = dec,
  journal = {Journal de Physique I},
  volume = {2},
  number = {12},
  pages = {2221--2229},
  issn = {1155-4304, 1286-4862},
  doi = {10.1051/jp1:1992277},
  urldate = {2023-01-10}
}

@misc{nicholPointESystemGenerating2022,
  title = {Point-{{E}}: {{A System}} for {{Generating 3D Point Clouds}} from {{Complex Prompts}}},
  shorttitle = {Point-{{E}}},
  author = {Nichol, Alex and Jun, Heewoo and Dhariwal, Prafulla and Mishkin, Pamela and Chen, Mark},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08751},
  eprint = {2212.08751},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-02-18},
  abstract = {While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/EGY8QG6C/Nichol et al. - 2022 - Point-E A System for Generating 3D Point Clouds f.pdf;/home/jeroen/Zotero/storage/ANINQA7V/2212.html}
}

@article{noaeenReinforcementLearningUrban2022,
  title = {Reinforcement Learning in Urban Network Traffic Signal Control: {{A}} Systematic Literature Review},
  shorttitle = {Reinforcement Learning in Urban Network Traffic Signal Control},
  author = {Noaeen, Mohammad and Naik, Atharva and Goodman, Liana and Crebo, Jared and Abrar, Taimoor and Abad, Zahra Shakeri Hossein and Bazzan, Ana L.C. and Far, Behrouz},
  year = {2022},
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {199},
  pages = {116830},
  issn = {09574174},
  doi = {10.1016/j.eswa.2022.116830},
  urldate = {2023-01-20},
  abstract = {Improvement of traffic signal control (TSC) efficiency has been found to lead to improved urban transportation and enhanced quality of life. Recently, the use of reinforcement learning (RL) in various areas of TSC has gained significant traction; thus, we conducted a systematic literature review as a systematic, comprehensive, and reproducible review to dissect all the existing research that applied RL in the network-level TSC domain, called as RL in NTSC or RL-NTSC for brevity. The review only targeted the network-level articles that tested the proposed methods in networks with two or more intersections. This review covers 160 peer-reviewed articles from 30 countries published from 1994 to March 2020. The goal of this study is to provide the research community with statistical and conceptual knowledge, summarize existence evidence, characterize RL applications in NTSC domains, explore all applied methods and major first events in the defined scope, and identify areas for further research based on the explored research problems in current research. We analyzed the extracted data from the included articles in the following seven categories: (i) publication and authors' data, (ii) method identification and analysis, (iii) environment attributes and traffic simulation, (iv) application domains of RL-NTSC, (v) major first events of RL-NTSC and authors' key statements, (vi) code availability, and (vii) evaluation. This paper provides a comprehensive view of the past 26 years of research on applying RL to NTSC. It also reveals the role of advancing deep learning methods in the revival of the research area, the rise of using non-commercial microscopic traffic simulators, a lack of interaction between traffic and transportation engineering practitioners and researchers, and a lack of proposal and creation of testbeds which can likely bring different communities together around common goals.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/37SRIMR9/Noaeen et al. - 2022 - Reinforcement learning in urban network traffic si.pdf}
}

@phdthesis{oblakovaQueueingModelsUrban2019,
  title = {Queueing Models for Urban Traffic Networks},
  author = {Oblakova, A.},
  year = {2019},
  month = sep,
  address = {Enschede, The Netherlands},
  doi = {10.3990/1.9789036548472},
  urldate = {2023-01-20},
  isbn = {9789036548472},
  langid = {english},
  school = {University of Twente},
  file = {/home/jeroen/Zotero/storage/KSB4JAIW/Oblakova - 2019 - Queueing models for urban traffic networks.pdf}
}

@book{oliehoekConciseIntroductionDecentralized2016,
  title = {A {{Concise Introduction}} to {{Decentralized POMDPs}}},
  author = {Oliehoek, Frans A. and Amato, Christopher},
  year = {2016},
  series = {{{SpringerBriefs}} in {{Intelligent Systems}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-28929-8},
  urldate = {2023-09-26},
  isbn = {978-3-319-28927-4 978-3-319-28929-8},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/AVTJKWEE/Oliehoek and Amato - 2016 - A Concise Introduction to Decentralized POMDPs.pdf}
}

@misc{ortegaThompsonSamplingBayesian2010,
  title = {Thompson {{Sampling}} \& {{Bayesian Control Rule}}},
  author = {Ortega, P.A.},
  year = {2010}
}

@article{ouelhadjSurveyDynamicScheduling2009,
  title = {A Survey of Dynamic Scheduling in Manufacturing Systems},
  author = {Ouelhadj, Djamila and Petrovic, Sanja},
  year = {2009},
  month = aug,
  journal = {Journal of Scheduling},
  volume = {12},
  number = {4},
  pages = {417--431},
  issn = {1099-1425},
  doi = {10.1007/s10951-008-0090-8},
  urldate = {2023-11-29},
  abstract = {In most real-world environments, scheduling is an ongoing reactive process where the presence of a variety of unexpected disruptions is usually inevitable, and continually forces reconsideration and revision of pre-established schedules. Many of the approaches developed to solve the problem of static scheduling are often impractical in real-world environments, and the near-optimal schedules with respect to the estimated data may become obsolete when they are released to the shop floor. This paper outlines the limitations of the static approaches to scheduling in the presence of real-time information and presents a number of issues that have come up in recent years on dynamic scheduling.},
  langid = {english},
  keywords = {Agent-based scheduling,Dynamic scheduling,Predictive-reactive scheduling,Robust scheduling},
  file = {/home/jeroen/Zotero/storage/578ZKAGC/Ouelhadj and Petrovic - 2009 - A survey of dynamic scheduling in manufacturing sy.pdf}
}

@article{panSurveyTransferLearning2010,
  title = {A {{Survey}} on {{Transfer Learning}}},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  year = {2010},
  month = oct,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {22},
  number = {10},
  pages = {1345--1359},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2009.191},
  urldate = {2023-01-20},
  abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/TVZKQKGH/Pan and Yang - 2010 - A Survey on Transfer Learning.pdf}
}

@misc{parkScheduleNetLearnSolve2021,
  title = {{{ScheduleNet}}: {{Learn}} to Solve Multi-Agent Scheduling Problems with Reinforcement Learning},
  shorttitle = {{{ScheduleNet}}},
  author = {Park, Junyoung and Bakhtiyar, Sanjar and Park, Jinkyoo},
  year = {2021},
  month = jun,
  number = {arXiv:2106.03051},
  eprint = {2106.03051},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.03051},
  urldate = {2024-12-06},
  abstract = {We propose ScheduleNet, a RL-based real-time scheduler, that can solve various types of multi-agent scheduling problems. We formulate these problems as a semi-MDP with episodic reward (makespan) and learn ScheduleNet, a decentralized decision-making policy that can effectively coordinate multiple agents to complete tasks. The decision making procedure of ScheduleNet includes: (1) representing the state of a scheduling problem with the agent-task graph, (2) extracting node embeddings for agent and tasks nodes, the important relational information among agents and tasks, by employing the type-aware graph attention (TGA), and (3) computing the assignment probability with the computed node embeddings. We validate the effectiveness of ScheduleNet as a general learning-based scheduler for solving various types of multi-agent scheduling tasks, including multiple salesman traveling problem (mTSP) and job shop scheduling problem (JSP).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/jeroen/Zotero/storage/ZV24898U/Park et al. - 2021 - ScheduleNet Learn to solve multi-agent scheduling problems with reinforcement learning.pdf;/home/jeroen/Zotero/storage/6HLZUIHG/2106.html}
}

@misc{pavseReducingSamplingError2020,
  title = {Reducing {{Sampling Error}} in {{Batch Temporal Difference Learning}}},
  author = {Pavse, Brahma and Durugkar, Ishan and Hanna, Josiah and Stone, Peter},
  year = {2020},
  month = aug,
  number = {arXiv:2008.06738},
  eprint = {2008.06738},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-12-08},
  abstract = {Temporal difference (TD) learning is one of the main foundations of modern reinforcement learning. This paper studies the use of TD(0), a canonical TD algorithm, to estimate the value function of a given policy from a batch of data. In this batch setting, we show that TD(0) may converge to an inaccurate value function because the update following an action is weighted according to the number of times that action occurred in the batch -- not the true probability of the action under the given policy. To address this limitation, we introduce policy sampling error corrected-TD(0) (PSEC-TD(0)). PSEC-TD(0) first estimates the empirical distribution of actions in each state in the batch and then uses importance sampling to correct for the mismatch between the empirical weighting and the correct weighting for updates following each action. We refine the concept of a certainty-equivalence estimate and argue that PSEC-TD(0) is a more data efficient estimator than TD(0) for a fixed batch of data. Finally, we conduct an empirical evaluation of PSEC-TD(0) on three batch value function learning tasks, with a hyperparameter sensitivity analysis, and show that PSEC-TD(0) produces value function estimates with lower mean squared error than TD(0).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/LLG3CN5B/Pavse et al. - 2020 - Reducing Sampling Error in Batch Temporal Differen.pdf}
}

@book{pearlBookWhyNew2018,
  title = {The {{Book}} of {{Why}}: {{The New Science}} of {{Cause}} and {{Effect}}},
  shorttitle = {The {{Book}} of {{Why}}},
  author = {Pearl, Judea and Mackenzie, Dana},
  year = {2018},
  edition = {1st},
  publisher = {Basic Books, Inc.},
  address = {USA},
  abstract = {A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence"Correlation is not causation." This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality--the study of cause and effect--on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.},
  isbn = {978-0-465-09760-9}
}

@book{pinedoSchedulingTheoryAlgorithms2016,
  title = {Scheduling: {{Theory}}, {{Algorithms}}, and {{Systems}}},
  author = {Pinedo, Michael L.},
  year = {2016},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-26580-3},
  urldate = {2023-10-18},
  isbn = {978-3-319-26578-0 978-3-319-26580-3},
  langid = {english},
  keywords = {Applications of schedule,Decision-making,Deterministic models,Scheduling,Stochastic models},
  file = {/home/jeroen/Zotero/storage/QZV8HWL5/Pinedo - 2016 - Scheduling.pdf}
}

@misc{pirnaySelfImprovementNeuralCombinatorial2024,
  title = {Self-{{Improvement}} for {{Neural Combinatorial Optimization}}: {{Sample}} without {{Replacement}}, but {{Improvement}}},
  shorttitle = {Self-{{Improvement}} for {{Neural Combinatorial Optimization}}},
  author = {Pirnay, Jonathan and Grimm, Dominik G.},
  year = {2024},
  month = jun,
  number = {arXiv:2403.15180},
  eprint = {2403.15180},
  publisher = {arXiv},
  urldate = {2024-10-15},
  abstract = {Current methods for end-to-end constructive neural combinatorial optimization usually train a policy using behavior cloning from expert solutions or policy gradient methods from reinforcement learning. While behavior cloning is straightforward, it requires expensive expert solutions, and policy gradient methods are often computationally demanding and complex to fine-tune. In this work, we bridge the two and simplify the training process by sampling multiple solutions for random instances using the current model in each epoch and then selecting the best solution as an expert trajectory for supervised imitation learning. To achieve progressively improving solutions with minimal sampling, we introduce a method that combines round-wise Stochastic Beam Search with an update strategy derived from a provable policy improvement. This strategy refines the policy between rounds by utilizing the advantage of the sampled sequences with almost no computational overhead. We evaluate our approach on the Traveling Salesman Problem and the Capacitated Vehicle Routing Problem. The models trained with our method achieve comparable performance and generalization to those trained with expert data. Additionally, we apply our method to the Job Shop Scheduling Problem using a transformer-based architecture and outperform existing state-of-the-art methods by a wide margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/7C5HMQY5/Pirnay and Grimm - 2024 - Self-Improvement for Neural Combinatorial Optimization Sample without Replacement, but Improvement.pdf;/home/jeroen/Zotero/storage/RYEQNCRP/2403.html}
}

@inproceedings{Pol2016CoordinatedDR,
  title = {Coordinated Deep Reinforcement Learners for Traffic Light Control},
  author = {{van der Pol}, Elise and Oliehoek, Frans A.},
  year = {2016},
  file = {/home/jeroen/Zotero/storage/ASFV2PZ9/van der Pol and Oliehoek - 2016 - Coordinated deep reinforcement learners for traffi.pdf}
}

@misc{provodinEmpiricalEvaluationPosterior2022,
  title = {An {{Empirical Evaluation}} of {{Posterior Sampling}} for {{Constrained Reinforcement Learning}}},
  author = {Provodin, Danil and Gajane, Pratik and Pechenizkiy, Mykola and Kaptein, Maurits},
  year = {2022},
  month = sep,
  number = {arXiv:2209.03596},
  eprint = {2209.03596},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-12-08},
  abstract = {We study a posterior sampling approach to efficient exploration in constrained reinforcement learning. Alternatively to existing algorithms, we propose two simple algorithms that are more efficient statistically, simpler to implement and computationally cheaper. The first algorithm is based on a linear formulation of CMDP, and the second algorithm leverages the saddle-point formulation of CMDP. Our empirical results demonstrate that, despite its simplicity, posterior sampling achieves state-of-the-art performance and, in some cases, significantly outperforms optimistic algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/ZWRQ4UPN/Provodin et al. - 2022 - An Empirical Evaluation of Posterior Sampling for .pdf}
}

@misc{provodinImpactBatchLearning2021,
  title = {The {{Impact}} of {{Batch Learning}} in {{Stochastic Bandits}}},
  author = {Provodin, Danil and Gajane, Pratik and Pechenizkiy, Mykola and Kaptein, Maurits},
  year = {2021},
  month = nov,
  number = {arXiv:2111.02071},
  eprint = {2111.02071},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-12-13},
  abstract = {We consider a special case of bandit problems, namely batched bandits. Motivated by natural restrictions of recommender systems and e-commerce platforms, we assume that a learning agent observes responses batched in groups over a certain time period. Unlike previous work, we consider a more practically relevant batchcentric scenario of batch learning. We provide a policy-agnostic regret analysis and demonstrate upper and lower bounds for the regret of a candidate policy. Our main theoretical results show that the impact of batch learning can be measured in terms of online behavior. Finally, we demonstrate the consistency of theoretical results by conducting empirical experiments and reflect on the optimal batch size choice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/KAMGEHJ9/Provodin et al. - 2021 - The Impact of Batch Learning in Stochastic Bandits.pdf}
}

@misc{provodinImpactBatchLearning2021a,
  title = {The {{Impact}} of {{Batch Learning}} in {{Stochastic Bandits}}},
  author = {Provodin, Danil and Gajane, Pratik and Pechenizkiy, Mykola and Kaptein, Maurits},
  year = {2021},
  month = nov,
  number = {arXiv:2111.02071},
  eprint = {2111.02071},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-12-08},
  abstract = {We consider a special case of bandit problems, namely batched bandits. Motivated by natural restrictions of recommender systems and e-commerce platforms, we assume that a learning agent observes responses batched in groups over a certain time period. Unlike previous work, we consider a more practically relevant batchcentric scenario of batch learning. We provide a policy-agnostic regret analysis and demonstrate upper and lower bounds for the regret of a candidate policy. Our main theoretical results show that the impact of batch learning can be measured in terms of online behavior. Finally, we demonstrate the consistency of theoretical results by conducting empirical experiments and reflect on the optimal batch size choice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/KV2PV2UT/Provodin et al. - 2021 - The Impact of Batch Learning in Stochastic Bandits.pdf}
}

@misc{prudencioSurveyOfflineReinforcement2022,
  title = {A {{Survey}} on {{Offline Reinforcement Learning}}: {{Taxonomy}}, {{Review}}, and {{Open Problems}}},
  shorttitle = {A {{Survey}} on {{Offline Reinforcement Learning}}},
  author = {Prudencio, Rafael Figueiredo and Maximo, Marcos R. O. A. and Colombini, Esther Luna},
  year = {2022},
  month = mar,
  number = {arXiv:2203.01387},
  eprint = {2203.01387},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-12-08},
  abstract = {With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications such as education, healthcare, and robotics. In this work, we propose a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field, and a review of existing benchmarks' properties and shortcomings. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/P2YTCKFY/Prudencio et al. - 2022 - A Survey on Offline Reinforcement Learning Taxono.pdf}
}

@misc{quintanarPredictingVehiclesTrajectories2021,
  title = {Predicting {{Vehicles Trajectories}} in {{Urban Scenarios}} with {{Transformer Networks}} and {{Augmented Information}}},
  author = {Quintanar, A. and {Fern{\'a}ndez-Llorca}, D. and Parra, I. and Izquierdo, R. and Sotelo, M. A.},
  year = {2021},
  month = jun,
  number = {arXiv:2106.00559},
  eprint = {2106.00559},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-01-20},
  abstract = {Understanding the behavior of road users is of vital importance for the development of trajectory prediction systems. In this context, the latest advances have focused on recurrent structures, establishing the social interaction between the agents involved in the scene. More recently, simpler structures have also been introduced for predicting pedestrian trajectories, based on Transformer Networks, and using positional information [1]. They allow the individual modelling of each agent's trajectory separately without any complex interaction terms. Our model exploits these simple structures by adding augmented data (position and heading), and adapting their use to the problem of vehicle trajectory prediction in urban scenarios in prediction horizons up to 5 seconds. In addition, a cross-performance analysis is performed between different types of scenarios, including highways, intersections and roundabouts, using recent datasets (inD, rounD, highD and INTERACTION). Our model achieves state-of-the-art results and proves to be flexible and adaptable to different types of urban contexts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/jeroen/Zotero/storage/Q92ZVIAZ/Quintanar et al. - 2021 - Predicting Vehicles Trajectories in Urban Scenario.pdf}
}

@article{raoNaiveControlDouble2001,
  title = {Naive Control of the Double Integrator},
  author = {Rao, V.G. and Bernstein, D.S.},
  year = {2001},
  month = oct,
  journal = {IEEE Control Systems Magazine},
  volume = {21},
  number = {5},
  pages = {86--97},
  issn = {1941-000X},
  doi = {10.1109/37.954521},
  urldate = {2024-11-28},
  abstract = {We deal with a form of controller evaluation that may be called naive control. In naive control, a control algorithm derived under nominal (or ideal) conditions is evaluated by analytical or numerical means under off-nominal (or nonideal) conditions that were not assumed in the formal synthesis procedure. Under such nonideal conditions, the controller may or may not perform well. This approach is distinct from robust control, which seeks to accommodate off-nominal perturbations in the synthesis procedure. We consider the double integrator plant, which is one of the most fundamental systems in control applications, representing single degree-of-freedom translational and rotational motion. Applications of the double integrator include low-friction, free rigid-body motion, such as single-axis spacecraft rotation and rotary crane motion. The double integrator plant considered includes a saturation nonlinearity on the control input.},
  keywords = {Adaptive control,Control system synthesis,Control systems,Cranes,Motion control,Programmable control,Sliding mode control,Space vehicles,Testing,Velocity control},
  file = {/home/jeroen/Zotero/storage/MPXGQEM2/954521.html}
}

@book{reckerScientificResearchInformation2013,
  title = {Scientific {{Research}} in {{Information Systems}}},
  author = {Recker, Jan},
  year = {2013},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-30048-6},
  urldate = {2022-12-08},
  isbn = {978-3-642-30047-9 978-3-642-30048-6},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/AKZWNSY2/Recker - 2013 - Scientific Research in Information Systems.pdf}
}

@article{resingPollingSystemsMultitype1993,
  title = {Polling Systems and Multitype Branching Processes},
  author = {Resing, J. A. C.},
  year = {1993},
  month = dec,
  journal = {Queueing Systems},
  volume = {13},
  number = {4},
  pages = {409--426},
  issn = {0257-0130, 1572-9443},
  doi = {10.1007/BF01149263},
  urldate = {2025-02-08},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/N3XG37CT/Resing - 1993 - Polling systems and multitype branching processes.pdf}
}

@article{richards1956a,
  title = {Shock Waves on the Highway},
  author = {Richards, P.I.},
  year = {1956},
  journal = {Operations Research},
  volume = {4},
  number = {1},
  pages = {42--51},
  langid = {english}
}

@article{rios-torresSurveyCoordinationConnected2017,
  title = {A {{Survey}} on the {{Coordination}} of {{Connected}} and {{Automated Vehicles}} at {{Intersections}} and {{Merging}} at {{Highway On-Ramps}}},
  author = {{Rios-Torres}, Jackeline and Malikopoulos, Andreas A.},
  year = {2017},
  month = may,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {18},
  number = {5},
  pages = {1066--1077},
  issn = {1558-0016},
  doi = {10.1109/TITS.2016.2600504},
  urldate = {2024-12-07},
  abstract = {Connected and automated vehicles (CAVs) have the potential to improve safety by reducing and mitigating traffic accidents. They can also provide opportunities to reduce transportation energy consumption and emissions by improving traffic flow. Vehicle communication with traffic structures and traffic lights can allow individual vehicles to optimize their operation and account for unpredictable changes. This paper summarizes the developments and the research trends in coordination with the CAVs that have been reported in the literature to date. Remaining challenges and potential future research directions are also discussed.},
  keywords = {Connected and automated vehicles (CAVs),cooperative driving,Indexes,intersection control,Merging,merging highways,Roads,Safety,vehicle coordination,vehicle-to-infrastructure (V2I) communication,vehicle-to-vehicle (V2V) communication,Vehicles},
  file = {/home/jeroen/Zotero/storage/GDFU3MVZ/7562449.html}
}

@inproceedings{riponJobShopScheduling2012,
  title = {Job {{Shop Scheduling}} with {{Transportation Delays}} and {{Layout Planning}} in {{Manufacturing Systems}}: {{A Multi-objective Evolutionary Approach}}},
  shorttitle = {Job {{Shop Scheduling}} with {{Transportation Delays}} and {{Layout Planning}} in {{Manufacturing Systems}}},
  booktitle = {Autonomous and {{Intelligent Systems}}},
  author = {Ripon, Kazi Shah Nawaz and Glette, Kyrre and Hovin, Mats and Torresen, Jim},
  editor = {Kamel, Mohamed and Karray, Fakhri and Hagras, Hani},
  year = {2012},
  pages = {209--219},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-31368-4_25},
  abstract = {The job shop scheduling problem (JSSP) and the facility layout planning (FLP) are two important factors influencing productivity and cost-controlling activities in any manufacturing system. In the past, a number of attempts have been made to solve these stubborn problems. Although, these two problems are strongly interconnected and solution of one significantly impacts the performance of other, so far, these problems are solved independently. Also, the majority of studies on JSSPs assume that the transportation delays among machines are negligible. In this paper, we introduce a general method using multi-objective genetic algorithm for solving the integrated problems of the FLP and the JSSP considering transportation delay having three objectives to optimize: makespan, total material handling costs, and closeness rating score. The proposed method makes use of Pareto dominance relationship to optimize multiple objectives simultaneously and a set of non-dominated solutions are obtained providing additional degrees of freedom for the production manager.},
  isbn = {978-3-642-31368-4},
  langid = {english},
  keywords = {Closeness Rating,Facility Layout,Facility Layout Problem,Material Handling Cost,Transportation Delay},
  file = {/home/jeroen/Zotero/storage/8X2VC44Y/Ripon et al. - 2012 - Job Shop Scheduling with Transportation Delays and Layout Planning in Manufacturing Systems A Multi.pdf}
}

@article{rodriguesdecamposTrafficCoordinationRoad2017,
  title = {Traffic Coordination at Road Intersections: {{Autonomous}} Decision-Making Algorithms Using Model-Based Heuristics},
  shorttitle = {Traffic Coordination at Road Intersections},
  author = {Rodrigues De Campos, Gabriel and Falcone, Paolo and Hult, Robert and Wymeersch, Henk and Sjoberg, Jonas},
  year = 2017,
  journal = {IEEE Intelligent Transportation Systems Magazine},
  volume = {9},
  number = {1},
  pages = {8--21},
  issn = {1939-1390},
  doi = {10.1109/MITS.2016.2630585},
  urldate = {2024-11-25},
  abstract = {This article focuses on the traffic coordination problem at traffic intersections. We present a decentralized coordination approach, combining optimal control with model-based heuristics. We show how model-based heuristics can lead to low-complexity solutions that are suitable for a fast online implementation, and analyze its properties in terms of efficiency, feasibility and optimality. Finally, simulation results for different scenarios are also presented.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/9QUBVPAS/Rodrigues De Campos et al. - 2017 - Traffic coordination at road intersections Autonomous decision-making algorithms using model-based.pdf}
}

@article{sanchez-lengelingGentleIntroductionGraph2021,
  title = {A {{Gentle Introduction}} to {{Graph Neural Networks}}},
  author = {{Sanchez-Lengeling}, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
  year = {2021},
  month = sep,
  journal = {Distill},
  volume = {6},
  number = {9},
  pages = {e33},
  issn = {2476-0757},
  doi = {10.23915/distill.00033},
  urldate = {2023-11-02},
  abstract = {What components are needed for building learning algorithms that leverage the structure and properties of graphs?},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/CD34W6R4/gnn-intro.html}
}

@phdthesis{sarrafzadehOfflinePolicysearchBayesian1990,
  title = {Offline {{Policy-search}} in {{Bayesian Reinforcement Learning}}},
  author = {Sarrafzadeh, M.},
  year = {1990},
  month = jun,
  urldate = {2022-12-08},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/ECQNPT29/Sarrafzadeh - 1990 - Department of electrical engineering and computer .pdf}
}

@inproceedings{sartorCombinatorialLearningTraffic2019,
  title = {Combinatorial {{Learning}} in {{Traffic Management}}},
  booktitle = {Machine {{Learning}}, {{Optimization}}, and {{Data Science}}},
  author = {Sartor, Giorgio and Mannino, Carlo and Bach, Lukas},
  editor = {Nicosia, Giuseppe and Pardalos, Panos and Umeton, Renato and Giuffrida, Giovanni and Sciacca, Vincenzo},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {384--395},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-37599-7_32},
  abstract = {We describe an exact combinatorial learning approach to solve dynamic job-shop scheduling problems arising in traffic management. When a set of vehicles has to be controlled in real-time, a new schedule must be computed whenever a deviation from the current plan is detected, or periodically after a short amount of time. This suggests that each two (or more) consecutive instances will be very similar. We exploit a recently introduced MILP formulation for job-shop scheduling (called path\&cycle) to develop an effective solution algorithm based on delayed row generation. In our re-optimization framework, the algorithm maintains a pool of combinatorial cuts separated during the solution of previous instances, and adapts them to warm start each new instance. In our experiments, this adaptive approach led to a 4-time average speedup over the static approach (where each instance is solved independently) for a critical application in air traffic management.},
  isbn = {978-3-030-37599-7},
  langid = {english},
  keywords = {Job-shop scheduling,Mixed Integer Linear Programming,Re-optimization},
  file = {/home/jeroen/Zotero/storage/IKJPBJI2/Sartor et al. - 2019 - Combinatorial Learning in Traffic Management.pdf}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.06347},
  urldate = {2024-12-06},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/NR2LX57V/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/jeroen/Zotero/storage/Z35WM4K4/1707.html}
}

@misc{seitaShouldUseOffline,
  title = {Should {{I Use Offline RL}} or {{Imitation Learning}}?},
  author = {Seita, Daniel and Levine, Ilya Kostrikov, Sergey, Aviral Kumar},
  journal = {The Berkeley Artificial Intelligence Research Blog},
  urldate = {2024-02-23},
  abstract = {The BAIR Blog},
  howpublished = {http://bair.berkeley.edu/blog/2022/04/25/rl-or-bc/},
  file = {/home/jeroen/Zotero/storage/EJB8XI76/rl-or-bc.html}
}

@book{sethiOptimalControlTheory2019,
  title = {Optimal {{Control Theory}}: {{Applications}} to {{Management Science}} and {{Economics}}},
  shorttitle = {Optimal {{Control Theory}}},
  author = {Sethi, Suresh P.},
  year = {2019},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-98237-3},
  urldate = {2025-02-15},
  copyright = {https://www.springer.com/tdm},
  isbn = {978-3-319-98236-6 978-3-319-98237-3},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/A9C2WZVT/Sethi - 2019 - Optimal Control Theory Applications to Management Science and Economics.pdf}
}

@inproceedings{shenFastMethodPrevent2017,
  title = {A {{Fast Method}} to {{Prevent Traffic Blockage}} by {{Signal Control Based}} on {{Reinforcement Learning}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Communication}} and {{Electronic Information Engineering}} ({{CEIE}} 2016)},
  author = {Shen, Mengjia},
  year = {2017},
  publisher = {Atlantis Press},
  address = {Guangzhou, China},
  doi = {10.2991/ceie-16.2017.36},
  urldate = {2023-01-20},
  isbn = {978-94-6252-312-8},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/2QU3WDTM/Shen - 2017 - A Fast Method to Prevent Traffic Blockage by Signa.pdf}
}

@article{shengrenOptimalEnergySystem2023,
  title = {Optimal Energy System Scheduling Using a Constraint-Aware Reinforcement Learning Algorithm},
  author = {Shengren, Hou and Vergara, Pedro P. and Salazar Duque, Edgar Mauricio and Palensky, Peter},
  year = {2023},
  month = oct,
  journal = {International Journal of Electrical Power \& Energy Systems},
  volume = {152},
  pages = {109230},
  issn = {01420615},
  doi = {10.1016/j.ijepes.2023.109230},
  urldate = {2023-09-27},
  abstract = {The massive integration of renewable-based distributed energy resources (DERs) inherently increases the energy system's complexity, especially when it comes to defining its operational schedule. Deep reinforcement learning (DRL) algorithms arise as a promising solution due to their data-driven and model-free features. However, current DRL algorithms fail to enforce rigorous operational constraints (e.g., power balance, ramping up or down constraints) limiting their implementation in real systems. To overcome this, in this paper, a DRL algorithm (namely MIP-DQN) is proposed, capable of strictly enforcing all operational constraints in the action space, ensuring the feasibility of the defined schedule in real-time operation. This is done by leveraging recent optimization advances for deep neural networks (DNNs) that allow their representation as a MIP formulation, enabling further consideration of any action space constraints. Comprehensive numerical simulations show that the proposed algorithm outperforms existing state-of-the-art DRL algorithms, obtaining a lower error when compared with the optimal global solution (upper boundary) obtained after solving a mathematical programming formulation with perfect forecast information; while strictly enforcing all operational constraints (even in unseen test days).},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/ZDQYZ7TW/Shengren et al. - 2023 - Optimal energy system scheduling using a constrain.pdf}
}

@inproceedings{shulinModelPredictiveControl2010,
  title = {Model {{Predictive Control}} for Urban Traffic Networks via {{MILP}}},
  booktitle = {Proceedings of the 2010 {{American Control Conference}}},
  author = {{Shu Lin} and De Schutter, B and {Yugeng Xi} and Hellendoorn, H},
  year = {2010},
  month = jun,
  pages = {2272--2277},
  publisher = {IEEE},
  address = {Baltimore, MD},
  doi = {10.1109/ACC.2010.5530534},
  urldate = {2023-09-14},
  abstract = {Model Predictive Control (MPC) is an advanced control strategy that can easily coordinate urban traffic networks. But, due to the nonlinearity of the traffic model, the optimization problem of the MPC controller will become intractable in practice when the scale of the controlled traffic network grows larger. To solve this problem, the nonlinear traffic model is reformulated into a model with only linear equations and inequalities. Mixed-Integer Linear Programming (MILP) algorithms can efficiently solve the reformulated optimization problem, and guarantee the global optimum at the same time. Moreover, the MILP optimization problem is further relaxed by model reduction and adding upper bound constraints.},
  isbn = {978-1-4244-7427-1 978-1-4244-7426-4 978-1-4244-7425-7},
  langid = {english},
  keywords = {MILP},
  file = {/home/jeroen/Zotero/storage/BTB9ZU5I/Shu Lin et al. - 2010 - Model Predictive Control for urban traffic network.pdf}
}

@article{shyalikaReinforcementLearningDynamic2020,
  title = {Reinforcement {{Learning}} in {{Dynamic Task Scheduling}}: {{A Review}}},
  shorttitle = {Reinforcement {{Learning}} in {{Dynamic Task Scheduling}}},
  author = {Shyalika, Chathurangi and Silva, Thushari and Karunananda, Asoka},
  year = {2020},
  month = sep,
  journal = {SN Computer Science},
  volume = {1},
  number = {6},
  pages = {306},
  issn = {2661-8907},
  doi = {10.1007/s42979-020-00326-5},
  urldate = {2023-09-27},
  abstract = {Scheduling is assigning shared resources over time to efficiently complete the tasks over a given period of time. The term is applied separately for tasks and resources correspondingly in task scheduling and resource allocation. Scheduling is a popular topic in operational management and computer science. Effective schedules ensure system efficiency, effective decision making, minimize resource wastage and cost, and enhance overall productivity. It is generally a tedious task to choose the most accurate resources in performing work items and schedules in both computing and business process execution. Especially in real-world dynamic systems where multiple agents involve in scheduling various dynamic tasks is a challenging issue. Reinforcement Learning is an emergent technology which has been able to solve the problem of the optimal task and resource scheduling dynamically. This review paper is about a research study that focused on Reinforcement Learning techniques that have been used for dynamic task scheduling. The paper addresses the results of the study by means of the state-of-the-art on Reinforcement learning techniques used in dynamic task scheduling and a comparative review of those techniques.},
  langid = {english},
  keywords = {Dynamic,Environment uncertainty,Multi-agent,Reinforcement learning,Task scheduling},
  file = {/home/jeroen/Zotero/storage/P83QVV5V/Shyalika et al. - 2020 - Reinforcement Learning in Dynamic Task Scheduling.pdf}
}

@article{silverMasteringGameGo2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature24270},
  urldate = {2023-01-10},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/EBUZTASC/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf}
}

@misc{smitGraphNeuralNetworks2024,
  title = {Graph {{Neural Networks}} for {{Job Shop Scheduling Problems}}: {{A Survey}}},
  shorttitle = {Graph {{Neural Networks}} for {{Job Shop Scheduling Problems}}},
  author = {Smit, Igor G. and Zhou, Jianan and Reijnen, Robbert and Wu, Yaoxin and Chen, Jian and Zhang, Cong and Bukhsh, Zaharah and Nuijten, Wim and Zhang, Yingqian},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2406.14096},
  urldate = {2024-11-15},
  abstract = {Job shop scheduling problems (JSSPs) represent a critical and challenging class of combinatorial optimization problems. Recent years have witnessed a rapid increase in the application of graph neural networks (GNNs) to solve JSSPs, albeit lacking a systematic survey of the relevant literature. This paper aims to thoroughly review prevailing GNN methods for different types of JSSPs and the closely related flow-shop scheduling problems (FSPs), especially those leveraging deep reinforcement learning (DRL). We begin by presenting the graph representations of various JSSPs, followed by an introduction to the most commonly used GNN architectures. We then review current GNN-based methods for each problem type, highlighting key technical elements such as graph representations, GNN architectures, GNN tasks, and training algorithms. Finally, we summarize and analyze the advantages and limitations of GNNs in solving JSSPs and provide potential future research opportunities. We hope this survey can motivate and inspire innovative approaches for more powerful GNN-based approaches in tackling JSSPs and other scheduling problems.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  langid = {english},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/home/jeroen/Zotero/storage/KCAG2BX4/Smit et al. - 2024 - Graph Neural Networks for Job Shop Scheduling Problems A Survey.pdf}
}

@article{smitGraphNeuralNetworks2025,
  title = {Graph Neural Networks for Job Shop Scheduling Problems: {{A}} Survey},
  shorttitle = {Graph Neural Networks for Job Shop Scheduling Problems},
  author = {Smit, Igor G. and Zhou, Jianan and Reijnen, Robbert and Wu, Yaoxin and Chen, Jian and Zhang, Cong and Bukhsh, Zaharah and Zhang, Yingqian and Nuijten, Wim},
  year = {2025},
  month = apr,
  journal = {Computers \& Operations Research},
  volume = {176},
  pages = {106914},
  issn = {0305-0548},
  doi = {10.1016/j.cor.2024.106914},
  urldate = {2025-02-19},
  abstract = {Job shop scheduling problems (JSSPs) represent a critical and challenging class of combinatorial optimization problems. Recent years have witnessed a rapid increase in the application of graph neural networks (GNNs) to solve JSSPs, albeit lacking a systematic survey of the relevant literature. This paper aims to thoroughly review prevailing GNN methods for different types of JSSPs and the closely related flow-shop scheduling problems (FSPs), especially those leveraging deep reinforcement learning (DRL). We begin by presenting the graph representations of various JSSPs, followed by an introduction to the most commonly used GNN architectures. We then review current GNN-based methods for each problem type, highlighting key technical elements such as graph representations, GNN architectures, GNN tasks, and training algorithms. Finally, we summarize and analyze the advantages and limitations of GNNs in solving JSSPs and provide potential future research opportunities. We hope this survey can motivate and inspire innovative approaches for more powerful GNN-based approaches in tackling JSSPs and other scheduling problems.},
  keywords = {Combinatorial optimization,Deep reinforcement learning,Flow-shop scheduling,Graph neural network,Job shop scheduling},
  file = {/home/jeroen/Zotero/storage/I9Y97RUU/Smit et al. - 2025 - Graph neural networks for job shop scheduling problems A survey.pdf;/home/jeroen/Zotero/storage/4HN2CMY8/S0305054824003861.html}
}

@misc{solmsSolvingNonpreemptiveTwo2021,
  title = {Solving the Non-Preemptive Two Queue Polling Model with Generally Distributed Service and Switch-over Durations and {{Poisson}} Arrivals as a {{Semi-Markov Decision Process}}},
  author = {Solms, Dylan},
  year = {2021},
  month = dec,
  number = {arXiv:2112.06578},
  eprint = {2112.06578},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-02-23},
  abstract = {The polling system with switch-over durations is a useful model with several practical applications. It is classified as a Discrete Event Dynamic System (DEDS) for which no one agreed upon modelling approach exists. Furthermore, DEDS are quite complex. To date, the most sophisticated approach to modelling the polling system of interest has been a Continuous-time Markov Decision Process (CTMDP). This paper presents a Semi-Markov Decision Process (SMDP) formulation of the polling system as to introduce additional modelling power. Such power comes at the expense of truncation errors and expensive numerical integrals which naturally leads to the question of whether the SMDP policy provides a worthwhile advantage. To further add to this scenario, it is shown how sparsity can be exploited in the CTMDP to develop a computationally efficient model. The discounted performance of the SMDP and CTMDP policies are evaluated using a Semi-Markov Process simulator. The two policies are accompanied by a heuristic policy specifically developed for this polling system a well as an exhaustive service policy. Parametric and non-parametric hypothesis tests are used to test whether differences in performance are statistically significant.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Optimization and Control},
  file = {/home/jeroen/Zotero/storage/UTXVSTU2/2112.06578.pdf;/home/jeroen/Zotero/storage/JFH53QL5/2112.html}
}

@article{sommerBidirectionallyCoupledNetwork2011,
  title = {Bidirectionally {{Coupled Network}} and {{Road Traffic Simulation}} for {{Improved IVC Analysis}}},
  shorttitle = {Veins},
  author = {Sommer, C and German, R and Dressler, F},
  year = {2011},
  month = jan,
  journal = {IEEE Transactions on Mobile Computing},
  volume = {10},
  number = {1},
  pages = {3--15},
  issn = {1536-1233},
  doi = {10.1109/TMC.2010.133},
  urldate = {2023-01-20},
  file = {/home/jeroen/Zotero/storage/M2QZETM7/Sommer et al. - 2011 - Bidirectionally Coupled Network and Road Traffic S.pdf}
}

@article{sorensenMetaheuristicsMetaphorExposed2015,
  title = {Metaheuristics---the Metaphor Exposed},
  author = {S{\"o}rensen, Kenneth},
  year = {2015},
  journal = {International Transactions in Operational Research},
  volume = {22},
  number = {1},
  pages = {3--18},
  issn = {1475-3995},
  doi = {10.1111/itor.12001},
  urldate = {2023-10-13},
  abstract = {In recent years, the field of combinatorial optimization has witnessed a true tsunami of ``novel'' metaheuristic methods, most of them based on a metaphor of some natural or man-made process. The behavior of virtually any species of insects, the flow of water, musicians playing together -- it seems that no idea is too far-fetched to serve as inspiration to launch yet another metaheuristic. In this paper, we will argue that this line of research is threatening to lead the area of metaheuristics away from scientific rigor. We will examine the historical context that gave rise to the increasing use of metaphors as inspiration and justification for the development of new methods, discuss the reasons for the vulnerability of the metaheuristics field to this line of research, and point out its fallacies. At the same time, truly innovative research of high quality is being performed as well. We conclude the paper by discussing some of the properties of this research and by pointing out some of the most promising research avenues for the field of metaheuristics.},
  copyright = {{\copyright} 2013 The Authors. International Transactions in Operational Research {\copyright} 2013 International Federation of Operational Research Societies Published by John Wiley \& Sons Ltd, 9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main St, Malden, MA02148, USA.},
  langid = {english},
  keywords = {combinatorial optimization,heuristics,metaheuristics,optimization},
  file = {/home/jeroen/Zotero/storage/HCB6R6BK/2013 - Wayback Machine.pdf;/home/jeroen/Zotero/storage/BIR54TBV/itor.html}
}

@book{stevanovicAdaptiveTrafficControl2010,
  title = {Adaptive {{Traffic Control Systems}}: {{Domestic}} and {{Foreign State}} of {{Practice}}},
  shorttitle = {Adaptive {{Traffic Control Systems}}},
  author = {Stevanovic, Aleksandar and {Transportation Research Board} and {National Cooperative Highway Research Program Synthesis Program} and {Transportation Research Board}},
  year = {2010},
  month = apr,
  pages = {14364},
  publisher = {National Academies Press},
  address = {Washington, D.C.},
  doi = {10.17226/14364},
  urldate = {2023-01-20},
  isbn = {978-0-309-28039-6},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/RU8A7VCB/Stevanovic et al. - 2010 - Adaptive Traffic Control Systems Domestic and For.pdf}
}

@misc{sumorl,
  title = {{{SUMO-RL}}},
  author = {Alegre, Lucas N.},
  year = {2019},
  publisher = {GitHub}
}

@misc{sundararajanAxiomaticAttributionDeep2017,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  year = {2017},
  month = jun,
  number = {arXiv:1703.01365},
  eprint = {1703.01365},
  publisher = {arXiv},
  urldate = {2024-10-15},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/4YV7ITX9/Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf;/home/jeroen/Zotero/storage/CSQUW9BJ/1703.html}
}

@article{suttonDynaIntegratedArchitecture1991,
  title = {Dyna, an Integrated Architecture for Learning, Planning, and Reacting},
  author = {Sutton, Richard S.},
  year = {1991},
  month = jul,
  journal = {ACM SIGART Bulletin},
  volume = {2},
  number = {4},
  pages = {160--163},
  issn = {0163-5719},
  doi = {10.1145/122344.122377},
  urldate = {2023-01-10},
  abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/ZUHCWX36/Sutton - 1991 - Dyna, an integrated architecture for learning, pla.pdf}
}

@article{suttonMDPsSemiMDPsFramework1999,
  title = {Between {{MDPs}} and Semi-{{MDPs}}: {{A}} Framework for Temporal Abstraction in Reinforcement Learning},
  shorttitle = {Between {{MDPs}} and Semi-{{MDPs}}},
  author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
  year = {1999},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {112},
  number = {1-2},
  pages = {181--211},
  issn = {00043702},
  doi = {10.1016/S0004-3702(99)00052-1},
  urldate = {2024-03-04},
  abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options---closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: 1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, 2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and 3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/URULZ8ES/Sutton et al. - 1999 - Between MDPs and semi-MDPs A framework for tempor.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning},
  file = {/home/jeroen/Zotero/storage/M2TL3FFQ/RLbook2020.pdf}
}

@article{sweetDoesTrafficCongestion2011,
  title = {Does {{Traffic Congestion Slow}} the {{Economy}}?},
  author = {Sweet, Matthias},
  year = {2011},
  month = nov,
  journal = {Journal of Planning Literature},
  volume = {26},
  number = {4},
  pages = {391--404},
  issn = {0885-4122, 1552-6593},
  doi = {10.1177/0885412211409754},
  urldate = {2023-01-10},
  abstract = {Does traffic congestion negatively impact the economic growth of metropolitan areas? This article reviews the findings of three research directions addressing this question. First, research on first-order impacts indicates that the economic value of congestion-induced travel delay is tenuous since travelers adapt. Second, research on second-order impacts suggests that congestion slows metropolitan growth, inhibits agglomeration economies, and shapes economic geographies. Third, research on public-sector congestion mitigation policies identifies significant fiscal burdens despite limited success at reducing congestion. In sum, research on individual, business, and public-sector responses to congestion demonstrate a shift from congestion mitigation toward adaptation.},
  langid = {english}
}

@book{szepesvariAlgorithmsReinforcementLearning2019,
  title = {Algorithms for {{Reinforcement Learning}}},
  author = {Szepesv{\'a}ri, Csaba},
  year = {2019},
  series = {Synthesis {{Lectures}} on {{Artificial Intelligence}} and {{Machine Learning}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-01551-9},
  urldate = {2022-12-09},
  isbn = {978-3-031-00423-0 978-3-031-01551-9},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/UK3TFXTN/Szepesvári - 2019 - Algorithms for Reinforcement Learning.pdf}
}

@article{taillardBenchmarksBasicScheduling1993,
  title = {Benchmarks for Basic Scheduling Problems},
  author = {Taillard, {\'E}ric},
  year = {1993},
  journal = {European Journal of Operational Research},
  volume = {64},
  pages = {278--285},
  abstract = {In this paper, we propose 260 scheduling problems whose size is greater than that of the rare examples published. Such sizes correspond to real dimensions of industrial problems.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/LF43VZIK/Taillard - BASIC SCHEDULING PROBLEMS.pdf}
}

@misc{tallapragadaDistributedControlVehicle2017,
  title = {Distributed Control of Vehicle Strings under Finite-Time and Safety Specifications},
  author = {Tallapragada, Pavankumar and Cortes, Jorge},
  year = {2017},
  month = jul,
  number = {arXiv:1701.03580},
  eprint = {1701.03580},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1701.03580},
  urldate = {2024-11-25},
  abstract = {This paper studies an optimal control problem for a string of vehicles with safety requirements and finite-time specifications on the approach time to a target region. Our problem formulation is motivated by scenarios involving autonomous vehicles circulating on arterial roads with intelligent management at traffic intersections. We propose a provably correct distributed control algorithm that ensures that the vehicles satisfy the finite-time specifications under speed limits, acceleration saturation, and safety requirements. The safety specifications are such that collisions can be avoided even in cases of communication failure. We also discuss how the proposed distributed algorithm can be integrated with an intelligent intersection manager to provide information about the feasible approach times of the vehicle string and a guaranteed bound of its time of occupancy of the intersection. Our simulation study illustrates the algorithm and its properties regarding approach time, occupancy time, and fuel and time cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Systems and Control,Mathematics - Optimization and Control},
  file = {/home/jeroen/Zotero/storage/SRRMDUBG/Tallapragada and Cortes - 2017 - Distributed control of vehicle strings under finite-time and safety specifications.pdf;/home/jeroen/Zotero/storage/GBAN7AX3/1701.html}
}

@misc{tallapragadaHierarchicaldistributedOptimizedCoordination2017,
  title = {Hierarchical-Distributed Optimized Coordination of Intersection Traffic},
  author = {Tallapragada, Pavankumar and Cort{\'e}s, Jorge},
  year = {2017},
  month = jan,
  number = {arXiv:1601.00246},
  eprint = {1601.00246},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-04-23},
  abstract = {This paper considers the problem of coordinating the vehicular traffic at an intersection and on the branches leading to it for minimizing a combination of total travel time and energy consumption. We propose a provably safe hierarchical-distributed solution to balance computational complexity and optimality of the system operation. In our design, a central intersection manager communicates with vehicles heading towards the intersection, groups them into clusters (termed bubbles) as they appear, and determines an optimal schedule of passage through the intersection for each bubble. The vehicles in each bubble receive their schedule and implement local distributed control to ensure system-wide inter-vehicular safety while respecting speed and acceleration limits, conforming to the assigned schedule, and seeking to optimize their individual trajectories. Our analysis rigorously establishes that the different aspects of the hierarchical design operate in concert and that the safety guarantees provided by the proposed design are satisfied. We illustrate its execution in a suite of simulations and compare its performance to traditional signal-based coordination over a wide range of system parameters.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  file = {/home/jeroen/Zotero/storage/7PR7I2WH/Tallapragada and Cortés - 2017 - Hierarchical-distributed optimized coordination of.pdf;/home/jeroen/Zotero/storage/YCDGDPIH/1601.html}
}

@misc{tangReinforcementLearningInteger2020,
  title = {Reinforcement {{Learning}} for {{Integer Programming}}: {{Learning}} to {{Cut}}},
  shorttitle = {Reinforcement {{Learning}} for {{Integer Programming}}},
  author = {Tang, Yunhao and Agrawal, Shipra and Faenza, Yuri},
  year = {2020},
  month = jul,
  number = {arXiv:1906.04859},
  eprint = {1906.04859},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.04859},
  urldate = {2023-09-25},
  abstract = {Integer programming (IP) is a general optimization framework widely applicable to a variety of unstructured and structured problems arising in, e.g., scheduling, production planning, and graph optimization. As IP models many provably hard to solve problems, modern IP solvers rely on many heuristics. These heuristics are usually human-designed, and naturally prone to suboptimality. The goal of this work is to show that the performance of those solvers can be greatly enhanced using reinforcement learning (RL). In particular, we investigate a specific methodology for solving IPs, known as the Cutting Plane Method. This method is employed as a subroutine by all modern IP solvers. We present a deep RL formulation, network architecture, and algorithms for intelligent adaptive selection of cutting planes (aka cuts). Across a wide range of IP tasks, we show that the trained RL agent significantly outperforms human-designed heuristics, and effectively generalizes to 10X larger instances and across IP problem classes. The trained agent is also demonstrated to benefit the popular downstream application of cutting plane methods in Branch-and-Cut algorithm, which is the backbone of state-of-the-art commercial IP solvers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/ZYWC4E8Z/Tang et al. - 2020 - Reinforcement Learning for Integer Programming Le.pdf;/home/jeroen/Zotero/storage/ZHQ44AZ7/1906.html}
}

@misc{tasselEndEndReinforcementLearning2023,
  title = {An {{End-to-End Reinforcement Learning Approach}} for {{Job-Shop Scheduling Problems Based}} on {{Constraint Programming}}},
  author = {Tassel, Pierre and Gebser, Martin and Schekotihin, Konstantin},
  year = {2023},
  month = jun,
  number = {arXiv:2306.05747},
  eprint = {2306.05747},
  publisher = {arXiv},
  urldate = {2024-11-15},
  abstract = {Constraint Programming (CP) is a declarative programming paradigm that allows for modeling and solving combinatorial optimization problems, such as the Job-Shop Scheduling Problem (JSSP). While CP solvers manage to find optimal or near-optimal solutions for small instances, they do not scale well to large ones, i.e., they require long computation times or yield low-quality solutions. Therefore, real-world scheduling applications often resort to fast, handcrafted, priority-based dispatching heuristics to find a good initial solution and then refine it using optimization methods. This paper proposes a novel end-to-end approach to solving scheduling problems by means of CP and Reinforcement Learning (RL). In contrast to previous RL methods, tailored for a given problem by including procedural simulation algorithms, complex feature engineering, or handcrafted reward functions, our neural-network architecture and training algorithm merely require a generic CP encoding of some scheduling problem along with a set of small instances. Our approach leverages existing CP solvers to train an agent learning a Priority Dispatching Rule (PDR) that generalizes well to large instances, even from separate datasets. We evaluate our method on seven JSSP datasets from the literature, showing its ability to find higher-quality solutions for very large instances than obtained by static PDRs and by a CP solver within the same time limit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/WI2R63GE/Tassel et al. - 2023 - An End-to-End Reinforcement Learning Approach for Job-Shop Scheduling Problems Based on Constraint P.pdf;/home/jeroen/Zotero/storage/U34FNVWD/2306.html}
}

@misc{tasselReinforcementLearningEnvironment2021,
  title = {A {{Reinforcement Learning Environment For Job-Shop Scheduling}}},
  author = {Tassel, Pierre and Gebser, Martin and Schekotihin, Konstantin},
  year = {2021},
  month = apr,
  number = {arXiv:2104.03760},
  eprint = {2104.03760},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-09-27},
  abstract = {Scheduling is a fundamental task occurring in various automated systems applications, e.g., optimal schedules for machines on a job shop allow for a reduction of production costs and waste. Nevertheless, finding such schedules is often intractable and cannot be achieved by Combinatorial Optimization Problem (COP) methods within a given time limit. Recent advances of Deep Reinforcement Learning (DRL) in learning complex behavior enable new COP application possibilities. This paper presents an efficient DRL environment for Job-Shop Scheduling -- an important problem in the field. Furthermore, we design a meaningful and compact state representation as well as a novel, simple dense reward function, closely related to the sparse make-span minimization criteria used by COP methods. We demonstrate that our approach significantly outperforms existing DRL methods on classic benchmark instances, coming close to state-of-the-art COP approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/jeroen/Zotero/storage/SJQW33NS/Tassel et al. - 2021 - A Reinforcement Learning Environment For Job-Shop .pdf;/home/jeroen/Zotero/storage/6JZN4CA5/2104.html}
}

@article{taylorTransferLearningReinforcement,
  title = {Transfer {{Learning}} for {{Reinforcement Learning Domains}}: {{A Survey}}},
  author = {Taylor, Matthew E and Stone, Peter},
  abstract = {The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/2P69864C/Taylor and Stone - Transfer Learning for Reinforcement Learning Domai.pdf}
}

@article{timmermanPlatoonFormingAlgorithms2021,
  title = {Platoon Forming Algorithms for Intelligent Street Intersections},
  author = {Timmerman, R. W. and Boon, M. A. A.},
  year = {2021},
  month = feb,
  journal = {Transportmetrica A: Transport Science},
  volume = {17},
  number = {3},
  pages = {278--307},
  issn = {2324-9935, 2324-9943},
  doi = {10.1080/23249935.2019.1692962},
  urldate = {2023-01-10},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/7A98IPE5/Timmerman and Boon - 2021 - Platoon forming algorithms for intelligent street .pdf}
}

@book{tomczakDeepGenerativeModeling2024,
  title = {Deep {{Generative Modeling}}},
  author = {Tomczak, Jakub M.},
  year = {2024},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-64087-2},
  urldate = {2025-03-26},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-031-64086-5 978-3-031-64087-2},
  langid = {english},
  keywords = {Autoregressive models,Boltzmann Machines,Diffusion models,Energy-based Models,Flow-based models,GANs,Generative AI,Hybrid Models,Large Language Models,Latent Variable Models,Probabilistic Circuits,Score-based Generative Models},
  file = {/home/jeroen/Zotero/storage/F9MUYDUH/Tomczak - 2024 - Deep Generative Modeling.pdf}
}

@misc{towers_gymnasium_2023,
  title = {Gymnasium},
  author = {Towers, Mark and Terry, Jordan K. and Kwiatkowski, Ariel and Balis, John U. and de Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and {Perez-Vicente}, Rodrigo and Pierr{\'e}, Andrea and Schulhoff, Sander and Tai, Jun Jet and Shen, Andrew Tan Jin and Younis, Omar G.},
  year = {2023},
  month = mar,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.8127026},
  urldate = {2023-07-08},
  abstract = {An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)}
}

@article{trecaGreenWaveCoordinRaetiionnforFcoermTernat,
  title = {Green {{Wave CoordinRaetiionnforFcoermTernat LceaSrignninagl Control Using Deep}}},
  author = {Tr{\'e}ca, Maxime and Zargayouna, Mahdi and Barth, Dominique and Garbiso, Julian},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/PDJIJI8C/Tréca et al. - Green Wave CoordinRaetiionnforFcoermTernatLceaSri.pdf}
}

@misc{uchenduJumpStartReinforcementLearning2023,
  title = {Jump-{{Start Reinforcement Learning}}},
  author = {Uchendu, Ikechukwu and Xiao, Ted and Lu, Yao and Zhu, Banghua and Yan, Mengyuan and Simon, Jos{\'e}phine and Bennice, Matthew and Fu, Chuyuan and Ma, Cong and Jiao, Jiantao and Levine, Sergey and Hausman, Karol},
  year = {2023},
  month = jul,
  number = {arXiv:2204.02372},
  eprint = {2204.02372},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.02372},
  urldate = {2024-02-22},
  abstract = {Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that JSRL is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/FJH5XJJL/Uchendu et al. - 2023 - Jump-Start Reinforcement Learning.pdf;/home/jeroen/Zotero/storage/PCRNMIQC/2204.html}
}

@article{vanleeuwaardenDelayAnalysisFixedCycle2006,
  title = {Delay {{Analysis}} for the {{Fixed-Cycle Traffic-Light Queue}}},
  author = {{van Leeuwaarden}, J. S. H.},
  year = {2006},
  month = may,
  journal = {Transportation Science},
  volume = {40},
  number = {2},
  pages = {189--199},
  issn = {0041-1655, 1526-5447},
  doi = {10.1287/trsc.1050.0125},
  urldate = {2023-01-10},
  abstract = {We consider the fixed-cycle traffic-light (FCTL) queue, where vehicles arrive at an intersection controlled by a traffic light and form a queue. The traffic-light signal alternates between green and red periods, and delayed vehicles are assumed to depart during the green period at equal time intervals.             Most of the research done on the FCTL queue assumes that the vehicles arrive at the intersection according to a Poisson process and focuses on deriving formulas for the mean queue length at the end of green periods and the mean delay. For a class of discrete arrival processes, including the Poisson process, we derive the probability generating function of both the queue length and delay, from which the whole queue length and delay distribution can be obtained. This allows for the evaluation of performance characteristics other than the mean, such as the variance and percentiles of the distribution.             We discuss the numerical procedures that are required to obtain the performance characteristics, and give several numerical examples.},
  langid = {english}
}

@misc{vanrielTransientBehaviorQueues2021,
  title = {Transient {{Behavior}} of {{Queues}} at {{Signalized Traffic Intersections}}},
  author = {{van Riel}, Jeroen},
  year = {2021},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/ZC2ENVNC/van Riel - Transient Behavior of Queues at Signalized Traffic.pdf}
}

@misc{vesselinovaLearningCombinatorialOptimization2020,
  title = {Learning {{Combinatorial Optimization}} on {{Graphs}}: {{A Survey}} with {{Applications}} to {{Networking}}},
  shorttitle = {Learning {{Combinatorial Optimization}} on {{Graphs}}},
  author = {Vesselinova, Natalia and Steinert, Rebecca and {Perez-Ramirez}, Daniel F. and Boman, Magnus},
  year = {2020},
  month = jul,
  number = {arXiv:2005.11081},
  eprint = {2005.11081},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.11081},
  urldate = {2024-11-09},
  abstract = {Existing approaches to solving combinatorial optimization problems on graphs suffer from the need to engineer each problem algorithmically, with practical problems recurring in many instances. The practical side of theoretical computer science, such as computational complexity, then needs to be addressed. Relevant developments in machine learning research on graphs are surveyed for this purpose. We organize and compare the structures involved with learning to solve combinatorial optimization problems, with a special eye on the telecommunications domain and its continuous development of live and research networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/BT5Z6FFG/Vesselinova et al. - 2020 - Learning Combinatorial Optimization on Graphs A Survey with Applications to Networking.pdf;/home/jeroen/Zotero/storage/4ZJ2NACG/2005.html}
}

@phdthesis{vestjensOnlineMachineScheduling1997,
  type = {Phd {{Thesis}} 1 ({{Research TU}}/e / {{Graduation TU}}/e)},
  title = {On-Line Machine Scheduling},
  author = {Vestjens, A.P.A.},
  year = {1997},
  address = {Eindhoven},
  doi = {10.6100/IR500043},
  isbn = {9789038605715},
  school = {Technische Universiteit Eindhoven}
}

@misc{vinyalsOrderMattersSequence2016,
  title = {Order {{Matters}}: {{Sequence}} to Sequence for Sets},
  shorttitle = {Order {{Matters}}},
  author = {Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},
  year = {2016},
  month = feb,
  number = {arXiv:1511.06391},
  eprint = {1511.06391},
  publisher = {arXiv},
  urldate = {2024-11-13},
  abstract = {Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/GZQ292PX/Vinyals et al. - 2016 - Order Matters Sequence to sequence for sets.pdf;/home/jeroen/Zotero/storage/3FLLJZ8Z/1511.html}
}

@misc{vinyalsPointerNetworks2017a,
  title = {Pointer {{Networks}}},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  year = {2017},
  month = jan,
  number = {arXiv:1506.03134},
  eprint = {1506.03134},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-02-21},
  abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/37GLHEAR/Vinyals et al. - 2017 - Pointer Networks.pdf;/home/jeroen/Zotero/storage/FX3A5ERG/1506.html}
}

@article{vivekanandanReinforcementLearningApproach2023,
  title = {A {{Reinforcement Learning Approach}} for {{Scheduling Problems}} with {{Improved Generalization}} through {{Order Swapping}}},
  author = {Vivekanandan, Deepak and Wirth, Samuel and Karlbauer, Patrick and Klarmann, Noah},
  year = {2023},
  month = jun,
  journal = {Machine Learning and Knowledge Extraction},
  volume = {5},
  number = {2},
  pages = {418--430},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2504-4990},
  doi = {10.3390/make5020025},
  urldate = {2023-10-14},
  abstract = {The scheduling of production resources (such as associating jobs to machines) plays a vital role for the manufacturing industry not only for saving energy, but also for increasing the overall efficiency. Among the different job scheduling problems, the Job Shop Scheduling Problem (JSSP) is addressed in this work. JSSP falls into the category of NP-hard Combinatorial Optimization Problem (COP), in which solving the problem through exhaustive search becomes unfeasible. Simple heuristics such as First-In, First-Out, Largest Processing Time First and metaheuristics such as taboo search are often adopted to solve the problem by truncating the search space. The viability of the methods becomes inefficient for large problem sizes as it is either far from the optimum or time consuming. In recent years, the research towards using Deep Reinforcement Learning (DRL) to solve COPs has gained interest and has shown promising results in terms of solution quality and computational efficiency. In this work, we provide an novel approach to solve the JSSP examining the objectives generalization and solution effectiveness using DRL. In particular, we employ the Proximal Policy Optimization (PPO) algorithm that adopts the policy-gradient paradigm that is found to perform well in the constrained dispatching of jobs. We incorporated a new method called Order Swapping Mechanism (OSM) in the environment to achieve better generalized learning of the problem. The performance of the presented approach is analyzed in depth by using a set of available benchmark instances and comparing our results with the work of other groups.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {generalization,Industry 4.0,Job Shop Scheduling,Markov Decision Process,Production Scheduling,Reinforcement Learning},
  file = {/home/jeroen/Zotero/storage/8WVQCT7T/Vivekanandan et al. - 2023 - A Reinforcement Learning Approach for Scheduling P.pdf}
}

@article{wageningen-kessels2015a,
  title = {Genealogy of Traffic Flow Models},
  author = {{Wageningen-Kessels}, F. and Lint, H. and Vuik, K. and Hoogendoorn, S.},
  year = {2015},
  journal = {EURO Journal on Transportation and Logistics},
  volume = {4},
  number = {4},
  pages = {445--473},
  langid = {english}
}

@misc{wangGradientBasedFeature2024,
  title = {Gradient Based {{Feature Attribution}} in {{Explainable AI}}: {{A Technical Review}}},
  shorttitle = {Gradient Based {{Feature Attribution}} in {{Explainable AI}}},
  author = {Wang, Yongjie and Zhang, Tong and Guo, Xu and Shen, Zhiqi},
  year = {2024},
  month = mar,
  number = {arXiv:2403.10415},
  eprint = {2403.10415},
  publisher = {arXiv},
  urldate = {2024-10-15},
  abstract = {The surge in black-box AI models has prompted the need to explain the internal mechanism and justify their reliability, especially in high-stakes applications, such as healthcare and autonomous driving. Due to the lack of a rigorous definition of explainable AI (XAI), a plethora of research related to explainability, interpretability, and transparency has been developed to explain and analyze the model from various perspectives. Consequently, with an exhaustive list of papers, it becomes challenging to have a comprehensive overview of XAI research from all aspects. Considering the popularity of neural networks in AI research, we narrow our focus to a specific area of XAI research: gradient based explanations, which can be directly adopted for neural network models. In this review, we systematically explore gradient based explanation methods to date and introduce a novel taxonomy to categorize them into four distinct classes. Then, we present the essence of technique details in chronological order and underscore the evolution of algorithms. Next, we introduce both human and quantitative evaluations to measure algorithm performance. More importantly, we demonstrate the general challenges in XAI and specific challenges in gradient based explanations. We hope that this survey can help researchers understand state-of-the-art progress and their corresponding disadvantages, which could spark their interest in addressing these issues in future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/jeroen/Zotero/storage/QSVKWGZE/Wang et al. - 2024 - Gradient based Feature Attribution in Explainable AI A Technical Review.pdf;/home/jeroen/Zotero/storage/TPL9GNSD/2403.html}
}

@article{waubertdepuiseauEffectivenessBottleneckInformation2023,
  title = {On {{The Effectiveness Of Bottleneck Information For Solving Job Shop Scheduling Problems Using Deep Reinforcement Learning}}},
  author = {{Waubert de Puiseau}, Constantin and Zey, Lennart and Demir, Merve and Tercan, Hasan and Meisen, Tobias},
  year = {2023},
  publisher = {Hannover : publish-Ing.},
  urldate = {2025-01-20},
  abstract = {Job shop scheduling problems (JSSPs) have been the subject of intense studies for decades because they are often at the core of significant industrial planning challenges and have a high optimization potential. As a result, the scientific community has developed clever heuristics to approximate optimal solutions. A prominent example is the shifting bottleneck heuristic, which iteratively identifies bottlenecks in the current schedule and uses this information to apply targeted optimization steps. In recent years, deep reinforcement learning (DRL) has gained increasing attention for solving scheduling problems in job shops and beyond. One design decision when applying DRL to JSSPs is the observation, i.e., the descriptive representation of the current problem and solution state. Interestingly, DRL solutions do not make use of explicit notions of bottlenecks that have been developed in the past when designing the observation. In this paper, we investigate ways to leverage a definition of bottlenecks inspired by the shifting bottleneck heuristic for JSSPs with DRL to increase the effectiveness and efficiency of model training. To this end, we train two different DRL base models with and without bottleneck features. However, our results indicate that previously developed bottleneck definitions neither increase training efficiency nor final model performance.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/6EMXFTJD/Waubert de Puiseau et al. - 2023 - On The Effectiveness Of Bottleneck Information For Solving Job Shop Scheduling Problems Using Deep R.pdf}
}

@techreport{websterTrafficSignalSettings1958,
  title = {Traffic Signal Settings},
  author = {Webster, F.V.},
  year = {1958}
}

@inproceedings{weiCoLightLearningNetworklevel2019,
  title = {{{CoLight}}: {{Learning Network-level Cooperation}} for {{Traffic Signal Control}}},
  shorttitle = {{{CoLight}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Wei, Hua and Xu, Nan and Zhang, Huichu and Zheng, Guanjie and Zang, Xinshi and Chen, Chacha and Zhang, Weinan and Zhu, Yanmin and Xu, Kai and Li, Zhenhui},
  year = {2019},
  month = nov,
  eprint = {1905.05717},
  primaryclass = {cs},
  pages = {1913--1922},
  doi = {10.1145/3357384.3357902},
  urldate = {2023-09-23},
  abstract = {Cooperation among the traffic signals enables vehicles to move through intersections more quickly. Conventional transportation approaches implement cooperation by pre-calculating the offsets between two intersections. Such pre-calculated offsets are not suitable for dynamic traffic environments. To enable cooperation of traffic signals, in this paper, we propose a model, CoLight, which uses graph attentional networks to facilitate communication. Specifically, for a target intersection in a network, CoLight can not only incorporate the temporal and spatial influences of neighboring intersections to the target intersection, but also build up index-free modeling of neighboring intersections. To the best of our knowledge, we are the first to use graph attentional networks in the setting of reinforcement learning for traffic signal control and to conduct experiments on the large-scale road network with hundreds of traffic signals. In experiments, we demonstrate that by learning the communication, the proposed model can achieve superior performance against the state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {68Txx,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/jeroen/Zotero/storage/72C9U98C/Wei et al. - 2019 - CoLight Learning Network-level Cooperation for Tr.pdf;/home/jeroen/Zotero/storage/Z6M9HYCP/1905.html}
}

@article{weiDeepReinforcementLearning2019,
  title = {Deep {{Reinforcement Learning}} for {{Traffic Signal Control}} along {{Arterials}}},
  author = {Wei, Hua and Chen, Chacha and Wu, Kan and Zheng, Guanjie and Yu, Zhengyao and Gayah, Vikash and Li, Zhenhui},
  year = {2019},
  abstract = {Arterial streets serve as the principal undertaker for urban mobility in a typical urban road network. In this paper, we propose a novel decentralized reinforcement learning method for multiintersection traffic signal control on arterial traffic, by applying reinforcement learning control agents in each intersection. While applying individual control to multi-intersection problems faces many challenges, two main adjustments are made to optimize the overall performance: 1) to provide simple yet novel contextual information to individual agents and 2) to train the RL agents in a transfer learning way. We test our method on synthetic data dataset and show that our proposed method outperforms the state-of-theart methods. We also interpret the policies learned by our method, which is the first time that the policy learned by the reinforcement learning control agents is interpreted using the traditional transportation coordination method on the arterial.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/46P599NU/Wei et al. - 2019 - Deep Reinforcement Learning for Traffic Signal Con.pdf}
}

@inproceedings{weiIntelliLightReinforcementLearning2018,
  title = {{{IntelliLight}}: {{A Reinforcement Learning Approach}} for {{Intelligent Traffic Light Control}}},
  shorttitle = {{{IntelliLight}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Wei, Hua and Zheng, Guanjie and Yao, Huaxiu and Li, Zhenhui},
  year = {2018},
  month = jul,
  pages = {2496--2505},
  publisher = {ACM},
  address = {London United Kingdom},
  doi = {10.1145/3219819.3220096},
  urldate = {2023-09-23},
  isbn = {978-1-4503-5552-0},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/NYXVAYX8/Wei et al. - 2018 - IntelliLight A Reinforcement Learning Approach fo.pdf}
}

@inproceedings{weiPressLightLearningMax2019,
  title = {{{PressLight}}: {{Learning Max Pressure Control}} to {{Coordinate Traffic Signals}} in {{Arterial Network}}},
  shorttitle = {{{PressLight}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Wei, Hua and Chen, Chacha and Zheng, Guanjie and Wu, Kan and Gayah, Vikash and Xu, Kai and Li, Zhenhui},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {1290--1298},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3292500.3330949},
  urldate = {2023-09-25},
  abstract = {Traffic signal control is essential for transportation efficiency in road networks. It has been a challenging problem because of the complexity in traffic dynamics. Conventional transportation research suffers from the incompetency to adapt to dynamic traffic situations. Recent studies propose to use reinforcement learning (RL) to search for more efficient traffic signal plans. However, most existing RL-based studies design the key elements - reward and state - in a heuristic way. This results in highly sensitive performances and a long learning process. To avoid the heuristic design of RL elements, we propose to connect RL with recent studies in transportation research. Our method is inspired by the state-of-the-art method max pressure (MP) in the transportation field. The reward design of our method is well supported by the theory in MP, which can be proved to be maximizing the throughput of the traffic network, i.e., minimizing the overall network travel time. We also show that our concise state representation can fully support the optimization of the proposed reward function. Through comprehensive experiments, we demonstrate that our method outperforms both conventional transportation approaches and existing learning-based methods.},
  isbn = {978-1-4503-6201-6},
  keywords = {deep reinforcement learning,multi-agent system,traffic signal control},
  file = {/home/jeroen/Zotero/storage/RV86D9SB/Wei et al. - 2019 - PressLight Learning Max Pressure Control to Coord.pdf}
}

@article{weisbrodMeasuringEconomicCosts2003,
  title = {Measuring {{Economic Costs}} of {{Urban Traffic Congestion}} to {{Business}}},
  author = {Weisbrod, Glen and Vary, Don and Treyz, George},
  year = {2003},
  month = jan,
  journal = {Transportation Research Record: Journal of the Transportation Research Board},
  volume = {1839},
  number = {1},
  pages = {98--106},
  issn = {0361-1981, 2169-4052},
  doi = {10.3141/1839-10},
  urldate = {2023-01-10},
  abstract = {Key findings are provided from NCHRP Study 2-21, which examined how urban traffic congestion imposes economic costs within metropolitan areas. Specifically, the study applied data from Chicago and Philadelphia to examine how various producers of economic goods and services are sensitive to congestion, through its impact on business costs, productivity, and output levels. The data analysis showed that sensitivity to traffic congestion varies by industry sector and is attributable to differences in each industry sector's mix of required inputs and hence its reliance on access to skilled labor, access to specialized inputs, and access to a large, transportation-based market area. Statistical analysis models were applied with the local data to demonstrate how congestion effectively shrinks business market areas and reduces the "agglomeration economies" of businesses operating in large urban areas, thus raising production costs. Overall, this research illustrates how it is possible to estimate the economic implications of congestion, an approach that may be applied in the future for benefit-cost analysis of urban congestion-reduction strategies or for development of congestion pricing strategies. The analysis also shows how congestion-reduction strategies can induce additional traffic as a result of economic benefits.},
  langid = {english}
}

@misc{weiSurveyTrafficSignal2020,
  title = {A {{Survey}} on {{Traffic Signal Control Methods}}},
  author = {Wei, Hua and Zheng, Guanjie and Gayah, Vikash and Li, Zhenhui},
  year = {2020},
  month = jan,
  number = {arXiv:1904.08117},
  eprint = {1904.08117},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-01-10},
  abstract = {Traffic signal control is an important and challenging real-world problem, which aims to minimize the travel time of vehicles by coordinating their movements at the road intersections. Current traffic signal control systems in use still rely heavily on oversimplified information and rule-based methods, although we now have richer data, more computing power and advanced methods to drive the development of intelligent transportation. With the growing interest in intelligent transportation using machine learning methods like reinforcement learning, this survey covers the widely acknowledged transportation approaches and a comprehensive list of recent literature on reinforcement for traffic signal control. We hope this survey can foster interdisciplinary research on this important topic.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68Txx,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/8NW67ZJX/Wei et al. - 2020 - A Survey on Traffic Signal Control Methods.pdf}
}

@inproceedings{wittJobShopScheduling2010,
  title = {Job {{Shop Scheduling}} with {{Buffer Constraints}} and {{Jobs Consuming Variable Buffer Space}}},
  booktitle = {Advanced {{Manufacturing}} and {{Sustainable Logistics}}},
  author = {Witt, Andreas and Vo{\ss}, Stefan},
  editor = {Dangelmaier, Wilhelm and Blecken, Alexander and Delius, Robin and Kl{\"o}pfer, Stefan},
  year = {2010},
  pages = {295--307},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-12494-5_27},
  abstract = {Job shop scheduling is among the most well studied problems in production planning. Yet, there is still a lack of comprehensive treatment of intermediate buffers. This paper deals with limited intermediate buffers with the important extension that not all jobs need to be handled in the same way regarding buffer space consumption. A heuristic to find feasible solutions is described and computational results are presented. While a reference procedure from literature devoted to case of unit buffer space consumption does not guarantee feasibility in all cases, the approach developed in this paper does. Moreover, it allows for variable space consumption, a feature which is new to literature paying better attention to more practical requirements than other models.},
  isbn = {978-3-642-12494-5},
  langid = {english},
  keywords = {buffer,generation scheme,job shop},
  file = {/home/jeroen/Zotero/storage/AEDK8PVG/Witt and Voß - 2010 - Job Shop Scheduling with Buffer Constraints and Jobs Consuming Variable Buffer Space.pdf}
}

@inproceedings{wittJobShopScheduling2010a,
  title = {Job {{Shop Scheduling}} with {{Buffer Constraints}} and {{Jobs Consuming Variable Buffer Space}}},
  booktitle = {Advanced {{Manufacturing}} and {{Sustainable Logistics}}},
  author = {Witt, Andreas and Vo{\ss}, Stefan},
  editor = {Dangelmaier, Wilhelm and Blecken, Alexander and Delius, Robin and Kl{\"o}pfer, Stefan},
  year = {2010},
  pages = {295--307},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-12494-5_27},
  abstract = {Job shop scheduling is among the most well studied problems in production planning. Yet, there is still a lack of comprehensive treatment of intermediate buffers. This paper deals with limited intermediate buffers with the important extension that not all jobs need to be handled in the same way regarding buffer space consumption. A heuristic to find feasible solutions is described and computational results are presented. While a reference procedure from literature devoted to case of unit buffer space consumption does not guarantee feasibility in all cases, the approach developed in this paper does. Moreover, it allows for variable space consumption, a feature which is new to literature paying better attention to more practical requirements than other models.},
  isbn = {978-3-642-12494-5},
  langid = {english},
  keywords = {buffer,generation scheme,job shop},
  file = {/home/jeroen/Zotero/storage/TKX6DP2I/Witt and Voß - 2010 - Job Shop Scheduling with Buffer Constraints and Jobs Consuming Variable Buffer Space.pdf}
}

@article{wuComprehensiveSurveyGraph2021,
  title = {A {{Comprehensive Survey}} on {{Graph Neural Networks}}},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  year = {2021},
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {1},
  eprint = {1901.00596},
  primaryclass = {cs},
  pages = {4--24},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2020.2978386},
  urldate = {2024-12-06},
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/52VSKYBX/Wu et al. - 2021 - A Comprehensive Survey on Graph Neural Networks.pdf;/home/jeroen/Zotero/storage/SH9QP44C/1901.html}
}

@misc{wuNeuralCombinatorialOptimization2024,
  title = {Neural {{Combinatorial Optimization Algorithms}} for {{Solving Vehicle Routing Problems}}: {{A Comprehensive Survey}} with {{Perspectives}}},
  shorttitle = {Neural {{Combinatorial Optimization Algorithms}} for {{Solving Vehicle Routing Problems}}},
  author = {Wu, Xuan and Wang, Di and Wen, Lijie and Xiao, Yubin and Wu, Chunguo and Wu, Yuesong and Yu, Chaoyu and Maskell, Douglas L. and Zhou, You},
  year = {2024},
  month = oct,
  number = {arXiv:2406.00415},
  eprint = {2406.00415},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.00415},
  urldate = {2025-01-16},
  abstract = {Although several surveys on Neural Combinatorial Optimization (NCO) solvers specifically designed to solve Vehicle Routing Problems (VRPs) have been conducted. These existing surveys did not cover the state-of-the-art (SOTA) NCO solvers emerged recently. More importantly, to provide a comprehensive taxonomy of NCO solvers with up-to-date coverage, based on our thorough review of relevant publications and preprints, we divide all NCO solvers into four distinct categories, namely Learning to Construct, Learning to Improve, Learning to Predict-Once, and Learning to Predict-Multiplicity solvers. Subsequently, we present the inadequacies of the SOTA solvers, including poor generalization, incapability to solve large-scale VRPs, inability to address most types of VRP variants simultaneously, and difficulty in comparing these NCO solvers with the conventional Operations Research algorithms. Simultaneously, we propose promising and viable directions to overcome these inadequacies. In addition, we compare the performance of representative NCO solvers from the Reinforcement, Supervised, and Unsupervised Learning paradigms across both small- and large-scale VRPs. Finally, following the proposed taxonomy, we provide an accompanying web page as a live repository for NCO solvers. Through this survey and the live repository, we hope to make the research community of NCO solvers for VRPs more thriving.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/jeroen/Zotero/storage/D9CMJ36Y/Wu et al. - 2024 - Neural Combinatorial Optimization Algorithms for Solving Vehicle Routing Problems A Comprehensive S.pdf;/home/jeroen/Zotero/storage/T36DE4JV/2406.html}
}

@misc{xuHowPowerfulAre2019,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  year = {2019},
  month = feb,
  number = {arXiv:1810.00826},
  eprint = {1810.00826},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-11-02},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/SZ8UX9HI/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf;/home/jeroen/Zotero/storage/TJK4HCM3/1810.html}
}

@article{xuLeveragingTransformerModel2022,
  title = {Leveraging {{Transformer Model}} to {{Predict Vehicle Trajectories}} in {{Congested Urban Traffic}}},
  author = {Xu, Yufei and Wang, Yu and Peeta, Srinivas},
  year = {2022},
  month = aug,
  journal = {Transportation Research Record: Journal of the Transportation Research Board},
  pages = {036119812211095},
  issn = {0361-1981, 2169-4052},
  doi = {10.1177/03611981221109594},
  urldate = {2023-01-20},
  abstract = {Accurate vehicle trajectory prediction enables safe, comfortable, and optimal proactive motion planning for connected and autonomous vehicles (CAVs). Because of rapid advances in learning techniques and increasing access to massive amounts of data, deep learning techniques have been applied to predict vehicle trajectories, especially the long short-term memory (LSTM) model. However, the accurate prediction of vehicle trajectories for congested urban traffic remains problematic, as existing LSTM models do not perform well. To address this gap, this paper proposes to leverage an emerging deep learning technique---transformer---and utilizes a recently released dataset (pNEUMA) for predicting vehicle trajectories in congested urban traffic. The proposed transformer model uses the self-attention mechanism, which helps to identify dependencies within the model inputs, to systematically determine the impacts of vehicular interactions on the target vehicle's future trajectory. The pNEUMA dataset, which provides drone-based large-scale data of congested urban traffic, is processed to fit a typical trajectory prediction scenario, and used to train the transformer model. Numerical studies are conducted to analyze the effectiveness of the proposed modeling approach. A comparison of the proposed model with representative LSTM models highlights the advantages of leveraging the transformer model characteristics for the vehicle trajectory prediction of congested urban traffic. By contrast, existing LSTM models may suffice for the trajectory prediction of freeway traffic. The results also indicate that, unlike for vehicle trajectory prediction for freeway traffic, a longer time window of inputs does not guarantee better prediction performance for congested urban traffic.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/73KTKYFW/Xu et al. - 2022 - Leveraging Transformer Model to Predict Vehicle Tr.pdf}
}

@article{yaoRethinkingSupervisedLearning2024,
  title = {Rethinking {{Supervised Learning}} Based {{Neural Combinatorial Optimization}} for {{Routing Problem}}},
  author = {Yao, Shunyu and Lin, Xi and Wang, Jiashu and Zhang, Qingfu and Wang, Zhenkun},
  year = {2024},
  month = oct,
  journal = {ACM Transactions on Evolutionary Learning and Optimization},
  pages = {3694690},
  issn = {2688-3007},
  doi = {10.1145/3694690},
  urldate = {2025-01-16},
  abstract = {Neural combinatorial optimization (NCO) is a promising learning-based approach to solving complex combinatorial optimization problems such as the traveling salesman problem (TSP), the vehicle routing problem (VRP), and the orienteering problem (OP). However, how to efficiently train a powerful NCO solver for routing problems remains a crucial challenge. The widely used reinforcement learning method suffers from sparse rewards and low data efficiency, while the supervised learning approach requires a large number of high-quality solutions (i.e., labels) that could be costly to obtain. In this work, we find that simple data augmentation operations can drastically reduce the number of required high-quality solutions for supervised learning. Moreover, simple boosting strategies that leverage the property of multiple optima can significantly improve training efficiency. With only a small set of                                {\textbackslash}(50,000{\textbackslash})                              labeled instances, supervised learning can achieve a competitive in-distribution performance with the widely-used reinforcement learning counterpart. Furthermore, we also investigate the generalization ability for larger out-of-distribution problems. We believe the findings from this work may lead to a rethinking of the value of data-efficient supervised learning for NCO solver training.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/XFV29QVL/Yao et al. - 2024 - Rethinking Supervised Learning based Neural Combinatorial Optimization for Routing Problem.pdf}
}

@article{yaoRethinkingSupervisedLearning2024b,
  title = {Rethinking {{Supervised Learning}} Based {{Neural Combinatorial Optimization}} for {{Routing Problem}}},
  author = {Yao, Shunyu and Lin, Xi and Wang, Jiashu and Zhang, Qingfu and Wang, Zhenkun},
  year = {2024},
  month = oct,
  journal = {ACM Trans. Evol. Learn. Optim.},
  doi = {10.1145/3694690},
  urldate = {2025-01-20},
  abstract = {Neural combinatorial optimization (NCO) is a promising learning-based approach to solving complex combinatorial optimization problems such as the traveling salesman problem (TSP), the vehicle routing problem (VRP), and the orienteering problem (OP). However, how to efficiently train a powerful NCO solver for routing problems remains a crucial challenge. The widely used reinforcement learning method suffers from sparse rewards and low data efficiency, while the supervised learning approach requires a large number of high-quality solutions (i.e., labels) that could be costly to obtain. In this work, we find that simple data augmentation operations can drastically reduce the number of required high-quality solutions for supervised learning. Moreover, simple boosting strategies that leverage the property of multiple optima can significantly improve training efficiency. With only a small set of  {\textbackslash}(50,000{\textbackslash})  labeled instances, supervised learning can achieve a competitive in-distribution performance with the widely-used reinforcement learning counterpart. Furthermore, we also investigate the generalization ability for larger out-of-distribution problems. We believe the findings from this work may lead to a rethinking of the value of data-efficient supervised learning for NCO solver training.},
  annotation = {Just Accepted},
  file = {/home/jeroen/Zotero/storage/U9NZ34DJ/Yao et al. - 2024 - Rethinking Supervised Learning based Neural Combinatorial Optimization for Routing Problem.pdf}
}

@misc{YaoxinWu,
  title = {Yaoxin {{Wu}}},
  urldate = {2024-12-06},
  howpublished = {https://www.tue.nl/en/research/researchers/yaoxin-wu},
  langid = {english}
}

@article{yauSurveyReinforcementLearning2018,
  title = {A {{Survey}} on {{Reinforcement Learning Models}} and {{Algorithms}} for {{Traffic Signal Control}}},
  author = {Yau, Kok-Lim Alvin and Qadir, Junaid and Khoo, Hooi Ling and Ling, Mee Hong and Komisarczuk, Peter},
  year = {2018},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {50},
  number = {3},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3068287},
  urldate = {2023-09-26},
  abstract = {Traffic congestion has become a vexing and complex issue in many urban areas. Of particular interest are the intersections where traffic bottlenecks are known to occur despite being traditionally signalized. Reinforcement learning (RL), which is an artificial intelligence approach, has been adopted in traffic signal control for monitoring and ameliorating traffic congestion. RL enables autonomous decision makers (e.g., traffic signal controllers) to observe, learn, and select the optimal action (e.g., determining the appropriate traffic phase and its timing) to manage traffic such that system performance is improved. This article reviews various RL models and algorithms applied to traffic signal control in the aspects of the representations of the RL model (i.e., state, action, and reward), performance measures, and complexity to establish a foundation for further investigation in this research field. Open issues are presented toward the end of this article to discover new research areas with the objective to spark new interest in this research field.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/4UJSP6QX/Yau et al. - 2018 - A Survey on Reinforcement Learning Models and Algo.pdf}
}

@misc{zarembaLearningExecute2015,
  title = {Learning to {{Execute}}},
  author = {Zaremba, Wojciech and Sutskever, Ilya},
  year = {2015},
  month = feb,
  number = {arXiv:1410.4615},
  eprint = {1410.4615},
  publisher = {arXiv},
  urldate = {2024-10-31},
  abstract = {Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99\% accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/jeroen/Zotero/storage/EXDGR9TA/Zaremba and Sutskever - 2015 - Learning to Execute.pdf;/home/jeroen/Zotero/storage/WDU5KWB8/1410.html}
}

@misc{zarembaReinforcementLearningNeural2016,
  title = {Reinforcement {{Learning Neural Turing Machines}} - {{Revised}}},
  author = {Zaremba, Wojciech and Sutskever, Ilya},
  year = {2016},
  month = jan,
  number = {arXiv:1505.00521},
  eprint = {1505.00521},
  publisher = {arXiv},
  urldate = {2024-10-31},
  abstract = {The Neural Turing Machine (NTM) is more expressive than all previously considered models because of its external memory. It can be viewed as a broader effort to use abstract external Interfaces and to learn a parametric model that interacts with them. The capabilities of a model can be extended by providing it with proper Interfaces that interact with the world. These external Interfaces include memory, a database, a search engine, or a piece of software such as a theorem verifier. Some of these Interfaces are provided by the developers of the model. However, many important existing Interfaces, such as databases and search engines, are discrete. We examine feasibility of learning models to interact with discrete Interfaces. We investigate the following discrete Interfaces: a memory Tape, an input Tape, and an output Tape. We use a Reinforcement Learning algorithm to train a neural network that interacts with such Interfaces to solve simple algorithmic tasks. Our Interfaces are expressive enough to make our model Turing complete.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/JJPECT72/Zaremba and Sutskever - 2016 - Reinforcement Learning Neural Turing Machines - Revised.pdf;/home/jeroen/Zotero/storage/22GHHRRE/1505.html}
}

@inproceedings{zhangCityFlowMultiAgentReinforcement2019,
  title = {{{CityFlow}}: {{A Multi-Agent Reinforcement Learning Environment}} for {{Large Scale City Traffic Scenario}}},
  shorttitle = {{{CityFlow}}},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Zhang, Huichu and Feng, Siyuan and Liu, Chang and Ding, Yaoyao and Zhu, Yichen and Zhou, Zihan and Zhang, Weinan and Yu, Yong and Jin, Haiming and Li, Zhenhui},
  year = {2019},
  month = may,
  eprint = {1905.05217},
  primaryclass = {cs},
  pages = {3620--3624},
  doi = {10.1145/3308558.3314139},
  urldate = {2023-09-25},
  abstract = {Traffic signal control is an emerging application scenario for reinforcement learning. Besides being as an important problem that affects people's daily life in commuting, traffic signal control poses its unique challenges for reinforcement learning in terms of adapting to dynamic traffic environment and coordinating thousands of agents including vehicles and pedestrians. A key factor in the success of modern reinforcement learning relies on a good simulator to generate a large number of data samples for learning. The most commonly used open-source traffic simulator SUMO is, however, not scalable to large road network and large traffic flow, which hinders the study of reinforcement learning on traffic scenarios. This motivates us to create a new traffic simulator CityFlow with fundamentally optimized data structures and efficient algorithms. CityFlow can support flexible definitions for road network and traffic flow based on synthetic and real-world data. It also provides user-friendly interface for reinforcement learning. Most importantly, CityFlow is more than twenty times faster than SUMO and is capable of supporting city-wide traffic simulation with an interactive render for monitoring. Besides traffic signal control, CityFlow could serve as the base for other transportation studies and can create new possibilities to test machine learning methods in the intelligent transportation domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/jeroen/Zotero/storage/L9EDGC59/Zhang et al. - 2019 - CityFlow A Multi-Agent Reinforcement Learning Env.pdf;/home/jeroen/Zotero/storage/VGJXZ99T/1905.html}
}

@misc{zhangDeepReinforcementLearning2024,
  title = {Deep {{Reinforcement Learning Guided Improvement Heuristic}} for {{Job Shop Scheduling}}},
  author = {Zhang, Cong and Cao, Zhiguang and Song, Wen and Wu, Yaoxin and Zhang, Jie},
  year = {2024},
  month = feb,
  number = {arXiv:2211.10936},
  eprint = {2211.10936},
  publisher = {arXiv},
  urldate = {2024-11-15},
  abstract = {Recent studies in using deep reinforcement learning (DRL) to solve Job-shop scheduling problems (JSSP) focus on construction heuristics. However, their performance is still far from optimality, mainly because the underlying graph representation scheme is unsuitable for modelling partial solutions at each construction step. This paper proposes a novel DRL-guided improvement heuristic for solving JSSP, where graph representation is employed to encode complete solutions. We design a Graph Neural-Network-based representation scheme, consisting of two modules to effectively capture the information of dynamic topology and different types of nodes in graphs encountered during the improvement process. To speed up solution evaluation during improvement, we present a novel message-passing mechanism that can evaluate multiple solutions simultaneously. We prove that the computational complexity of our method scales linearly with problem size. Experiments on classic benchmarks show that the improvement policy learned by our method outperforms state-of-the-art DRL-based methods by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/jeroen/Zotero/storage/CXKHI6BE/Zhang et al. - 2024 - Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling.pdf;/home/jeroen/Zotero/storage/6MHL3V3S/2211.html}
}

@misc{zhangLearningDispatchJob2020,
  title = {Learning to {{Dispatch}} for {{Job Shop Scheduling}} via {{Deep Reinforcement Learning}}},
  author = {Zhang, Cong and Song, Wen and Cao, Zhiguang and Zhang, Jie and Tan, Puay Siew and Xu, Chi},
  year = {2020},
  month = oct,
  number = {arXiv:2010.12367},
  eprint = {2010.12367},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.12367},
  urldate = {2023-09-27},
  abstract = {Priority dispatching rule (PDR) is widely used for solving real-world Job-shop scheduling problem (JSSP). However, the design of effective PDRs is a tedious task, requiring a myriad of specialized knowledge and often delivering limited performance. In this paper, we propose to automatically learn PDRs via an end-to-end deep reinforcement learning agent. We exploit the disjunctive graph representation of JSSP, and propose a Graph Neural Network based scheme to embed the states encountered during solving. The resulting policy network is size-agnostic, effectively enabling generalization on large-scale instances. Experiments show that the agent can learn high-quality PDRs from scratch with elementary raw features, and demonstrates strong performance against the best existing PDRs. The learned policies also perform well on much larger instances that are unseen in training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/QP64X5B2/Zhang et al. - 2020 - Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning.pdf}
}

@inproceedings{zhangLearningDispatchJob2020a,
  title = {Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Zhang, Cong and Song, Wen and Cao, Zhiguang and Zhang, Jie and Tan, Puay Siew and Xu, Chi},
  year = {2020},
  month = dec,
  series = {{{NIPS}} '20},
  pages = {1621--1632},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-02-19},
  abstract = {Priority dispatching rule (PDR) is widely used for solving real-world Job-shop scheduling problem (JSSP). However, the design of effective PDRs is a tedious task, requiring a myriad of specialized knowledge and often delivering limited performance. In this paper, we propose to automatically learn PDRs via an end-to-end deep reinforcement learning agent. We exploit the disjunctive graph representation of JSSP, and propose a Graph Neural Network based scheme to embed the states encountered during solving. The resulting policy network is size-agnostic, effectively enabling generalization on large-scale instances. Experiments show that the agent can learn high-quality PDRs from scratch with elementary raw features, and demonstrates strong performance against the best existing PDRs. The learned policies also perform well on much larger instances that are unseen in training.},
  isbn = {978-1-7138-2954-6},
  file = {/home/jeroen/Zotero/storage/UT8W6W9Y/Zhang et al. - 2020 - Learning to dispatch for job shop scheduling via deep reinforcement learning.pdf}
}

@article{zhaoBilevelProgrammingModel2021,
  title = {A Bilevel Programming Model for Autonomous Intersection Control and Trajectory Planning},
  author = {Zhao, Weiming and Liu, Ronghui and Ngoduy, Dong},
  year = {2021},
  month = jan,
  journal = {Transportmetrica A: Transport Science},
  volume = {17},
  number = {1},
  pages = {34--58},
  issn = {2324-9935, 2324-9943},
  doi = {10.1080/23249935.2018.1563921},
  urldate = {2024-11-16},
  abstract = {Advances in autonomous and connected vehicles bring new opportunities for intelligent intersection control strategies. In this paper, we propose a centralised way to jointly integrate an intersection control problem with vehicle trajectory planning. It is formulated as a bilevel optimisation problem in which the upper level is designed to minimise the total travel time by a mixed integer linear programming (MILP) model. In contrast, the lower level is a linear programming (LP) model with an objective function to maximise the total speed entering the intersection. The two levels are coupled by the arrival time and terminal speed. By using the relationship between the safe time headway and the process time, a novel platoon based method is developed to reduce the computational burden. Finally, simulation tests are carried out to investigate the control performance under different demands, intersection lengths, communication ranges and traffic compositions.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/QDA2JRRB/Zhao et al. - 2021 - A bilevel programming model for autonomous intersection control and trajectory planning.pdf}
}

@article{zhaoBilevelProgrammingModel2021a,
  title = {A Bilevel Programming Model for Autonomous Intersection Control and Trajectory Planning},
  author = {Zhao, Weiming and Liu, Ronghui and Ngoduy, Dong},
  year = {2021},
  month = jan,
  journal = {Transportmetrica A: Transport Science},
  publisher = {Taylor \& Francis},
  issn = {2324-9935},
  urldate = {2024-11-25},
  abstract = {Advances in autonomous and connected vehicles bring new opportunities for intelligent intersection control strategies. In this paper, we propose a centralised way to jointly integrate an intersecti...},
  copyright = {{\copyright} 2019 Hong Kong Society for Transportation Studies Limited},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/CAYQV3KD/Zhao et al. - 2021 - A bilevel programming model for autonomous intersection control and trajectory planning.pdf;/home/jeroen/Zotero/storage/HLA6C45G/23249935.2018.html}
}

@misc{zhengConstrainedUpperConfidence2020,
  title = {Constrained {{Upper Confidence Reinforcement Learning}}},
  author = {Zheng, Liyuan and Ratliff, Lillian J.},
  year = {2020},
  month = jan,
  number = {arXiv:2001.09377},
  eprint = {2001.09377},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-12-13},
  abstract = {Constrained Markov Decision Processes are a class of stochastic decision problems in which the decision maker must select a policy that satisfies auxiliary cost constraints. This paper extends upper confidence reinforcement learning for settings in which the reward function and the constraints, described by cost functions, are unknown a priori but the transition kernel is known. Such a setting is well-motivated by a number of applications including exploration of unknown, potentially unsafe, environments. We present an algorithm C-UCRL and show that it achieves sub-linear regret (\$ O(T{\textasciicircum}\{{\textbackslash}frac\{3\}\{4\}\}{\textbackslash}sqrt\{{\textbackslash}log(T/{\textbackslash}delta)\})\$) with respect to the reward while satisfying the constraints even while learning with probability \$1-{\textbackslash}delta\$. Illustrative examples are provided.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/Zotero/storage/WJGU7JUL/Zheng and Ratliff - 2020 - Constrained Upper Confidence Reinforcement Learnin.pdf}
}

@article{zhengReinforcementLearningBased2022,
  title = {A {{Reinforcement Learning Based Traffic Control Strategy}} in a {{Macroscopic Fundamental Diagram Region}}},
  author = {Zheng, Lingyu and Wu, Bing},
  editor = {Jin, Peter J.},
  year = {2022},
  month = apr,
  journal = {Journal of Advanced Transportation},
  volume = {2022},
  pages = {1--12},
  issn = {2042-3195, 0197-6729},
  doi = {10.1155/2022/5681234},
  urldate = {2023-01-20},
  abstract = {Urban traffic control systems (UTCSs) are deployed to a great number of urban cities despite lacking feedback when adjusting the traffic signals. The development of reinforcement learning (RL) makes it possible to apply feedback to UTCS, and great efforts have been made on RL-based traffic control strategies. However, those studies are regardless of the traffic flow theory of the network and the road users' perspectives on the performance of traffic. This study proposes a multiagent reinforcement learning (MARL) based traffic control strategy, in which each intersection in a macroscopic fundamental diagram (MFD) region was controlled by one agent using the level of services (LOS) and MFD-based parameters as rewards. The proposed MARL strategy was evaluated by simulation in a 3{\texttimes}3 grid network compared with pretimed, actuated, and MFD-based traffic control strategies. The evaluation results showed that, at different demand levels, the proposed MARL strategy outperforms the other three traffic control strategies in terms of average intersection queue length and average intersection waiting time to a different extent. Results also showed that the proposed MARL dissipated the congestion faster than the other three control strategies. Results of the Friedman test indicated that the differences in performances between the proposed MARL and other strategies were statistically significant regardless of the demand level. The MFD in the testbed network controlled by the proposed MARL was different from that controlled by the pretimed strategy, especially the MFD scatter plot. It provides insights on considering the traffic flow theory of the network when applying MARL to traffic control strategies.},
  langid = {english},
  file = {/home/jeroen/Zotero/storage/2NVJYMTI/Zheng and Wu - 2022 - A Reinforcement Learning Based Traffic Control Str.pdf}
}
