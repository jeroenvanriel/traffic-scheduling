
@article{pan_survey_2010,
	title = {A {Survey} on {Transfer} {Learning}},
	volume = {22},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5288526/},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
	language = {en},
	number = {10},
	urldate = {2023-01-20},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	month = oct,
	year = {2010},
	pages = {1345--1359},
	file = {Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:/home/jeroen/Zotero/storage/TVZKQKGH/Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:application/pdf},
}

@article{zheng_reinforcement_2022,
	title = {A {Reinforcement} {Learning} {Based} {Traffic} {Control} {Strategy} in a {Macroscopic} {Fundamental} {Diagram} {Region}},
	volume = {2022},
	issn = {2042-3195, 0197-6729},
	url = {https://www.hindawi.com/journals/jat/2022/5681234/},
	doi = {10.1155/2022/5681234},
	abstract = {Urban traffic control systems (UTCSs) are deployed to a great number of urban cities despite lacking feedback when adjusting the traffic signals. The development of reinforcement learning (RL) makes it possible to apply feedback to UTCS, and great efforts have been made on RL-based traffic control strategies. However, those studies are regardless of the traffic flow theory of the network and the road users’ perspectives on the performance of traffic. This study proposes a multiagent reinforcement learning (MARL) based traffic control strategy, in which each intersection in a macroscopic fundamental diagram (MFD) region was controlled by one agent using the level of services (LOS) and MFD-based parameters as rewards. The proposed MARL strategy was evaluated by simulation in a 3×3 grid network compared with pretimed, actuated, and MFD-based traffic control strategies. The evaluation results showed that, at different demand levels, the proposed MARL strategy outperforms the other three traffic control strategies in terms of average intersection queue length and average intersection waiting time to a different extent. Results also showed that the proposed MARL dissipated the congestion faster than the other three control strategies. Results of the Friedman test indicated that the differences in performances between the proposed MARL and other strategies were statistically significant regardless of the demand level. The MFD in the testbed network controlled by the proposed MARL was different from that controlled by the pretimed strategy, especially the MFD scatter plot. It provides insights on considering the traffic flow theory of the network when applying MARL to traffic control strategies.},
	language = {en},
	urldate = {2023-01-20},
	journal = {Journal of Advanced Transportation},
	author = {Zheng, Lingyu and Wu, Bing},
	editor = {Jin, Peter J.},
	month = apr,
	year = {2022},
	pages = {1--12},
	file = {Zheng and Wu - 2022 - A Reinforcement Learning Based Traffic Control Str.pdf:/home/jeroen/Zotero/storage/2NVJYMTI/Zheng and Wu - 2022 - A Reinforcement Learning Based Traffic Control Str.pdf:application/pdf},
}

@incollection{chen_comparative_2022,
	address = {Singapore},
	title = {A {Comparative} {Study} of {Algorithms} for {Intelligent} {Traffic} {Signal} {Control}},
	volume = {269},
	isbn = {9789811679957 9789811679964},
	url = {https://link.springer.com/10.1007/978-981-16-7996-4_19},
	abstract = {In this paper, methods have been explored to effectively optimise traffic signal control to minimise waiting times and queue lengths, thereby increasing traffic flow. The traffic intersection was first defined as a Markov Decision Process, and a state representation, actions and rewards were chosen. Simulation of Urban MObility (SUMO) was used to simulate an intersection and then compare a Round Robin Scheduler, a Feedback Control mechanism and two Reinforcement Learning techniques - Deep Q Network (DQN) and Advantage Actor-Critic (A2C), as the policy for the traffic signal in the simulation under different scenarios. Finally, the methods were tested on a simulation of a real-world intersection in Bengaluru, India.},
	language = {en},
	urldate = {2023-01-20},
	booktitle = {Machine {Learning} and {Autonomous} {Systems}},
	publisher = {Springer Nature Singapore},
	author = {Chaudhuri, Hrishit and Masti, Vibha and Veerendranath, Vishruth and Natarajan, S.},
	editor = {Chen, Joy Iong-Zong and Wang, Haoxiang and Du, Ke-Lin and Suma, V.},
	year = {2022},
	doi = {10.1007/978-981-16-7996-4_19},
	note = {Series Title: Smart Innovation, Systems and Technologies},
	pages = {271--287},
	file = {Chaudhuri et al. - 2022 - A Comparative Study of Algorithms for Intelligent .pdf:/home/jeroen/Zotero/storage/BYK43CTD/Chaudhuri et al. - 2022 - A Comparative Study of Algorithms for Intelligent .pdf:application/pdf},
}

@article{noaeen_reinforcement_2022,
	title = {Reinforcement learning in urban network traffic signal control: {A} systematic literature review},
	volume = {199},
	issn = {09574174},
	shorttitle = {Reinforcement learning in urban network traffic signal control},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422002858},
	doi = {10.1016/j.eswa.2022.116830},
	abstract = {Improvement of traffic signal control (TSC) efficiency has been found to lead to improved urban transportation and enhanced quality of life. Recently, the use of reinforcement learning (RL) in various areas of TSC has gained significant traction; thus, we conducted a systematic literature review as a systematic, comprehensive, and reproducible review to dissect all the existing research that applied RL in the network-level TSC domain, called as RL in NTSC or RL-NTSC for brevity. The review only targeted the network-level articles that tested the proposed methods in networks with two or more intersections. This review covers 160 peer-reviewed articles from 30 countries published from 1994 to March 2020. The goal of this study is to provide the research community with statistical and conceptual knowledge, summarize existence evidence, characterize RL applications in NTSC domains, explore all applied methods and major first events in the defined scope, and identify areas for further research based on the explored research problems in current research. We analyzed the extracted data from the included articles in the following seven categories: (i) publication and authors’ data, (ii) method identification and analysis, (iii) environment attributes and traffic simulation, (iv) application domains of RL-NTSC, (v) major first events of RL-NTSC and authors’ key statements, (vi) code availability, and (vii) evaluation. This paper provides a comprehensive view of the past 26 years of research on applying RL to NTSC. It also reveals the role of advancing deep learning methods in the revival of the research area, the rise of using non-commercial microscopic traffic simulators, a lack of interaction between traffic and transportation engineering practitioners and researchers, and a lack of proposal and creation of testbeds which can likely bring different communities together around common goals.},
	language = {en},
	urldate = {2023-01-20},
	journal = {Expert Systems with Applications},
	author = {Noaeen, Mohammad and Naik, Atharva and Goodman, Liana and Crebo, Jared and Abrar, Taimoor and Abad, Zahra Shakeri Hossein and Bazzan, Ana L.C. and Far, Behrouz},
	month = aug,
	year = {2022},
	pages = {116830},
	file = {Noaeen et al. - 2022 - Reinforcement learning in urban network traffic si.pdf:/home/jeroen/Zotero/storage/37SRIMR9/Noaeen et al. - 2022 - Reinforcement learning in urban network traffic si.pdf:application/pdf},
}

@phdthesis{oblakova_queueing_2019,
	address = {Enschede, The Netherlands},
	type = {{PhD}},
	title = {Queueing models for urban traffic networks},
	url = {http://purl.org/utwente/doi/10.3990/1.9789036548472},
	language = {en},
	urldate = {2023-01-20},
	school = {University of Twente},
	author = {Oblakova, A.},
	month = sep,
	year = {2019},
	doi = {10.3990/1.9789036548472},
	note = {ISBN: 9789036548472},
	file = {Oblakova - 2019 - Queueing models for urban traffic networks.pdf:/home/jeroen/Zotero/storage/KSB4JAIW/Oblakova - 2019 - Queueing models for urban traffic networks.pdf:application/pdf},
}

@article{little_versatile_nodate,
	title = {A {Versatile} {Program} for {Setting} {Signals} on {Arteries} and {Triangular} {Networks}},
	language = {en},
	author = {Little, John D C and Kelson, Mark D and Gartner, Nathan H},
	annote = {MAXBAND
},
	file = {Little et al. - A Versatile Program for Setting Signals on Arterie.pdf:/home/jeroen/Zotero/storage/5UZLGK52/Little et al. - A Versatile Program for Setting Signals on Arterie.pdf:application/pdf},
}

@article{hoogendoorn_model-based_nodate,
	title = {Model-based {Stochastic} {Control} of {Traffic} {Networks}},
	abstract = {Uncertainty of traffic network operations has been a subject of lively debate in the last decade. However, little efforts have been put in developing control frameworks that are not only aimed at improving the mean performance of the system, but also at improving the system robustness and reliability. In fact, it can be argued that most of the current control approaches are only aimed at improving the efficiency, which can even be counterproductive from a robustness point of view.},
	language = {en},
	author = {Hoogendoorn, S P and Knoop, V L and van Zuylen, H J},
	file = {Hoogendoorn et al. - Model-based Stochastic Control of Traffic Networks.pdf:/home/jeroen/Zotero/storage/LZMGC6NL/Hoogendoorn et al. - Model-based Stochastic Control of Traffic Networks.pdf:application/pdf},
}

@article{haydari_deep_2022,
	title = {Deep {Reinforcement} {Learning} for {Intelligent} {Transportation} {Systems}: {A} {Survey}},
	volume = {23},
	issn = {1524-9050, 1558-0016},
	shorttitle = {Deep {Reinforcement} {Learning} for {Intelligent} {Transportation} {Systems}},
	url = {https://ieeexplore.ieee.org/document/9146378/},
	doi = {10.1109/TITS.2020.3008612},
	abstract = {Latest technological improvements increased the quality of transportation. New data-driven approaches bring out a new research direction for all control-based systems, e.g., in transportation, robotics, IoT and power systems. Combining data-driven applications with transportation systems plays a key role in recent transportation applications. In this paper, the latest deep reinforcement learning (RL) based trafﬁc control applications are surveyed. Speciﬁcally, trafﬁc signal control (TSC) applications based on (deep) RL, which have been studied extensively in the literature, are discussed in detail. Different problem formulations, RL parameters, and simulation environments for TSC are discussed comprehensively. In the literature, there are also several autonomous driving applications studied with deep RL models. Our survey extensively summarizes existing works in this ﬁeld by categorizing them with respect to application types, control models and studied algorithms. In the end, we discuss the challenges and open questions regarding deep RL-based transportation applications.},
	language = {en},
	number = {1},
	urldate = {2023-01-20},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Haydari, Ammar and Yilmaz, Yasin},
	month = jan,
	year = {2022},
	pages = {11--32},
	file = {Haydari and Yilmaz - 2022 - Deep Reinforcement Learning for Intelligent Transp.pdf:/home/jeroen/Zotero/storage/ZAD9J8I2/Haydari and Yilmaz - 2022 - Deep Reinforcement Learning for Intelligent Transp.pdf:application/pdf},
}

@inproceedings{Pol2016CoordinatedDR,
	title = {Coordinated deep reinforcement learners for traffic light control},
	author = {van der Pol, Elise and Oliehoek, Frans A.},
	year = {2016},
	file = {van der Pol and Oliehoek - 2016 - Coordinated deep reinforcement learners for traffi.pdf:/home/jeroen/Zotero/storage/ASFV2PZ9/van der Pol and Oliehoek - 2016 - Coordinated deep reinforcement learners for traffi.pdf:application/pdf},
}

@inproceedings{shen_fast_2017,
	address = {Guangzhou, China},
	title = {A {Fast} {Method} to {Prevent} {Traffic} {Blockage} by {Signal} {Control} {Based} on {Reinforcement} {Learning}},
	isbn = {978-94-6252-312-8},
	url = {http://www.atlantis-press.com/php/paper-details.php?id=25872577},
	doi = {10.2991/ceie-16.2017.36},
	language = {en},
	urldate = {2023-01-20},
	booktitle = {Proceedings of the {International} {Conference} on {Communication} and {Electronic} {Information} {Engineering} ({CEIE} 2016)},
	publisher = {Atlantis Press},
	author = {Shen, Mengjia},
	year = {2017},
	file = {Shen - 2017 - A Fast Method to Prevent Traffic Blockage by Signa.pdf:/home/jeroen/Zotero/storage/2QU3WDTM/Shen - 2017 - A Fast Method to Prevent Traffic Blockage by Signa.pdf:application/pdf},
}

@inproceedings{bouderba_reinforcement_2019,
	address = {Rabat Morocco},
	title = {Reinforcement {Learning} ({Q}-{LEARNING}) traffic light controller within intersection traffic system},
	isbn = {978-1-4503-7240-4},
	url = {https://dl.acm.org/doi/10.1145/3372938.3372999},
	doi = {10.1145/3372938.3372999},
	abstract = {In this paper we study the effect of signalized traffic intersection control strategies in a cellular automaton model for transportation in urban networks. Starting with a simple synchronized strategy, then a green wave which gave a surprising result. Finally, a reinforcement learning approach (Q-LEARNING) is presented to learn the traffic light controller how to interact with drivers with different situation. By keeping a belief the level of cooperation drivers, we improved the performance of Q-LEARNING algorithms. We show that our traffic light controller successfully learns how to manage the intersection with less deadlocks than without learning.},
	language = {en},
	urldate = {2023-01-20},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Big} {Data} and {Internet} of {Things}},
	publisher = {ACM},
	author = {Bouderba, Saif Islam and Moussa, Najem},
	month = oct,
	year = {2019},
	pages = {1--6},
	file = {Bouderba and Moussa - 2019 - Reinforcement Learning (Q-LEARNING) traffic light .pdf:/home/jeroen/Zotero/storage/7PLTGWMR/Bouderba and Moussa - 2019 - Reinforcement Learning (Q-LEARNING) traffic light .pdf:application/pdf},
}

@book{stevanovic_adaptive_2010,
	address = {Washington, D.C.},
	title = {Adaptive {Traffic} {Control} {Systems}: {Domestic} and {Foreign} {State} of {Practice}},
	isbn = {978-0-309-28039-6},
	shorttitle = {Adaptive {Traffic} {Control} {Systems}},
	url = {http://www.nap.edu/catalog/14364},
	language = {en},
	urldate = {2023-01-20},
	publisher = {National Academies Press},
	author = {Stevanovic, Aleksandar and {Transportation Research Board} and {National Cooperative Highway Research Program Synthesis Program} and {Transportation Research Board}},
	month = apr,
	year = {2010},
	doi = {10.17226/14364},
	note = {Pages: 14364},
	file = {Stevanovic et al. - 2010 - Adaptive Traffic Control Systems Domestic and For.pdf:/home/jeroen/Zotero/storage/RU8A7VCB/Stevanovic et al. - 2010 - Adaptive Traffic Control Systems Domestic and For.pdf:application/pdf},
}

@misc{sumorl,
	title = {{SUMO}-{RL}},
	url = {https://github.com/LucasAlegre/sumo-rl},
	publisher = {GitHub},
	author = {Alegre, Lucas N.},
	year = {2019},
}

@article{sutton_dyna_1991,
	title = {Dyna, an integrated architecture for learning, planning, and reacting},
	volume = {2},
	issn = {0163-5719},
	url = {https://dl.acm.org/doi/10.1145/122344.122377},
	doi = {10.1145/122344.122377},
	abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
	language = {en},
	number = {4},
	urldate = {2023-01-10},
	journal = {ACM SIGART Bulletin},
	author = {Sutton, Richard S.},
	month = jul,
	year = {1991},
	pages = {160--163},
	file = {Submitted Version:/home/jeroen/Zotero/storage/ZUHCWX36/Sutton - 1991 - Dyna, an integrated architecture for learning, pla.pdf:application/pdf},
}

@article{nagel_cellular_1992,
	title = {A cellular automaton model for freeway traffic},
	volume = {2},
	issn = {1155-4304, 1286-4862},
	url = {http://www.edpsciences.org/10.1051/jp1:1992277},
	doi = {10.1051/jp1:1992277},
	number = {12},
	urldate = {2023-01-10},
	journal = {Journal de Physique I},
	author = {Nagel, Kai and Schreckenberg, Michael},
	month = dec,
	year = {1992},
	pages = {2221--2229},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	language = {en},
	number = {7676},
	urldate = {2023-01-10},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	pages = {354--359},
	file = {Submitted Version:/home/jeroen/Zotero/storage/EBUZTASC/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
}

@misc{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv:1312.5602 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: NIPS Deep Learning Workshop 2013},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/9KEM54CE/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/THQC2DX6/1312.html:text/html},
}

@misc{wei_survey_2020,
	title = {A {Survey} on {Traffic} {Signal} {Control} {Methods}},
	url = {http://arxiv.org/abs/1904.08117},
	abstract = {Traffic signal control is an important and challenging real-world problem, which aims to minimize the travel time of vehicles by coordinating their movements at the road intersections. Current traffic signal control systems in use still rely heavily on oversimplified information and rule-based methods, although we now have richer data, more computing power and advanced methods to drive the development of intelligent transportation. With the growing interest in intelligent transportation using machine learning methods like reinforcement learning, this survey covers the widely acknowledged transportation approaches and a comprehensive list of recent literature on reinforcement for traffic signal control. We hope this survey can foster interdisciplinary research on this important topic.},
	language = {en},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Wei, Hua and Zheng, Guanjie and Gayah, Vikash and Li, Zhenhui},
	month = jan,
	year = {2020},
	note = {arXiv:1904.08117 [cs, stat]},
	keywords = {Computer Science - Machine Learning, 68Txx, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {Comment: 32 pages},
	file = {Wei et al. - 2020 - A Survey on Traffic Signal Control Methods.pdf:/home/jeroen/Zotero/storage/8NW67ZJX/Wei et al. - 2020 - A Survey on Traffic Signal Control Methods.pdf:application/pdf},
}

@techreport{webster_traffic_1958,
	title = {Traffic signal settings},
	author = {Webster, F.V.},
	year = {1958},
}

@article{timmerman_platoon_2021,
	title = {Platoon forming algorithms for intelligent street intersections},
	volume = {17},
	issn = {2324-9935, 2324-9943},
	url = {https://www.tandfonline.com/doi/full/10.1080/23249935.2019.1692962},
	doi = {10.1080/23249935.2019.1692962},
	language = {en},
	number = {3},
	urldate = {2023-01-10},
	journal = {Transportmetrica A: Transport Science},
	author = {Timmerman, R. W. and Boon, M. A. A.},
	month = feb,
	year = {2021},
	pages = {278--307},
	file = {Full Text:/home/jeroen/Zotero/storage/7A98IPE5/Timmerman and Boon - 2021 - Platoon forming algorithms for intelligent street .pdf:application/pdf},
}

@inproceedings{lopez_microscopic_2018,
	address = {Maui, HI},
	title = {Microscopic {Traffic} {Simulation} using {SUMO}},
	isbn = {978-1-72810-321-1 978-1-72810-323-5},
	url = {https://ieeexplore.ieee.org/document/8569938/},
	doi = {10.1109/ITSC.2018.8569938},
	urldate = {2023-01-10},
	booktitle = {2018 21st {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	publisher = {IEEE},
	author = {Lopez, Pablo Alvarez and Wiessner, Evamarie and Behrisch, Michael and Bieker-Walz, Laura and Erdmann, Jakob and Flotterod, Yun-Pang and Hilbrich, Robert and Lucken, Leonhard and Rummel, Johannes and Wagner, Peter},
	month = nov,
	year = {2018},
	pages = {2575--2582},
	file = {Accepted Version:/home/jeroen/Zotero/storage/KV3FY2AH/Lopez et al. - 2018 - Microscopic Traffic Simulation using SUMO.pdf:application/pdf},
}

@misc{noauthor_notitle_nodate,
}

@article{van_leeuwaarden_delay_2006,
	title = {Delay {Analysis} for the {Fixed}-{Cycle} {Traffic}-{Light} {Queue}},
	volume = {40},
	issn = {0041-1655, 1526-5447},
	url = {http://pubsonline.informs.org/doi/10.1287/trsc.1050.0125},
	doi = {10.1287/trsc.1050.0125},
	abstract = {We consider the fixed-cycle traffic-light (FCTL) queue, where vehicles arrive at an intersection controlled by a traffic light and form a queue. The traffic-light signal alternates between green and red periods, and delayed vehicles are assumed to depart during the green period at equal time intervals.
            Most of the research done on the FCTL queue assumes that the vehicles arrive at the intersection according to a Poisson process and focuses on deriving formulas for the mean queue length at the end of green periods and the mean delay. For a class of discrete arrival processes, including the Poisson process, we derive the probability generating function of both the queue length and delay, from which the whole queue length and delay distribution can be obtained. This allows for the evaluation of performance characteristics other than the mean, such as the variance and percentiles of the distribution.
            We discuss the numerical procedures that are required to obtain the performance characteristics, and give several numerical examples.},
	language = {en},
	number = {2},
	urldate = {2023-01-10},
	journal = {Transportation Science},
	author = {van Leeuwaarden, J. S. H.},
	month = may,
	year = {2006},
	pages = {189--199},
}

@article{darroch_traffic-light_1964,
	title = {On the {Traffic}-light {Queue}},
	volume = {35},
	issn = {0003-4851},
	url = {http://projecteuclid.org/euclid.aoms/1177703761},
	doi = {10.1214/aoms/1177703761},
	language = {en},
	number = {1},
	urldate = {2023-01-10},
	journal = {The Annals of Mathematical Statistics},
	author = {Darroch, J. N.},
	month = mar,
	year = {1964},
	pages = {380--388},
	file = {Full Text:/home/jeroen/Zotero/storage/IPWE9LMT/Darroch - 1964 - On the Traffic-light Queue.pdf:application/pdf},
}

@article{bailey_queueing_1954,
	title = {On {Queueing} {Processes} with {Bulk} {Service}},
	volume = {16},
	issn = {00359246},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1954.tb00149.x},
	doi = {10.1111/j.2517-6161.1954.tb00149.x},
	language = {en},
	number = {1},
	urldate = {2023-01-10},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Bailey, Norman T. J.},
	month = jan,
	year = {1954},
	pages = {80--87},
}

@book{mcshane_traffic_1990,
	address = {Englewood Cliffs, N.J},
	series = {Prentice {Hall} polytechnic series in traffic engineering},
	title = {Traffic engineering},
	isbn = {978-0-13-926148-0},
	publisher = {Prentice-Hall},
	author = {McShane, William R. and Roess, Roger P.},
	year = {1990},
	keywords = {Traffic engineering, United States},
}

@article{sweet_does_2011,
	title = {Does {Traffic} {Congestion} {Slow} the {Economy}?},
	volume = {26},
	issn = {0885-4122, 1552-6593},
	url = {http://journals.sagepub.com/doi/10.1177/0885412211409754},
	doi = {10.1177/0885412211409754},
	abstract = {Does traffic congestion negatively impact the economic growth of metropolitan areas? This article reviews the findings of three research directions addressing this question. First, research on first-order impacts indicates that the economic value of congestion-induced travel delay is tenuous since travelers adapt. Second, research on second-order impacts suggests that congestion slows metropolitan growth, inhibits agglomeration economies, and shapes economic geographies. Third, research on public-sector congestion mitigation policies identifies significant fiscal burdens despite limited success at reducing congestion. In sum, research on individual, business, and public-sector responses to congestion demonstrate a shift from congestion mitigation toward adaptation.},
	language = {en},
	number = {4},
	urldate = {2023-01-10},
	journal = {Journal of Planning Literature},
	author = {Sweet, Matthias},
	month = nov,
	year = {2011},
	pages = {391--404},
}

@article{weisbrod_measuring_2003,
	title = {Measuring {Economic} {Costs} of {Urban} {Traffic} {Congestion} to {Business}},
	volume = {1839},
	issn = {0361-1981, 2169-4052},
	url = {http://journals.sagepub.com/doi/10.3141/1839-10},
	doi = {10.3141/1839-10},
	abstract = {Key findings are provided from NCHRP Study 2-21, which examined how urban traffic congestion imposes economic costs within metropolitan areas. Specifically, the study applied data from Chicago and Philadelphia to examine how various producers of economic goods and services are sensitive to congestion, through its impact on business costs, productivity, and output levels. The data analysis showed that sensitivity to traffic congestion varies by industry sector and is attributable to differences in each industry sector's mix of required inputs and hence its reliance on access to skilled labor, access to specialized inputs, and access to a large, transportation-based market area. Statistical analysis models were applied with the local data to demonstrate how congestion effectively shrinks business market areas and reduces the "agglomeration economies" of businesses operating in large urban areas, thus raising production costs. Overall, this research illustrates how it is possible to estimate the economic implications of congestion, an approach that may be applied in the future for benefit-cost analysis of urban congestion-reduction strategies or for development of congestion pricing strategies. The analysis also shows how congestion-reduction strategies can induce additional traffic as a result of economic benefits.},
	language = {en},
	number = {1},
	urldate = {2023-01-10},
	journal = {Transportation Research Record: Journal of the Transportation Research Board},
	author = {Weisbrod, Glen and Vary, Don and Treyz, George},
	month = jan,
	year = {2003},
	pages = {98--106},
}

@article{el-tantawy_design_2014,
	title = {Design of {Reinforcement} {Learning} {Parameters} for {Seamless} {Application} of {Adaptive} {Traffic} {Signal} {Control}},
	volume = {18},
	issn = {1547-2450, 1547-2442},
	url = {https://www.tandfonline.com/doi/full/10.1080/15472450.2013.810991},
	doi = {10.1080/15472450.2013.810991},
	language = {en},
	number = {3},
	urldate = {2023-01-06},
	journal = {Journal of Intelligent Transportation Systems},
	author = {El-Tantawy, Samah and Abdulhai, Baher and Abdelgawad, Hossam},
	month = jul,
	year = {2014},
	pages = {227--245},
	file = {El-Tantawy et al. - 2014 - Design of Reinforcement Learning Parameters for Se.pdf:/home/jeroen/Zotero/storage/34WWS5TM/El-Tantawy et al. - 2014 - Design of Reinforcement Learning Parameters for Se.pdf:application/pdf},
}

@article{el-tantawy_multiagent_2013,
	title = {Multiagent {Reinforcement} {Learning} for {Integrated} {Network} of {Adaptive} {Traffic} {Signal} {Controllers} ({MARLIN}-{ATSC}): {Methodology} and {Large}-{Scale} {Application} on {Downtown} {Toronto}},
	volume = {14},
	issn = {1524-9050, 1558-0016},
	shorttitle = {Multiagent {Reinforcement} {Learning} for {Integrated} {Network} of {Adaptive} {Traffic} {Signal} {Controllers} ({MARLIN}-{ATSC})},
	url = {https://ieeexplore.ieee.org/document/6502719},
	doi = {10.1109/TITS.2013.2255286},
	number = {3},
	urldate = {2023-01-06},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {El-Tantawy, Samah and Abdulhai, Baher and Abdelgawad, Hossam},
	month = sep,
	year = {2013},
	pages = {1140--1150},
	file = {El-Tantawy et al. - 2013 - Multiagent Reinforcement Learning for Integrated N.pdf:/home/jeroen/Zotero/storage/7MIEKXXA/El-Tantawy et al. - 2013 - Multiagent Reinforcement Learning for Integrated N.pdf:application/pdf},
}

@article{ghanadbashi_using_2022,
	title = {Using ontology to guide reinforcement learning agents in unseen situations: {A} traffic signal control system case study},
	volume = {52},
	issn = {0924-669X, 1573-7497},
	shorttitle = {Using ontology to guide reinforcement learning agents in unseen situations},
	url = {https://link.springer.com/10.1007/s10489-021-02449-5},
	doi = {10.1007/s10489-021-02449-5},
	abstract = {In multi-agent systems, goal achievement is challenging when agents operate in ever-changing environments and face unseen situations, where not all the goals are known or predefined. In such cases, agents need to identify the changes and adapt their behaviour, by evolving their goals or even generating new goals to address the emerging requirements. Learning and practical reasoning techniques have been used to enable agents with limited knowledge to adapt to new circumstances. However, they depend on the availability of large amounts of data, require long exploration periods, and cannot help agents to set new goals. Furthermore, the accuracy of agents’ actions is improved by introducing added intelligence through integrating conceptual features extracted from ontologies. However, the concerns related to taking suitable actions when unseen situations occur are not addressed. This paper proposes a new Automatic Goal Generation Model (AGGM) that enables agents to create new goals to handle unseen situations and to adapt to their ever-changing environment on a real-time basis. AGGM is compared to Q-learning, SARSA, and Deep Q Network in a Traffic Signal Control System case study. The results show that AGGM outperforms the baseline algorithms in unseen situations while handling the seen situations as well as the baseline algorithms.},
	language = {en},
	number = {2},
	urldate = {2022-12-21},
	journal = {Applied Intelligence},
	author = {Ghanadbashi, Saeedeh and Golpayegani, Fatemeh},
	month = jan,
	year = {2022},
	pages = {1808--1824},
	file = {Ghanadbashi and Golpayegani - 2022 - Using ontology to guide reinforcement learning age.pdf:/home/jeroen/Zotero/storage/ABJVYHG8/Ghanadbashi and Golpayegani - 2022 - Using ontology to guide reinforcement learning age.pdf:application/pdf},
}

@article{alegre_quantifying_2021,
	title = {Quantifying the impact of non-stationarity in reinforcement learning-based traffic signal control},
	volume = {7},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-575},
	doi = {10.7717/peerj-cs.575},
	abstract = {In reinforcement learning (RL), dealing with non-stationarity is a challenging issue. However, some domains such as trafﬁc optimization are inherently non-stationary. Causes for and effects of this are manifold. In particular, when dealing with trafﬁc signal controls, addressing non-stationarity is key since trafﬁc conditions change over time and as a function of trafﬁc control decisions taken in other parts of a network. In this paper we analyze the effects that different sources of non-stationarity have in a network of trafﬁc signals, in which each signal is modeled as a learning agent. More precisely, we study both the effects of changing the context in which an agent learns (e.g., a change in ﬂow rates experienced by it), as well as the effects of reducing agent observability of the true environment state. Partial observability may cause distinct states (in which distinct actions are optimal) to be seen as the same by the trafﬁc signal agents. This, in turn, may lead to sub-optimal performance. We show that the lack of suitable sensors to provide a representative observation of the real state seems to affect the performance more drastically than the changes to the underlying trafﬁc patterns.},
	language = {en},
	urldate = {2022-12-21},
	journal = {PeerJ Computer Science},
	author = {Alegre, Lucas N. and Bazzan, Ana L.C. and da Silva, Bruno C.},
	month = may,
	year = {2021},
	pages = {e575},
	file = {Alegre et al. - 2021 - Quantifying the impact of non-stationarity in rein.pdf:/home/jeroen/Zotero/storage/JB7J3AXL/Alegre et al. - 2021 - Quantifying the impact of non-stationarity in rein.pdf:application/pdf},
}

@article{ault_reinforcement_nodate,
	title = {Reinforcement {Learning} {Benchmarks} for {Traffic} {Signal} {Control}},
	abstract = {We propose a toolkit for developing and comparing reinforcement learning (RL)based traffic signal controllers. The toolkit includes implementation of state-of-theart deep-RL algorithms for signal control along with benchmark control problems that are based on realistic traffic scenarios. Importantly, the toolkit allows a firstof-its-kind comparison between state-of-the-art RL-based signal controllers while providing benchmarks for future comparisons. Consequently, we compare and report the relative performance of current RL algorithms. The experimental results suggest that previous algorithms are not robust to varying sensing assumptions and non-stylized intersection layouts. When more realistic signal layouts and advanced sensing capabilities are considered, a distributed deep Q-learning approach is shown to outperform previously reported state-of-the-art algorithms in many cases.},
	language = {en},
	author = {Ault, James and Sharon, Guni},
	file = {Ault and Sharon - Reinforcement Learning Benchmarks for Traffic Sign.pdf:/home/jeroen/Zotero/storage/XQ6XB54M/Ault and Sharon - Reinforcement Learning Benchmarks for Traffic Sign.pdf:application/pdf},
}

@misc{boon_optimal_2022,
	title = {Optimal capacity allocation for heavy-traffic fixed-cycle traffic-light queues and intersections},
	url = {http://arxiv.org/abs/2104.04303},
	abstract = {Setting trafﬁc light signals is a classical topic in trafﬁc engineering, and important in heavy-trafﬁc conditions when green times become scarce and longer queues are inevitably formed. For the ﬁxed-cycle trafﬁc-light queue, an elementary queueing model for one trafﬁc light with cyclic signaling, we obtain heavy-trafﬁc limits that capture the long-term queue behavior. We leverage the limit theorems to obtain sharp performance approximations for one queue in heavy trafﬁc. We also consider optimization problems that aim for optimal division of green times among multiple conﬂicting trafﬁc streams. We show that inserting heavy-trafﬁc approximations leads to tractable optimization problems and close-to-optimal signal prescriptions.},
	language = {en},
	urldate = {2022-12-21},
	publisher = {arXiv},
	author = {Boon, Marko and Janssen, Guido and van Leeuwaarden, Johan and Timmerman, Rik},
	month = aug,
	year = {2022},
	note = {arXiv:2104.04303 [math]},
	keywords = {Mathematics - Optimization and Control, Mathematics - Probability},
	file = {Boon et al. - 2022 - Optimal capacity allocation for heavy-traffic fixe.pdf:/home/jeroen/Zotero/storage/NQIN56RM/Boon et al. - 2022 - Optimal capacity allocation for heavy-traffic fixe.pdf:application/pdf},
}

@misc{van_riel_transient_2021,
	title = {Transient {Behavior} of {Queues} at {Signalized} {Traffic} {Intersections}},
	language = {en},
	author = {van Riel, Jeroen},
	year = {2021},
	file = {van Riel - Transient Behavior of Queues at Signalized Traffic.pdf:/home/jeroen/Zotero/storage/ZC2ENVNC/van Riel - Transient Behavior of Queues at Signalized Traffic.pdf:application/pdf},
}

@article{agand_ecolight_2021,
	title = {{EcoLight}: {Reward} {Shaping} in {Deep} {Reinforcement} {Learning} for {Ergonomic} {Trafﬁc} {Signal} {Control}},
	abstract = {Mobility, the environment, and human health are all harmed by sub-optimal control policies in transportation systems. Intersection trafﬁc signal controllers are a crucial part of today’s transportation infrastructure, as sub-optimal policies may lead to trafﬁc jams and as a result increased levels of air pollution and wasted time. Many adaptive trafﬁc signal controllers have been proposed in the literature, but research on their relative performance differences is limited. On the other hand, to the best of our knowledge there has been no work that directly targets CO2 emission reduction, even though pollution is currently a critical issue. In this paper, we propose a reward shaping scheme for various RL algorithms that not only produces lowers CO2 emissions, but also produces respectable outcomes in terms of other metrics such as travel time. We compare multiple RL algorithms — sarsa, and A2C — as well as diverse scenarios with a mix of different road users emitting varied amounts of pollution.},
	language = {en},
	author = {Agand, Pedram and Iskrov, Alexey},
	year = {2021},
	file = {Agand and Iskrov - EcoLight Reward Shaping in Deep Reinforcement Lea.pdf:/home/jeroen/Zotero/storage/JY2AYWIY/Agand and Iskrov - EcoLight Reward Shaping in Deep Reinforcement Lea.pdf:application/pdf},
}

@article{antes_information_2022,
	title = {Information upwards, recommendation downwards: reinforcement learning with hierarchy for traffic signal control},
	volume = {201},
	issn = {18770509},
	shorttitle = {Information upwards, recommendation downwards},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050922004185},
	doi = {10.1016/j.procs.2022.03.006},
	language = {en},
	urldate = {2022-12-21},
	journal = {Procedia Computer Science},
	author = {Antes, Taylor de O. and Bazzan, Ana L.C. and Tavares, Anderson Rocha},
	year = {2022},
	pages = {24--31},
	file = {Antes et al. - 2022 - Information upwards, recommendation downwards rei.pdf:/home/jeroen/Zotero/storage/AN5WPLVM/Antes et al. - 2022 - Information upwards, recommendation downwards rei.pdf:application/pdf},
}

@misc{ortega_thompson_2010,
	title = {Thompson {Sampling} \& {Bayesian} {Control} {Rule}},
	url = {https://www.adaptiveagents.org/bayesian_control_rule},
	author = {Ortega, P.A.},
	year = {2010},
}

@misc{janner_sequence_2021,
	title = {Sequence {Modeling} {Solutions} for {Reinforcement} {Learning} {Problems}},
	url = {https://bair.berkeley.edu/blog/2021/11/19/trajectory-transformer/},
	author = {Janner, Michael},
	month = nov,
	year = {2021},
}

@book{altman_constrained_2021,
	address = {Boca Raton},
	edition = {1},
	title = {Constrained {Markov} {Decision} {Processes}: {Stochastic} {Modeling}},
	isbn = {978-1-315-14022-3},
	shorttitle = {Constrained {Markov} {Decision} {Processes}},
	url = {https://www.taylorfrancis.com/books/9781315140223},
	language = {en},
	urldate = {2022-12-13},
	publisher = {Routledge},
	author = {Altman, Eitan},
	month = dec,
	year = {2021},
	doi = {10.1201/9781315140223},
	file = {Altman - 2021 - Constrained Markov Decision Processes Stochastic .pdf:/home/jeroen/Zotero/storage/5EM7MRDE/Altman - 2021 - Constrained Markov Decision Processes Stochastic .pdf:application/pdf},
}

@inproceedings{cai_real-time_2017,
	title = {Real-{Time} {Bidding} by {Reinforcement} {Learning} in {Display} {Advertising}},
	url = {http://arxiv.org/abs/1701.02490},
	doi = {10.1145/3018661.3018702},
	abstract = {The majority of online display ads are served through realtime bidding (RTB) — each ad display impression is auctioned oﬀ in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall eﬀectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign’s real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks. The empirical study on two large-scale real-world datasets and the live A/B testing on a commercial platform have demonstrated the superior performance and high eﬃciency compared to state-of-the-art methods.},
	language = {en},
	urldate = {2022-12-13},
	booktitle = {Proceedings of the {Tenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	author = {Cai, Han and Ren, Kan and Zhang, Weinan and Malialis, Kleanthis and Wang, Jun and Yu, Yong and Guo, Defeng},
	month = feb,
	year = {2017},
	note = {arXiv:1701.02490 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
	pages = {661--670},
	annote = {Comment: WSDM 2017},
	file = {Cai et al. - 2017 - Real-Time Bidding by Reinforcement Learning in Dis.pdf:/home/jeroen/Zotero/storage/8HEL4ZHB/Cai et al. - 2017 - Real-Time Bidding by Reinforcement Learning in Dis.pdf:application/pdf},
}

@misc{zheng_constrained_2020,
	title = {Constrained {Upper} {Confidence} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2001.09377},
	abstract = {Constrained Markov Decision Processes are a class of stochastic decision problems in which the decision maker must select a policy that satisfies auxiliary cost constraints. This paper extends upper confidence reinforcement learning for settings in which the reward function and the constraints, described by cost functions, are unknown a priori but the transition kernel is known. Such a setting is well-motivated by a number of applications including exploration of unknown, potentially unsafe, environments. We present an algorithm C-UCRL and show that it achieves sub-linear regret (\$ O(T{\textasciicircum}\{{\textbackslash}frac\{3\}\{4\}\}{\textbackslash}sqrt\{{\textbackslash}log(T/{\textbackslash}delta)\})\$) with respect to the reward while satisfying the constraints even while learning with probability \$1-{\textbackslash}delta\$. Illustrative examples are provided.},
	language = {en},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Zheng, Liyuan and Ratliff, Lillian J.},
	month = jan,
	year = {2020},
	note = {arXiv:2001.09377 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zheng and Ratliff - 2020 - Constrained Upper Confidence Reinforcement Learnin.pdf:/home/jeroen/Zotero/storage/WJGU7JUL/Zheng and Ratliff - 2020 - Constrained Upper Confidence Reinforcement Learnin.pdf:application/pdf},
}

@misc{le_batch_2019,
	title = {Batch {Policy} {Learning} under {Constraints}},
	url = {http://arxiv.org/abs/1903.08738},
	abstract = {When learning policies for real-world domains, two important questions arise: (i) how to efﬁciently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We ﬁrst propose a ﬂexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a speciﬁc algorithmic instantiation and provide performance guarantees for the main objective and all constraints. To certify constraint satisfaction, we propose a new and simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting.},
	language = {en},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Le, Hoang M. and Voloshin, Cameron and Yue, Yisong},
	month = mar,
	year = {2019},
	note = {arXiv:1903.08738 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {Le et al. - 2019 - Batch Policy Learning under Constraints.pdf:/home/jeroen/Zotero/storage/5XG8FNW3/Le et al. - 2019 - Batch Policy Learning under Constraints.pdf:application/pdf},
}

@misc{provodin_impact_2021,
	title = {The {Impact} of {Batch} {Learning} in {Stochastic} {Bandits}},
	url = {http://arxiv.org/abs/2111.02071},
	abstract = {We consider a special case of bandit problems, namely batched bandits. Motivated by natural restrictions of recommender systems and e-commerce platforms, we assume that a learning agent observes responses batched in groups over a certain time period. Unlike previous work, we consider a more practically relevant batchcentric scenario of batch learning. We provide a policy-agnostic regret analysis and demonstrate upper and lower bounds for the regret of a candidate policy. Our main theoretical results show that the impact of batch learning can be measured in terms of online behavior. Finally, we demonstrate the consistency of theoretical results by conducting empirical experiments and reﬂect on the optimal batch size choice.},
	language = {en},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Provodin, Danil and Gajane, Pratik and Pechenizkiy, Mykola and Kaptein, Maurits},
	month = nov,
	year = {2021},
	note = {arXiv:2111.02071 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear at the workshop on the Ecological Theory of Reinforcement Learning, NeurIPS 2021},
	file = {Provodin et al. - 2021 - The Impact of Batch Learning in Stochastic Bandits.pdf:/home/jeroen/Zotero/storage/KAMGEHJ9/Provodin et al. - 2021 - The Impact of Batch Learning in Stochastic Bandits.pdf:application/pdf},
}

@misc{kidambi_morel_2021,
	title = {{MOReL} : {Model}-{Based} {Offline} {Reinforcement} {Learning}},
	shorttitle = {{MOReL}},
	url = {http://arxiv.org/abs/2005.05951},
	abstract = {In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. The ability to train RL policies offline can greatly expand the applicability of RL, its data efficiency, and its experimental velocity. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; and (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP. This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied offline RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g. generative modeling, uncertainty estimation, planning etc.) to directly translate into advances for offline RL.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
	month = mar,
	year = {2021},
	note = {arXiv:2005.05951 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {Comment: First two authors contributed equally. Published at NeurIPS 2020. After publication at NeurIPS 2020, (1) D4RL benchmark results have been added; (2) hyper-parameter ablation studies have been added; (3) scope of Lemma 3 has been extended},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/IV2THQ4G/Kidambi et al. - 2021 - MOReL  Model-Based Offline Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/SGVCJAZB/2005.html:text/html},
}

@article{morimura_nonparametric_nodate,
	title = {Nonparametric {Return} {Distribution} {Approximation}  for {Reinforcement} {Learning}},
	abstract = {Standard Reinforcement Learning (RL) aims to optimize decision-making rules in terms of the expected return. However, especially for risk-management purposes, other criteria such as the expected shortfall are sometimes preferred. Here, we describe a method of approximating the distribution of returns, which allows us to derive various kinds of information about the returns. We ﬁrst show that the Bellman equation, which is a recursive formula for the expected return, can be extended to the cumulative return distribution. Then we derive a nonparametric return distribution estimator with particle smoothing based on this extended Bellman equation. A key aspect of the proposed algorithm is to represent the recursion relation in the extended Bellman equation by a simple replacement procedure of particles associated with a state by using those of the successor state. We show that our algorithm leads to a risksensitive RL paradigm. The usefulness of the proposed approach is demonstrated through numerical experiments.},
	language = {en},
	author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
	pages = {8},
	file = {Morimura et al. - Nonparametric Return Distribution Approximation  f.pdf:/home/jeroen/Zotero/storage/ZF9RBJ2V/Morimura et al. - Nonparametric Return Distribution Approximation  f.pdf:application/pdf},
}

@misc{morimura_parametric_2012,
	title = {Parametric {Return} {Density} {Estimation} for {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1203.3497},
	abstract = {Most conventional Reinforcement Learning (RL) algorithms aim to optimize decision-making rules in terms of the expected returns. However, especially for risk management purposes, other risk-sensitive criteria such as the value-at-risk or the expected shortfall are sometimes preferred in real applications. Here, we describe a parametric method for estimating density of the returns, which allows us to handle various criteria in a unified manner. We first extend the Bellman equation for the conditional expected return to cover a conditional probability density of the returns. Then we derive an extension of the TD-learning algorithm for estimating the return densities in an unknown environment. As test instances, several parametric density estimation algorithms are presented for the Gaussian, Laplace, and skewed Laplace distributions. We show that these algorithms lead to risk-sensitive as well as robust RL paradigms through numerical experiments.},
	urldate = {2022-12-09},
	publisher = {arXiv},
	author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
	month = mar,
	year = {2012},
	note = {arXiv:1203.3497 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/RABI4SWB/Morimura et al. - 2012 - Parametric Return Density Estimation for Reinforce.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/YFDU5G5F/1203.html:text/html},
}

@misc{cesa-bianchi_boltzmann_2017,
	title = {Boltzmann {Exploration} {Done} {Right}},
	url = {http://arxiv.org/abs/1705.10257},
	abstract = {Boltzmann exploration is a classic strategy for sequential decision-making under uncertainty, and is one of the most standard tools in Reinforcement Learning (RL). Despite its widespread use, there is virtually no theoretical understanding about the limitations or the actual benefits of this exploration scheme. Does it drive exploration in a meaningful way? Is it prone to misidentifying the optimal actions or spending too much time exploring the suboptimal ones? What is the right tuning for the learning rate? In this paper, we address several of these questions in the classic setup of stochastic multi-armed bandits. One of our main results is showing that the Boltzmann exploration strategy with any monotone learning-rate sequence will induce suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that guarantees near-optimal performance, albeit only when given prior access to key problem parameters that are typically not available in practical situations (like the time horizon \$T\$ and the suboptimality gap \${\textbackslash}Delta\$). More importantly, we propose a novel variant that uses different learning rates for different arms, and achieves a distribution-dependent regret bound of order \${\textbackslash}frac\{K{\textbackslash}log{\textasciicircum}2 T\}\{{\textbackslash}Delta\}\$ and a distribution-independent bound of order \${\textbackslash}sqrt\{KT\}{\textbackslash}log K\$ without requiring such prior knowledge. To demonstrate the flexibility of our technique, we also propose a variant that guarantees the same performance bounds even if the rewards are heavy-tailed.},
	urldate = {2022-12-09},
	publisher = {arXiv},
	author = {Cesa-Bianchi, Nicolò and Gentile, Claudio and Lugosi, Gábor and Neu, Gergely},
	month = nov,
	year = {2017},
	note = {arXiv:1705.10257 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/J9KN3GKJ/Cesa-Bianchi et al. - 2017 - Boltzmann Exploration Done Right.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/A9LIPILZ/1705.html:text/html},
}

@book{szepesvari_algorithms_2019,
	address = {Cham},
	series = {Synthesis {Lectures} on {Artificial} {Intelligence} and {Machine} {Learning}},
	title = {Algorithms for {Reinforcement} {Learning}},
	isbn = {978-3-031-00423-0 978-3-031-01551-9},
	url = {https://link.springer.com/10.1007/978-3-031-01551-9},
	language = {en},
	urldate = {2022-12-09},
	publisher = {Springer International Publishing},
	author = {Szepesvári, Csaba},
	year = {2019},
	doi = {10.1007/978-3-031-01551-9},
	file = {Szepesvári - 2019 - Algorithms for Reinforcement Learning.pdf:/home/jeroen/Zotero/storage/UK3TFXTN/Szepesvári - 2019 - Algorithms for Reinforcement Learning.pdf:application/pdf},
}

@misc{agarwal_provable_2022,
	title = {Provable {Benefits} of {Representational} {Transfer} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2205.14571},
	abstract = {We study the problem of representational transfer in RL, where an agent ﬁrst pretrains in a number of source tasks to discover a shared representation, which is subsequently used to learn a good policy in a target task. We propose a new notion of task relatedness between source and target tasks, and develop a novel approach for representational transfer under this assumption. Concretely, we show that given a generative access to source tasks, we can discover a representation, using which subsequent linear RL techniques quickly converge to a near-optimal policy, with only online access to the target task. The sample complexity is close to knowing the ground truth features in the target task, and comparable to prior representation learning results in the source tasks. We complement our positive results with lower bounds without generative access, and validate our ﬁndings with empirical evaluation on rich observation MDPs that require deep exploration.},
	language = {en},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Agarwal, Alekh and Song, Yuda and Sun, Wen and Wang, Kaiwen and Wang, Mengdi and Zhang, Xuezhou},
	month = may,
	year = {2022},
	note = {arXiv:2205.14571 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Agarwal et al. - 2022 - Provable Benefits of Representational Transfer in .pdf:/home/jeroen/Zotero/storage/5GURUTLZ/Agarwal et al. - 2022 - Provable Benefits of Representational Transfer in .pdf:application/pdf},
}

@article{bubeck_regret_2012,
	title = {Regret {Analysis} of {Stochastic} and {Nonstochastic} {Multi}-armed {Bandit} {Problems}},
	volume = {5},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-024},
	doi = {10.1561/2200000024},
	abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration–exploitation trade-oﬀ. This is the balance between staying with the option that gave highest payoﬀs in the past and exploring new options that might give higher payoﬀs in the future. Although the study of bandit problems dates back to the 1930s, exploration–exploitation trade-oﬀs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is deﬁned by the payoﬀ process associated with each option. In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoﬀs and adversarial payoﬀs. Besides the basic setting of ﬁnitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
	language = {en},
	number = {1},
	urldate = {2022-12-08},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Bubeck, Sébastien},
	year = {2012},
	pages = {1--122},
	file = {Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf:/home/jeroen/Zotero/storage/I589D9ZR/Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf:application/pdf},
}

@misc{pavse_reducing_2020,
	title = {Reducing {Sampling} {Error} in {Batch} {Temporal} {Difference} {Learning}},
	url = {http://arxiv.org/abs/2008.06738},
	abstract = {Temporal difference (TD) learning is one of the main foundations of modern reinforcement learning. This paper studies the use of TD(0), a canonical TD algorithm, to estimate the value function of a given policy from a batch of data. In this batch setting, we show that TD(0) may converge to an inaccurate value function because the update following an action is weighted according to the number of times that action occurred in the batch – not the true probability of the action under the given policy. To address this limitation, we introduce policy sampling error corrected-TD(0) (PSEC-TD(0)). PSEC-TD(0) ﬁrst estimates the empirical distribution of actions in each state in the batch and then uses importance sampling to correct for the mismatch between the empirical weighting and the correct weighting for updates following each action. We reﬁne the concept of a certainty-equivalence estimate and argue that PSEC-TD(0) is a more data efﬁcient estimator than TD(0) for a ﬁxed batch of data. Finally, we conduct an empirical evaluation of PSEC-TD(0) on three batch value function learning tasks, with a hyperparameter sensitivity analysis, and show that PSEC-TD(0) produces value function estimates with lower mean squared error than TD(0).},
	language = {en},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Pavse, Brahma and Durugkar, Ishan and Hanna, Josiah and Stone, Peter},
	month = aug,
	year = {2020},
	note = {arXiv:2008.06738 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {Comment: Accepted to International Conference on Machine Learning (ICML) 2020},
	file = {Pavse et al. - 2020 - Reducing Sampling Error in Batch Temporal Differen.pdf:/home/jeroen/Zotero/storage/LLG3CN5B/Pavse et al. - 2020 - Reducing Sampling Error in Batch Temporal Differen.pdf:application/pdf},
}

@misc{provodin_empirical_2022,
	title = {An {Empirical} {Evaluation} of {Posterior} {Sampling} for {Constrained} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2209.03596},
	abstract = {We study a posterior sampling approach to efﬁcient exploration in constrained reinforcement learning. Alternatively to existing algorithms, we propose two simple algorithms that are more efﬁcient statistically, simpler to implement and computationally cheaper. The ﬁrst algorithm is based on a linear formulation of CMDP, and the second algorithm leverages the saddle-point formulation of CMDP. Our empirical results demonstrate that, despite its simplicity, posterior sampling achieves state-of-the-art performance and, in some cases, signiﬁcantly outperforms optimistic algorithms.},
	language = {en},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Provodin, Danil and Gajane, Pratik and Pechenizkiy, Mykola and Kaptein, Maurits},
	month = sep,
	year = {2022},
	note = {arXiv:2209.03596 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Provodin et al. - 2022 - An Empirical Evaluation of Posterior Sampling for .pdf:/home/jeroen/Zotero/storage/ZWRQ4UPN/Provodin et al. - 2022 - An Empirical Evaluation of Posterior Sampling for .pdf:application/pdf},
}

@misc{prudencio_survey_2022,
	title = {A {Survey} on {Offline} {Reinforcement} {Learning}: {Taxonomy}, {Review}, and {Open} {Problems}},
	shorttitle = {A {Survey} on {Offline} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2203.01387},
	abstract = {With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Ofﬂine RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective ofﬂine RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications such as education, healthcare, and robotics. In this work, we propose a unifying taxonomy to classify ofﬂine RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the ﬁeld, and a review of existing benchmarks’ properties and shortcomings. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing ﬁeld.},
	language = {en},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Prudencio, Rafael Figueiredo and Maximo, Marcos R. O. A. and Colombini, Esther Luna},
	month = mar,
	year = {2022},
	note = {arXiv:2203.01387 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {Comment: 21 pages; corrected typos},
	file = {Prudencio et al. - 2022 - A Survey on Offline Reinforcement Learning Taxono.pdf:/home/jeroen/Zotero/storage/P2YTCKFY/Prudencio et al. - 2022 - A Survey on Offline Reinforcement Learning Taxono.pdf:application/pdf},
}

@misc{levine_offline_2020,
	title = {Offline {Reinforcement} {Learning}: {Tutorial}, {Review}, and {Perspectives} on {Open} {Problems}},
	shorttitle = {Offline {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2005.01643},
	abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on ofﬂine reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Ofﬂine reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective ofﬂine reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difﬁcult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the ﬁeld.},
	language = {en},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	month = nov,
	year = {2020},
	note = {arXiv:2005.01643 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf:/home/jeroen/Zotero/storage/JQVHKDUG/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf:application/pdf},
}

@misc{achiam_constrained_2017,
	title = {Constrained {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1705.10528},
	abstract = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.},
	language = {en},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
	month = may,
	year = {2017},
	note = {arXiv:1705.10528 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Accepted to ICML 2017},
	file = {Achiam et al. - 2017 - Constrained Policy Optimization.pdf:/home/jeroen/Zotero/storage/LUQLNWJJ/Achiam et al. - 2017 - Constrained Policy Optimization.pdf:application/pdf},
}

@inproceedings{liu_policy_2021,
	address = {Montreal, Canada},
	title = {Policy {Learning} with {Constraints} in {Model}-free {Reinforcement} {Learning}: {A} {Survey}},
	isbn = {978-0-9992411-9-6},
	shorttitle = {Policy {Learning} with {Constraints} in {Model}-free {Reinforcement} {Learning}},
	url = {https://www.ijcai.org/proceedings/2021/614},
	doi = {10.24963/ijcai.2021/614},
	abstract = {Reinforcement Learning (RL) algorithms have had tremendous success in simulated domains. These algorithms, however, often cannot be directly applied to physical systems, especially in cases where there are constraints to satisfy (e.g. to ensure safety or limit resource consumption). In standard RL, the agent is incentivized to explore any policy with the sole goal of maximizing reward; in the real world, however, ensuring satisfaction of certain constraints in the process is also necessary and essential. In this article, we overview existing approaches addressing constraints in model-free reinforcement learning. We model the problem of learning with constraints as a Constrained Markov Decision Process and consider two main types of constraints: cumulative and instantaneous. We summarize existing approaches and discuss their pros and cons. To evaluate policy performance under constraints, we introduce a set of standard benchmarks and metrics. We also summarize limitations of current methods and present open questions for future research.},
	language = {en},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Liu, Yongshuai and Halev, Avishai and Liu, Xin},
	month = aug,
	year = {2021},
	pages = {4508--4515},
	file = {Liu et al. - 2021 - Policy Learning with Constraints in Model-free Rei.pdf:/home/jeroen/Zotero/storage/NLYBQNWC/Liu et al. - 2021 - Policy Learning with Constraints in Model-free Rei.pdf:application/pdf},
}

@article{ghavamzadeh_bayesian_2015,
	title = {Bayesian {Reinforcement} {Learning}: {A} {Survey}},
	volume = {8},
	issn = {1935-8237, 1935-8245},
	shorttitle = {Bayesian {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1609.04436},
	doi = {10.1561/2200000049},
	abstract = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
	language = {en},
	number = {5-6},
	urldate = {2022-12-08},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
	year = {2015},
	note = {arXiv:1609.04436 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	pages = {359--483},
	file = {Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf:/home/jeroen/Zotero/storage/8MG2AF4R/Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf:application/pdf},
}

@misc{provodin_impact_2021-1,
	title = {The {Impact} of {Batch} {Learning} in {Stochastic} {Bandits}},
	url = {http://arxiv.org/abs/2111.02071},
	abstract = {We consider a special case of bandit problems, namely batched bandits. Motivated by natural restrictions of recommender systems and e-commerce platforms, we assume that a learning agent observes responses batched in groups over a certain time period. Unlike previous work, we consider a more practically relevant batchcentric scenario of batch learning. We provide a policy-agnostic regret analysis and demonstrate upper and lower bounds for the regret of a candidate policy. Our main theoretical results show that the impact of batch learning can be measured in terms of online behavior. Finally, we demonstrate the consistency of theoretical results by conducting empirical experiments and reﬂect on the optimal batch size choice.},
	language = {en},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Provodin, Danil and Gajane, Pratik and Pechenizkiy, Mykola and Kaptein, Maurits},
	month = nov,
	year = {2021},
	note = {arXiv:2111.02071 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Provodin et al. - 2021 - The Impact of Batch Learning in Stochastic Bandits.pdf:/home/jeroen/Zotero/storage/KV2PV2UT/Provodin et al. - 2021 - The Impact of Batch Learning in Stochastic Bandits.pdf:application/pdf},
}

@article{noauthor_adaptive_2022,
	title = {An {Adaptive} {Sampling} {Algorithm} for {Solving} {Markov} {Decision} {Processes}},
	language = {en},
	year = {2022},
	pages = {15},
	file = {2022 - An Adaptive Sampling Algorithm for Solving Markov .pdf:/home/jeroen/Zotero/storage/BBJI4IYS/2022 - An Adaptive Sampling Algorithm for Solving Markov .pdf:application/pdf},
}

@misc{moldovan_safe_2012,
	title = {Safe {Exploration} in {Markov} {Decision} {Processes}},
	url = {http://arxiv.org/abs/1205.4810},
	abstract = {In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don’t satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We ﬁrst propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an eﬃcient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.},
	language = {en},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Moldovan, Teodor Mihai and Abbeel, Pieter},
	month = jul,
	year = {2012},
	note = {arXiv:1205.4810 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Moldovan and Abbeel - 2012 - Safe Exploration in Markov Decision Processes.pdf:/home/jeroen/Zotero/storage/MBKV6JAT/Moldovan and Abbeel - 2012 - Safe Exploration in Markov Decision Processes.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
	file = {Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:/home/jeroen/Zotero/storage/P6DD4LLQ/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:application/pdf},
}

@book{recker_scientific_2013,
	address = {Berlin, Heidelberg},
	title = {Scientific {Research} in {Information} {Systems}},
	isbn = {978-3-642-30047-9 978-3-642-30048-6},
	url = {http://link.springer.com/10.1007/978-3-642-30048-6},
	language = {en},
	urldate = {2022-12-08},
	publisher = {Springer Berlin Heidelberg},
	author = {Recker, Jan},
	year = {2013},
	doi = {10.1007/978-3-642-30048-6},
	file = {Recker - 2013 - Scientific Research in Information Systems.pdf:/home/jeroen/Zotero/storage/AKZWNSY2/Recker - 2013 - Scientific Research in Information Systems.pdf:application/pdf},
}

@article{garcia_comprehensive_nodate,
	title = {A {Comprehensive} {Survey} on {Safe} {Reinforcement} {Learning}},
	abstract = {Safe Reinforcement Learning can be deﬁned as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The ﬁrst is based on the modiﬁcation of the optimality criterion, the classic discounted ﬁnite/inﬁnite horizon, with a safety factor. The second is based on the modiﬁcation of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classiﬁcation to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.},
	language = {en},
	author = {Garcıa, Javier and Fernandez, Fernando},
	pages = {44},
	file = {Garcıa and Fernandez - A Comprehensive Survey on Safe Reinforcement Learn.pdf:/home/jeroen/Zotero/storage/H467LGLD/Garcıa and Fernandez - A Comprehensive Survey on Safe Reinforcement Learn.pdf:application/pdf},
}

@article{huang2022cleanrl,
	title = {{CleanRL}: {High}-quality single-file implementations of deep reinforcement learning algorithms},
	volume = {23},
	url = {http://jmlr.org/papers/v23/21-1342.html},
	number = {274},
	journal = {Journal of Machine Learning Research},
	author = {Huang, Shengyi and Dossa, Rousslan Fernand Julien and Ye, Chang and Braga, Jeff and Chakraborty, Dipam and Mehta, Kinal and Araújo, João G.M.},
	year = {2022},
	pages = {1--18},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2024-01-16},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Number: 7540
Publisher: Nature Publishing Group},
	keywords = {Computer science},
	pages = {529--533},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/5HYMQIMI/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf},
}

@misc{gurobi,
	title = {Gurobi optimizer reference manual},
	url = {https://www.gurobi.com},
	author = {{Gurobi Optimization, LLC}},
	year = {2024},
}

@misc{towers_gymnasium_2023,
	title = {Gymnasium},
	url = {https://zenodo.org/record/8127025},
	abstract = {An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)},
	urldate = {2023-07-08},
	publisher = {Zenodo},
	author = {Towers, Mark and Terry, Jordan K. and Kwiatkowski, Ariel and Balis, John U. and Cola, Gianluca de and Deleu, Tristan and Goulão, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and Perez-Vicente, Rodrigo and Pierré, Andrea and Schulhoff, Sander and Tai, Jun Jet and Shen, Andrew Tan Jin and Younis, Omar G.},
	month = mar,
	year = {2023},
	doi = {10.5281/zenodo.8127026},
}

@phdthesis{vestjens_-line_1997,
	address = {Eindhoven},
	type = {Phd {Thesis} 1 ({Research} {TU}/e / {Graduation} {TU}/e)},
	title = {On-line machine scheduling},
	school = {Technische Universiteit Eindhoven},
	author = {Vestjens, A.P.A.},
	year = {1997},
	doi = {10.6100/IR500043},
	note = {ISBN: 9789038605715},
}

@phdthesis{limpens_online_2023,
	type = {Bachelor},
	title = {Online {Platoon} {Forming} {Algorithms} for automated vehicles: {A} more efficient approach},
	school = {Eindhoven University of Technology},
	author = {Limpens, Matthijs},
	month = sep,
	year = {2023},
}

@misc{miculescu_polling-systems-based_2016,
	title = {Polling-systems-based {Autonomous} {Vehicle} {Coordination} in {Traffic} {Intersections} with {No} {Traffic} {Signals}},
	url = {http://arxiv.org/abs/1607.07896},
	abstract = {The rapid development of autonomous vehicles spurred a careful investigation of the potential benefits of all-autonomous transportation networks. Most studies conclude that autonomous systems can enable drastic improvements in performance. A widely studied concept is all-autonomous, collision-free intersections, where vehicles arriving in a traffic intersection with no traffic light adjust their speeds to cross safely through the intersection as quickly as possible. In this paper, we propose a coordination control algorithm for this problem, assuming stochastic models for the arrival times of the vehicles. The proposed algorithm provides provable guarantees on safety and performance. More precisely, it is shown that no collisions occur surely, and moreover a rigorous upper bound is provided for the expected wait time. The algorithm is also demonstrated in simulations. The proposed algorithms are inspired by polling systems. In fact, the problem studied in this paper leads to a new polling system where customers are subject to differential constraints, which may be interesting in its own right.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Miculescu, David and Karaman, Sertac},
	month = jul,
	year = {2016},
	note = {arXiv:1607.07896 [cs, math]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Optimization and Control, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv.org Snapshot:/home/jeroen/Zotero/storage/7VMJSZ9T/1607.html:text/html;Full Text PDF:/home/jeroen/Zotero/storage/IPVPFI5B/Miculescu and Karaman - 2016 - Polling-systems-based Autonomous Vehicle Coordinat.pdf:application/pdf},
}

@mastersthesis{claassen_application_2022,
	title = {Application of {Deep} {Reinforcement} {Learning} and {Graph} {Neural} {Networks} to the {Machine} {Scheduling} {Problem}},
	url = {https://research.tue.nl/en/studentTheses/application-of-deep-reinforcement-learning-and-graph-neural-netwo},
	language = {en},
	urldate = {2023-11-29},
	author = {Claassen, Rob},
	month = oct,
	year = {2022},
	file = {Snapshot:/home/jeroen/Zotero/storage/F5IFHGUY/application-of-deep-reinforcement-learning-and-graph-neural-netwo.html:text/html},
}

@article{ouelhadj_survey_2009,
	title = {A survey of dynamic scheduling in manufacturing systems},
	volume = {12},
	issn = {1099-1425},
	url = {https://doi.org/10.1007/s10951-008-0090-8},
	doi = {10.1007/s10951-008-0090-8},
	abstract = {In most real-world environments, scheduling is an ongoing reactive process where the presence of a variety of unexpected disruptions is usually inevitable, and continually forces reconsideration and revision of pre-established schedules. Many of the approaches developed to solve the problem of static scheduling are often impractical in real-world environments, and the near-optimal schedules with respect to the estimated data may become obsolete when they are released to the shop floor. This paper outlines the limitations of the static approaches to scheduling in the presence of real-time information and presents a number of issues that have come up in recent years on dynamic scheduling.},
	language = {en},
	number = {4},
	urldate = {2023-11-29},
	journal = {Journal of Scheduling},
	author = {Ouelhadj, Djamila and Petrovic, Sanja},
	month = aug,
	year = {2009},
	keywords = {Agent-based scheduling, Dynamic scheduling, Predictive–reactive scheduling, Robust scheduling},
	pages = {417--431},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/578ZKAGC/Ouelhadj and Petrovic - 2009 - A survey of dynamic scheduling in manufacturing sy.pdf:application/pdf},
}

@book{cygan_parameterized_2015,
	address = {Cham},
	title = {Parameterized {Algorithms}},
	isbn = {978-3-319-21274-6 978-3-319-21275-3},
	url = {https://link.springer.com/10.1007/978-3-319-21275-3},
	language = {en},
	urldate = {2023-11-15},
	publisher = {Springer International Publishing},
	author = {Cygan, Marek and Fomin, Fedor V. and Kowalik, Łukasz and Lokshtanov, Daniel and Marx, Dániel and Pilipczuk, Marcin and Pilipczuk, Michał and Saurabh, Saket},
	year = {2015},
	doi = {10.1007/978-3-319-21275-3},
	keywords = {algorithm analysis and problem complexity, Branching, Fixed-parameter tractability (FPT);, Integer linear programming (ILP), Kernels, Matroids, Parameterized algorithms, Parameterized complexity, Treewidth},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/I3V363VE/Cygan et al. - 2015 - Parameterized Algorithms.pdf:application/pdf},
}

@article{sanchez-lengeling_gentle_2021,
	title = {A {Gentle} {Introduction} to {Graph} {Neural} {Networks}},
	volume = {6},
	issn = {2476-0757},
	url = {https://distill.pub/2021/gnn-intro},
	doi = {10.23915/distill.00033},
	abstract = {What components are needed for building learning algorithms that leverage the structure and properties of graphs?},
	language = {en},
	number = {9},
	urldate = {2023-11-02},
	journal = {Distill},
	author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
	month = sep,
	year = {2021},
	pages = {e33},
	file = {Snapshot:/home/jeroen/Zotero/storage/CD34W6R4/gnn-intro.html:text/html},
}

@misc{xu_how_2019,
	title = {How {Powerful} are {Graph} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/1810.00826},
	abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
	urldate = {2023-11-02},
	publisher = {arXiv},
	author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	month = feb,
	year = {2019},
	note = {arXiv:1810.00826 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/jeroen/Zotero/storage/TJK4HCM3/1810.html:text/html;Full Text PDF:/home/jeroen/Zotero/storage/SZ8UX9HI/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf:application/pdf},
}

@book{pinedo_scheduling_2016,
	address = {Cham},
	title = {Scheduling: {Theory}, {Algorithms}, and {Systems}},
	isbn = {978-3-319-26578-0 978-3-319-26580-3},
	url = {http://link.springer.com/10.1007/978-3-319-26580-3},
	language = {en},
	urldate = {2023-10-18},
	publisher = {Springer International Publishing},
	author = {Pinedo, Michael L.},
	year = {2016},
	doi = {10.1007/978-3-319-26580-3},
	keywords = {Applications of schedule, Decision-making, Deterministic models, Scheduling, Stochastic models},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/QZV8HWL5/Pinedo - 2016 - Scheduling.pdf:application/pdf},
}

@article{ingimundardottir_discovering_2018,
	title = {Discovering dispatching rules from data using imitation learning: {A} case study for the job-shop problem},
	volume = {21},
	issn = {1099-1425},
	shorttitle = {Discovering dispatching rules from data using imitation learning},
	url = {https://doi.org/10.1007/s10951-017-0534-0},
	doi = {10.1007/s10951-017-0534-0},
	abstract = {Dispatching rules can be automatically generated from scheduling data. This paper will demonstrate that the key to learning an effective dispatching rule is through the careful construction of the training data, \$\${\textbackslash}\{{\textbackslash}mathbf \{x\}\_i(k),y\_i(k){\textbackslash}\}\_\{k=1\}{\textasciicircum}K{\textbackslash}in \{{\textbackslash}mathscr \{D\}\}\$\$, where (i) features of partially constructed schedules \$\${\textbackslash}mathbf \{x\}\_i\$\$should necessarily reflect the induced data distribution \$\$\{{\textbackslash}mathscr \{D\}\}\$\$for when the rule is applied. This is achieved by updating the learned model in an active imitation learning fashion; (ii) \$\$y\_i\$\$is labelled optimally using a MIP solver; and (iii) data need to be balanced, as the set is unbalanced with respect to the dispatching step k. Using the guidelines set by our framework the design of custom dispatching rules, for a particular scheduling application, will become more effective. In the study presented three different distributions of the job-shop will be considered. The machine learning approach considered is based on preference learning, i.e. which dispatch (post-decision state) is preferable to another.},
	language = {en},
	number = {4},
	urldate = {2023-11-01},
	journal = {Journal of Scheduling},
	author = {Ingimundardottir, Helga and Runarsson, Thomas Philip},
	month = aug,
	year = {2018},
	keywords = {Scheduling, Composite dispatching rules, DAgger, Imitation learning, Performance analysis, Preference learning},
	pages = {413--428},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/ZSDVWWJ3/Ingimundardottir and Runarsson - 2018 - Discovering dispatching rules from data using imit.pdf:application/pdf},
}

@inproceedings{iklassov_study_2023,
	address = {Macau, SAR China},
	title = {On the {Study} of {Curriculum} {Learning} for {Inferring} {Dispatching} {Policies} on the {Job} {Shop} {Scheduling}},
	isbn = {978-1-956792-03-4},
	url = {https://www.ijcai.org/proceedings/2023/594},
	doi = {10.24963/ijcai.2023/594},
	abstract = {This paper studies the use of Curriculum Learning on Reinforcement Learning (RL) to improve the performance of the dispatching policies learned on the Job-shop Scheduling Problem (JSP). Current works in the literature present a large optimality gap when learning end-to-end solutions on this problem. In this regard, we identify the difficulty for RL to learn directly on large instances as part of the issue and use Curriculum Learning (CL) to mitigate this effect. Particularly, CL sequences the learning process in a curriculum of increasing complexity tasks, which allows learning on large instances that otherwise would be impossible to learn from scratch. In this paper, we present a size-agnostic model that enables us to demonstrate that current curriculum strategies have a major impact on the quality of the solution inferred. In addition, we introduce a novel Reinforced Adaptive Staircase Curriculum Learning (RASCL) strategy, which adjusts the difficulty level during the learning process by revisiting the worstperforming instances. Conducted experiments on Taillard’s and Demirkol’s datasets show that the presented approach significantly improves the current state-of-the-art models on the JSP. It reduces the average optimality gap from 19.35\% to 10.46\% on Taillard’s instances and from 38.43\% to 18.85\% on Demirkol’s instances.},
	language = {en},
	urldate = {2023-10-28},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Iklassov, Zangir and Medvedev, Dmitrii and Solozabal Ochoa De Retana, Ruben and Takac, Martin},
	month = aug,
	year = {2023},
	pages = {5350--5358},
	file = {Iklassov et al. - 2023 - On the Study of Curriculum Learning for Inferring .pdf:/home/jeroen/Zotero/storage/EFP8YQ2B/Iklassov et al. - 2023 - On the Study of Curriculum Learning for Inferring .pdf:application/pdf},
}

@incollection{graham_optimization_1979,
	series = {Discrete {Optimization} {II}},
	title = {Optimization and {Approximation} in {Deterministic} {Sequencing} and {Scheduling}: a {Survey}},
	volume = {5},
	shorttitle = {Optimization and {Approximation} in {Deterministic} {Sequencing} and {Scheduling}},
	url = {https://www.sciencedirect.com/science/article/pii/S016750600870356X},
	abstract = {The theory of deterministic sequencing and scheduling has expanded rapidly during the past years. In this paper we survey the state of the art with respect to optimization and approximation algorithms and interpret these in terms of computational complexity theory. Special cases considered are single machine scheduling, identical, uniform and unrelated parallel machine scheduling, and open shop, flow shop and job shop scheduling. We indicate some problems for future research and include a selective bibliography.},
	urldate = {2023-10-23},
	booktitle = {Annals of {Discrete} {Mathematics}},
	publisher = {Elsevier},
	author = {Graham, R. L. and Lawler, E. L. and Lenstra, J. K. and Kan, A. H. G. Rinnooy},
	editor = {Hammer, P. L. and Johnson, E. L. and Korte, B. H.},
	month = jan,
	year = {1979},
	doi = {10.1016/S0167-5060(08)70356-X},
	pages = {287--326},
	file = {ScienceDirect Full Text PDF:/home/jeroen/Zotero/storage/6INJSUCP/Graham et al. - 1979 - Optimization and Approximation in Deterministic Se.pdf:application/pdf},
}

@incollection{lenstra_complexity_1977,
	series = {Studies in {Integer} {Programming}},
	title = {Complexity of {Machine} {Scheduling} {Problems}},
	volume = {1},
	url = {https://www.sciencedirect.com/science/article/pii/S016750600870743X},
	abstract = {We survey and extend the results on the complexity of machine scheduling problems. After a brief review of the central concept of NP-completeness we give a classification of scheduling problems on single, different and identical machines and study the influence of various parameters on their complexity. The problems for which a polynomial-bounded algorithm is available are listed and NP-completeness is established for a large number of other machine scheduling problems. We finally discuss some questions that remain unanswered.},
	urldate = {2023-10-23},
	booktitle = {Annals of {Discrete} {Mathematics}},
	publisher = {Elsevier},
	author = {Lenstra, J. K. and Rinnooy Kan, A. H. G. and Brucker, P.},
	editor = {Hammer, P. L. and Johnson, E. L. and Korte, B. H. and Nemhauser, G. L.},
	month = jan,
	year = {1977},
	doi = {10.1016/S0167-5060(08)70743-X},
	pages = {343--362},
	file = {ScienceDirect Snapshot:/home/jeroen/Zotero/storage/6NLB2NZ4/S016750600870743X.html:text/html;Submitted Version:/home/jeroen/Zotero/storage/NPNFKNAU/Lenstra et al. - 1977 - Complexity of Machine Scheduling Problems.pdf:application/pdf},
}

@article{belouadah_scheduling_1992,
	title = {Scheduling with release dates on a single machine to minimize total weighted completion time},
	volume = {36},
	issn = {0166-218X},
	url = {https://www.sciencedirect.com/science/article/pii/0166218X92902559},
	doi = {10.1016/0166-218X(92)90255-9},
	abstract = {This paper considers the problem of scheduling jobs with release dates on a single machine to minimize the total weighted completion time. A branch and bound algorithm is proposed which incorporates three special features that contribute to its efficiency. Firstly, quickly computed lower bounds are obtained using a procedure which is based on job splitting. The job splitting methodology is shown to be applicable to a range of total weighted completion time scheduling problems. Secondly, the branching rule includes a release date adjustment mechanism which increases release dates at certain nodes of the tree with a view to tightening lower bounds. Thirdly, the branch and bound algorithm includes a new dominance rule for eliminating nodes of the search tree. Computational experience on problems with up to 50 jobs indicates that the proposed algorithm is superior to other known algorithms.},
	number = {3},
	urldate = {2023-10-23},
	journal = {Discrete Applied Mathematics},
	author = {Belouadah, H. and Posner, M. E. and Potts, C. N.},
	month = may,
	year = {1992},
	pages = {213--231},
	file = {ScienceDirect Full Text PDF:/home/jeroen/Zotero/storage/ZBKRIBP9/Belouadah et al. - 1992 - Scheduling with release dates on a single machine .pdf:application/pdf;ScienceDirect Snapshot:/home/jeroen/Zotero/storage/NGL4CCBH/0166218X92902559.html:text/html},
}

@article{cire_multivalued_nodate,
	title = {Multivalued {Decision} {Diagrams} for {Sequencing} {Problems}},
	abstract = {Sequencing problems are among the most prominent problems studied in operations research, with primary application in, e.g., scheduling and routing. We propose a novel approach to solving generic sequencing problems using multivalued decision diagrams (MDDs). Because an MDD representation may grow exponentially large, we apply MDDs of limited size as a discrete relaxation to the problem. We show that MDDs can be used to represent a wide range of sequencing problems with various side constraints and objective functions, and demonstrate how MDDs can be added to existing constraint-based scheduling systems. Our computational results indicate that the additional inference obtained by our MDDs can speed up a state-of-the art solver by several orders of magnitude, for a range of diﬀerent problem classes.},
	language = {en},
	author = {Cire, Andre A and van Hoeve, Willem-Jan},
	file = {Cire and van Hoeve - Multivalued Decision Diagrams for Sequencing Probl.pdf:/home/jeroen/Zotero/storage/VEJBLVRL/Cire and van Hoeve - Multivalued Decision Diagrams for Sequencing Probl.pdf:application/pdf},
}

@article{chou_algorithms_2009,
	title = {Algorithms for the single machine total weighted completion time scheduling problem with release times and sequence-dependent setups},
	volume = {43},
	issn = {0268-3768, 1433-3015},
	url = {http://link.springer.com/10.1007/s00170-008-1762-4},
	doi = {10.1007/s00170-008-1762-4},
	abstract = {This paper considers the problem of scheduling n jobs on a single machine to minimize the total weighted completion time in the presence of sequence-dependent setup times and release times. To the best of our knowledge, little research has been devoted to this scheduling problem. Therefore, we developed two exact algorithms, including a constraint programming model and a branch-and-bound method for small problems. The obtained optimal solutions can be used as a benchmark for evaluating the performance of heuristics. With the complexity in mind, two heuristics, including a best index dispatch (BID) and a modified weighted shortest processing time (MWSPT) based on non-delay concepts are also proposed for large problems. The time complexities of the two proposed heuristics are O(n4) and O(n3), respectively. The computational results showed that the branch-andbound method could solve most instances with 40 jobs under the time limit of 7,200 s. The BID heuristic is superior to the MWSPT in solution quality, although both can efficiently and effectively obtain near-optimal solutions for large instances.},
	language = {en},
	number = {7-8},
	urldate = {2023-10-20},
	journal = {The International Journal of Advanced Manufacturing Technology},
	author = {Chou, Fuh-Der and Wang, Hui-Mei and Chang, Tzu-Yun},
	month = aug,
	year = {2009},
	pages = {810--821},
	file = {Chou et al. - 2009 - Algorithms for the single machine total weighted c.pdf:/home/jeroen/Zotero/storage/9SEVG8S6/Chou et al. - 2009 - Algorithms for the single machine total weighted c.pdf:application/pdf},
}

@article{balas_one-machine_1995,
	title = {The {One}-{Machine} {Problem} with {Delayed} {Precedence} {Constraints} and its {Use} in {Job} {Shop} {Scheduling}},
	copyright = {© 1995 INFORMS},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.41.1.94},
	doi = {10.1287/mnsc.41.1.94},
	abstract = {We study the one machine scheduling problem with release and delivery times and the minimum makespan objective, in the presence of constraints that for certain pairs of jobs require a delay between...},
	language = {en},
	urldate = {2023-10-20},
	journal = {Management Science},
	author = {Balas, Egon and Lenstra, Jan Karel and Vazacopoulos, Alkis},
	month = jan,
	year = {1995},
	note = {Publisher: INFORMS},
	file = {Snapshot:/home/jeroen/Zotero/storage/IQSCZJN8/mnsc.41.1.html:text/html},
}

@article{lin_single_2022,
	title = {Single machine scheduling problems with sequence-dependent setup times and precedence delays},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-13278-y},
	doi = {10.1038/s41598-022-13278-y},
	abstract = {Sequence-dependent setup times and precedence delays occur frequently in various production environments. This study investigates the single machine scheduling problem with setup times and precedence delays that occur in an amplifier assembly company. This study proposes a novel mixed-integer linear programming model and a lean iterated greedy algorithm to minimize the makespan for this problem. Based on the property of delayed precedence constraints, the lean iterated greedy (LIG) algorithm uses a simple but effective lean construction mechanism that can discard infeasible solutions to reduce the waste of unnecessary searches and quickly converge to the (near) global optimum. The computational results show that LIG significantly outperforms the state-of-the-art algorithm in terms of solution quality and computational efficiency. This study mainly contributes to providing a simple, effective, and efficient algorithm that can facilitate industrial applications and serve as a new benchmark approach for future research.},
	language = {en},
	number = {1},
	urldate = {2023-10-20},
	journal = {Scientific Reports},
	author = {Lin, Shih-Wei and Ying, Kuo-Ching},
	month = jun,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Engineering, Mathematics and computing},
	pages = {9430},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/5X5QHDPC/Lin and Ying - 2022 - Single machine scheduling problems with sequence-d.pdf:application/pdf},
}

@article{brucker_scheduling_2006,
	title = {Scheduling chains with identical jobs and constant delays on a single machine},
	volume = {63},
	issn = {1432-5217},
	url = {https://doi.org/10.1007/s00186-005-0014-8},
	doi = {10.1007/s00186-005-0014-8},
	abstract = {In this paper we study the single-machine problem 1{\textbar}chains(l), pj= p{\textbar}∑ Cjin which jobs with constant processing times and generalized precedence constraints in form of chains with constant delays are given. One has to schedule the jobs on a single machine such that all delays between consecutive jobs in a chain are satisfied and the sum of all completion times of the jobs is minimized. We show that this problem is polynomially solvable.},
	language = {en},
	number = {1},
	urldate = {2023-10-20},
	journal = {Mathematical Methods of Operations Research},
	author = {Brucker, Peter and Knust, Sigrid and Oğuz, Ceyda},
	month = feb,
	year = {2006},
	keywords = {Scheduling, Complexity results, Delays, Time-lags},
	pages = {63--75},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/YIUHYYQ7/Brucker et al. - 2006 - Scheduling chains with identical jobs and constant.pdf:application/pdf},
}

@misc{vinyals_pointer_2017,
	title = {Pointer {Networks}},
	url = {http://arxiv.org/abs/1506.03134},
	doi = {10.48550/arXiv.1506.03134},
	abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
	month = jan,
	year = {2017},
	note = {arXiv:1506.03134 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computational Geometry, Computer Science - Neural and Evolutionary Computing, deep learning},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/YP7JHMXP/Vinyals et al. - 2017 - Pointer Networks.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/MS2A84PV/1506.html:text/html},
}

@misc{han_research_nodate,
	title = {Research on {Adaptive} {Job} {Shop} {Scheduling} {Problems} {Based} on {Dueling} {Double} {DQN} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/9218934?denied=},
	urldate = {2023-10-14},
	author = {Han, Bao-An and Yang, Jian-Jun},
}

@article{lamorgese_optimal_2016,
	title = {Optimal {Train} {Dispatching} by {Benders}’-{Like} {Reformulation}},
	volume = {50},
	issn = {0041-1655},
	url = {https://pubsonline.informs.org/doi/10.1287/trsc.2015.0605},
	doi = {10.1287/trsc.2015.0605},
	abstract = {Train movements on railway lines are generally controlled by human dispatchers. Because disruptions often occur, dispatchers make real-time scheduling and routing decisions in an attempt to minimize deviations from the official timetable. This optimization problem is called train dispatching. We represent it as a mixed integer linear programming model, and solve it with a Benders’-like decomposition within a suitable master/slave scheme. Interestingly, the master and the slave problems correspond to a macroscopic and microscopic representation of the railway, recently exploited in heuristic approaches to the problem. The decomposition, along with some new modeling ideas, allowed us to solve real-life instances of practical interest to optimality. Automatic dispatching systems based on our macro/micro decomposition—in which both master and slave are solved heuristically—have been in operation in several Italian lines since 2011. The exact approach described in this paper outperforms such systems on our test bed of real-life instances. Furthermore, a system based on another version of the exact decomposition approach has been in operation since February 2014 on a line in Norway.},
	number = {3},
	urldate = {2023-10-16},
	journal = {Transportation Science},
	author = {Lamorgese, Leonardo and Mannino, Carlo and Piacentini, Mauro},
	month = aug,
	year = {2016},
	note = {Publisher: INFORMS},
	keywords = {integer programming, logic Benders’ decomposition, railway optimization},
	pages = {910--925},
	file = {Lamorgese et al. - 2016 - Optimal Train Dispatching by Benders’-Like Reformu.pdf:/home/jeroen/Zotero/storage/9NLH7JB7/Lamorgese et al. - 2016 - Optimal Train Dispatching by Benders’-Like Reformu.pdf:application/pdf},
}

@article{lamorgese_exact_2015,
	title = {An {Exact} {Decomposition} {Approach} for the {Real}-{Time} {Train} {Dispatching} {Problem}},
	volume = {63},
	issn = {0030-364X},
	url = {https://pubsonline.informs.org/doi/10.1287/opre.2014.1327},
	doi = {10.1287/opre.2014.1327},
	abstract = {Trains’ movements on a railway network are regulated by official timetables. Deviations and delays occur quite often in practice, demanding fast rescheduling and rerouting decisions in order to avoid conflicts and minimize overall delay. This is the real-time train dispatching problem. In contrast with the classic “holistic” approach, we show how to decompose the problem into smaller subproblems associated with the line and the stations. This decomposition is the basis for a master-slave solution algorithm, in which the master problem is associated with the line and the slave problem is associated with the stations. The two subproblems are modeled as mixed integer linear programs, with specific sets of variables and constraints. Similarly to the classical Benders’ decomposition approach, slave and master communicate through suitable feasibility cuts in the variables of the master. Extensive tests on real-life instances from single and double-track lines in Italy showed significant improvements over current dispatching performances. A decision support system based on this exact approach has been in operation in Norway since February 2014 and represents one of the first operative applications of mathematical optimization to train dispatching.},
	number = {1},
	urldate = {2023-10-16},
	journal = {Operations Research},
	author = {Lamorgese, Leonardo and Mannino, Carlo},
	month = feb,
	year = {2015},
	note = {Publisher: INFORMS},
	keywords = {railway optimization, discrete mathematics, train dispatching},
	pages = {48--64},
	file = {Lamorgese and Mannino - 2015 - An Exact Decomposition Approach for the Real-Time .pdf:/home/jeroen/Zotero/storage/K7SJ2Z57/Lamorgese and Mannino - 2015 - An Exact Decomposition Approach for the Real-Time .pdf:application/pdf},
}

@article{vivekanandan_reinforcement_2023,
	title = {A {Reinforcement} {Learning} {Approach} for {Scheduling} {Problems} with {Improved} {Generalization} through {Order} {Swapping}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-4990},
	url = {https://www.mdpi.com/2504-4990/5/2/25},
	doi = {10.3390/make5020025},
	abstract = {The scheduling of production resources (such as associating jobs to machines) plays a vital role for the manufacturing industry not only for saving energy, but also for increasing the overall efficiency. Among the different job scheduling problems, the Job Shop Scheduling Problem (JSSP) is addressed in this work. JSSP falls into the category of NP-hard Combinatorial Optimization Problem (COP), in which solving the problem through exhaustive search becomes unfeasible. Simple heuristics such as First-In, First-Out, Largest Processing Time First and metaheuristics such as taboo search are often adopted to solve the problem by truncating the search space. The viability of the methods becomes inefficient for large problem sizes as it is either far from the optimum or time consuming. In recent years, the research towards using Deep Reinforcement Learning (DRL) to solve COPs has gained interest and has shown promising results in terms of solution quality and computational efficiency. In this work, we provide an novel approach to solve the JSSP examining the objectives generalization and solution effectiveness using DRL. In particular, we employ the Proximal Policy Optimization (PPO) algorithm that adopts the policy-gradient paradigm that is found to perform well in the constrained dispatching of jobs. We incorporated a new method called Order Swapping Mechanism (OSM) in the environment to achieve better generalized learning of the problem. The performance of the presented approach is analyzed in depth by using a set of available benchmark instances and comparing our results with the work of other groups.},
	language = {en},
	number = {2},
	urldate = {2023-10-14},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Vivekanandan, Deepak and Wirth, Samuel and Karlbauer, Patrick and Klarmann, Noah},
	month = jun,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {generalization, Industry 4.0, Job Shop Scheduling, Markov Decision Process, Production Scheduling, Reinforcement Learning},
	pages = {418--430},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/8WVQCT7T/Vivekanandan et al. - 2023 - A Reinforcement Learning Approach for Scheduling P.pdf:application/pdf},
}

@article{hullermeier_aleatoric_2021,
	title = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
	volume = {110},
	issn = {1573-0565},
	shorttitle = {Aleatoric and epistemic uncertainty in machine learning},
	url = {https://doi.org/10.1007/s10994-021-05946-3},
	doi = {10.1007/s10994-021-05946-3},
	abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
	language = {en},
	number = {3},
	urldate = {2023-10-13},
	journal = {Machine Learning},
	author = {Hüllermeier, Eyke and Waegeman, Willem},
	month = mar,
	year = {2021},
	keywords = {Bayesian inference, Calibration, Conformal prediction, Credal sets and classifiers, Deep neural networks, Ensembles, Epistemic uncertainty, Gaussian processes, Generative models, Likelihood-based methods, Probability, Set-valued prediction, Uncertainty, Version space learning},
	pages = {457--506},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/JUPP7XHF/Hüllermeier and Waegeman - 2021 - Aleatoric and epistemic uncertainty in machine lea.pdf:application/pdf},
}

@article{sorensen_metaheuristicsmetaphor_2015,
	title = {Metaheuristics—the metaphor exposed},
	volume = {22},
	copyright = {© 2013 The Authors. International Transactions in Operational Research © 2013 International Federation of Operational Research Societies Published by John Wiley \& Sons Ltd, 9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main St, Malden, MA02148, USA.},
	issn = {1475-3995},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/itor.12001},
	doi = {10.1111/itor.12001},
	abstract = {In recent years, the field of combinatorial optimization has witnessed a true tsunami of “novel” metaheuristic methods, most of them based on a metaphor of some natural or man-made process. The behavior of virtually any species of insects, the flow of water, musicians playing together – it seems that no idea is too far-fetched to serve as inspiration to launch yet another metaheuristic. In this paper, we will argue that this line of research is threatening to lead the area of metaheuristics away from scientific rigor. We will examine the historical context that gave rise to the increasing use of metaphors as inspiration and justification for the development of new methods, discuss the reasons for the vulnerability of the metaheuristics field to this line of research, and point out its fallacies. At the same time, truly innovative research of high quality is being performed as well. We conclude the paper by discussing some of the properties of this research and by pointing out some of the most promising research avenues for the field of metaheuristics.},
	language = {en},
	number = {1},
	urldate = {2023-10-13},
	journal = {International Transactions in Operational Research},
	author = {Sörensen, Kenneth},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/itor.12001},
	keywords = {combinatorial optimization, heuristics, metaheuristics, optimization},
	pages = {3--18},
	file = {PDF Snapshot:/home/jeroen/Zotero/storage/HCB6R6BK/2013 - Wayback Machine.pdf:application/pdf;Snapshot:/home/jeroen/Zotero/storage/BIR54TBV/itor.html:text/html},
}

@article{mannino_pathcycle_2018,
	title = {The {Path}\&{Cycle} {Formulation} for the {Hotspot} {Problem} in {Air} {Traffic} {Management}},
	copyright = {Creative Commons Attribution 3.0 Unported license (CC-BY 3.0)},
	url = {http://drops.dagstuhl.de/opus/volltexte/2018/9719/},
	doi = {10.4230/OASICS.ATMOS.2018.14},
	abstract = {The Hotspot Problem in Air Traﬃc Management consists of optimally rescheduling a set of airplanes that are forecast to occupy an overcrowded region of the airspace, should they follow their original schedule. We ﬁrst provide a MILP model for the Hotspot Problem using a standard big-M formulation. Then, we present a novel MILP model that gets rid of the big-M coeﬃcients. The new formulation contains only simple combinatorial constraints, corresponding to paths and cycles in an associated disjunctive graph. We report computational results on a set of randomly generated instances. In the experiments, the new formulation consistently outperforms the big-M formulation, both in terms of running times and number of branching nodes.},
	language = {en},
	urldate = {2023-10-12},
	author = {Mannino, Carlo and Sartor, Giorgio},
	collaborator = {Wagner, Michael},
	year = {2018},
	note = {Artwork Size: 11 pages
Medium: application/pdf
Publisher: Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany},
	keywords = {000 Computer science, knowledge, general works, Computer Science},
	pages = {11 pages},
	annote = {Other
The Hotspot Problem in Air Traffic Management consists of optimally rescheduling a set of airplanes that are forecast to occupy an overcrowded region of the airspace, should they follow their original schedule. We first provide a MILP model for the Hotspot Problem using a standard big-M formulation. Then, we present a novel MILP model that gets rid of the big-M coefficients. The new formulation contains only simple combinatorial constraints, corresponding to paths and cycles in an associated disjunctive graph. We report computational results on a set of randomly generated instances. In the experiments, the new formulation consistently outperforms the big-M formulation, both in terms of running times and number of branching nodes.},
	file = {Mannino and Sartor - 2018 - The Path&amp\;Cycle Formulation for the Hotspot Pro.pdf:/home/jeroen/Zotero/storage/PKMK754N/Mannino and Sartor - 2018 - The Path&amp\;Cycle Formulation for the Hotspot Pro.pdf:application/pdf},
}

@article{monch_distributed_2005,
	title = {A distributed shifting bottleneck heuristic for complex job shops},
	volume = {49},
	issn = {0360-8352},
	url = {https://www.sciencedirect.com/science/article/pii/S0360835205000793},
	doi = {10.1016/j.cie.2005.06.004},
	abstract = {In this paper, we consider distributed versions of a modified shifting bottleneck heuristic for complex job shops. The considered job shop environment contains parallel batching machines, machines with sequence-dependent setup times and reentrant process flows. Semiconductor wafer fabrication facilities are typical examples for manufacturing systems with these characteristics. The used performance measure is total weighted tardiness (TWT). We suggest a two-layer hierarchical approach in order to decompose the overall scheduling problem. The upper (or top) layer works on an aggregated model. Based on appropriately aggregated routes it determines start dates and planned due dates for the jobs within each single work area, where a work area is defined as a set of parallel machine groups. The lower (or base) layer uses the start dates and planned due dates in order to apply shifting bottleneck heuristic type solution approaches for the jobs in each single work area. We conduct simulation experiments in a dynamic job shop environment in order to assess the performance of the heuristic. It turns out that the suggested approach outperforms a pure First In First Out (FIFO) dispatching scheme and provides a similar solution quality as the original modified shifting bottleneck heuristic.},
	number = {3},
	urldate = {2023-10-12},
	journal = {Computers \& Industrial Engineering},
	author = {Mönch, Lars and Drießel, René},
	month = nov,
	year = {2005},
	keywords = {Complex job shops, Computational experiments, Distributed scheduling, Hierarchical production control, Shifting bottleneck heuristic},
	pages = {363--380},
	file = {ScienceDirect Full Text PDF:/home/jeroen/Zotero/storage/EESQXWLU/Mönch and Drießel - 2005 - A distributed shifting bottleneck heuristic for co.pdf:application/pdf;ScienceDirect Snapshot:/home/jeroen/Zotero/storage/RPCP6CZG/S0360835205000793.html:text/html},
}

@article{lamorgese_noncompact_2019,
	title = {A {Noncompact} {Formulation} for {Job}-{Shop} {Scheduling} {Problems} in {Traffic} {Management}},
	volume = {67},
	issn = {0030-364X},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/opre.2018.1837},
	doi = {10.1287/opre.2018.1837},
	abstract = {A central problem in traffic management is that of scheduling the movements of vehicles so as to minimize the cost of the schedule. It arises in important applications such as train timetabling, rescheduling, delay and disruption management, airplane surface routing, runway scheduling, air-traffic control, and more. This problem can be modeled as a job-shop scheduling problem. We introduce a new mixed-integer linear program (MILP) formulation for job-shop scheduling, which is an alternative to classical approaches, namely, big-M and time-indexed formulations. It does not make use of artificially large coefficients, and its constraints correspond to basic graph structures, such as paths, cycles, and trees. The new formulation can be obtained by strengthening and lifting the constraints of a classical Benders’ reformulation. Tests on a large set of real-life instances from train rescheduling show that the new approach performs on average better than our previous approaches based on big-M formulations and particularly better on a class of instances with nonconvex costs very common in the practice.},
	number = {6},
	urldate = {2023-10-12},
	journal = {Operations Research},
	author = {Lamorgese, Leonardo and Mannino, Carlo},
	month = nov,
	year = {2019},
	note = {Publisher: INFORMS},
	keywords = {algorithms, Benders' decomposition, Integer programming, scheduling, Transportation, Transportation technology, transportation: scheduling, vehicles},
	pages = {1586--1609},
	file = {Accepted Version:/home/jeroen/Zotero/storage/HASPKTYU/Lamorgese and Mannino - 2019 - A Noncompact Formulation for Job-Shop Scheduling P.pdf:application/pdf},
}

@inproceedings{sartor_combinatorial_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Combinatorial {Learning} in {Traffic} {Management}},
	isbn = {978-3-030-37599-7},
	doi = {10.1007/978-3-030-37599-7_32},
	abstract = {We describe an exact combinatorial learning approach to solve dynamic job-shop scheduling problems arising in traffic management. When a set of vehicles has to be controlled in real-time, a new schedule must be computed whenever a deviation from the current plan is detected, or periodically after a short amount of time. This suggests that each two (or more) consecutive instances will be very similar. We exploit a recently introduced MILP formulation for job-shop scheduling (called path\&cycle) to develop an effective solution algorithm based on delayed row generation. In our re-optimization framework, the algorithm maintains a pool of combinatorial cuts separated during the solution of previous instances, and adapts them to warm start each new instance. In our experiments, this adaptive approach led to a 4-time average speedup over the static approach (where each instance is solved independently) for a critical application in air traffic management.},
	language = {en},
	booktitle = {Machine {Learning}, {Optimization}, and {Data} {Science}},
	publisher = {Springer International Publishing},
	author = {Sartor, Giorgio and Mannino, Carlo and Bach, Lukas},
	editor = {Nicosia, Giuseppe and Pardalos, Panos and Umeton, Renato and Giuffrida, Giovanni and Sciacca, Vincenzo},
	year = {2019},
	keywords = {Job-shop scheduling, Mixed Integer Linear Programming, Re-optimization},
	pages = {384--395},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/IKJPBJI2/Sartor et al. - 2019 - Combinatorial Learning in Traffic Management.pdf:application/pdf},
}

@misc{he_online_2022,
	title = {Online {Planning} in {POMDPs} with {Self}-{Improving} {Simulators}},
	url = {http://arxiv.org/abs/2201.11404},
	doi = {10.48550/arXiv.2201.11404},
	abstract = {How can we plan efficiently in a large and complex environment when the time budget is limited? Given the original simulator of the environment, which may be computationally very demanding, we propose to learn online an approximate but much faster simulator that improves over time. To plan reliably and efficiently while the approximate simulator is learning, we develop a method that adaptively decides which simulator to use for every simulation, based on a statistic that measures the accuracy of the approximate simulator. This allows us to use the approximate simulator to replace the original simulator for faster simulations when it is accurate enough under the current context, thus trading off simulation speed and accuracy. Experimental results in two large domains show that when integrated with POMCP, our approach allows to plan with improving efficiency over time.},
	urldate = {2023-09-27},
	publisher = {arXiv},
	author = {He, Jinke and Suau, Miguel and Baier, Hendrik and Kaisers, Michael and Oliehoek, Frans A.},
	month = dec,
	year = {2022},
	note = {arXiv:2201.11404 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: presented at IJCAI 2022},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/6CY9E44N/He et al. - 2022 - Online Planning in POMDPs with Self-Improving Simu.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/427D6ZVR/2201.html:text/html},
}

@misc{zhang_learning_2020,
	title = {Learning to {Dispatch} for {Job} {Shop} {Scheduling} via {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2010.12367},
	doi = {10.48550/arXiv.2010.12367},
	abstract = {Priority dispatching rule (PDR) is widely used for solving real-world Job-shop scheduling problem (JSSP). However, the design of effective PDRs is a tedious task, requiring a myriad of specialized knowledge and often delivering limited performance. In this paper, we propose to automatically learn PDRs via an end-to-end deep reinforcement learning agent. We exploit the disjunctive graph representation of JSSP, and propose a Graph Neural Network based scheme to embed the states encountered during solving. The resulting policy network is size-agnostic, effectively enabling generalization on large-scale instances. Experiments show that the agent can learn high-quality PDRs from scratch with elementary raw features, and demonstrates strong performance against the best existing PDRs. The learned policies also perform well on much larger instances that are unseen in training.},
	urldate = {2023-09-27},
	publisher = {arXiv},
	author = {Zhang, Cong and Song, Wen and Cao, Zhiguang and Zhang, Jie and Tan, Puay Siew and Xu, Chi},
	month = oct,
	year = {2020},
	note = {arXiv:2010.12367 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/X9IDBWNN/Zhang et al. - 2020 - Learning to Dispatch for Job Shop Scheduling via D.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/DIPBMFPB/2010.html:text/html},
}

@misc{tassel_reinforcement_2021,
	title = {A {Reinforcement} {Learning} {Environment} {For} {Job}-{Shop} {Scheduling}},
	url = {http://arxiv.org/abs/2104.03760},
	abstract = {Scheduling is a fundamental task occurring in various automated systems applications, e.g., optimal schedules for machines on a job shop allow for a reduction of production costs and waste. Nevertheless, finding such schedules is often intractable and cannot be achieved by Combinatorial Optimization Problem (COP) methods within a given time limit. Recent advances of Deep Reinforcement Learning (DRL) in learning complex behavior enable new COP application possibilities. This paper presents an efficient DRL environment for Job-Shop Scheduling -- an important problem in the field. Furthermore, we design a meaningful and compact state representation as well as a novel, simple dense reward function, closely related to the sparse make-span minimization criteria used by COP methods. We demonstrate that our approach significantly outperforms existing DRL methods on classic benchmark instances, coming close to state-of-the-art COP approaches.},
	urldate = {2023-09-27},
	publisher = {arXiv},
	author = {Tassel, Pierre and Gebser, Martin and Schekotihin, Konstantin},
	month = apr,
	year = {2021},
	note = {arXiv:2104.03760 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 7 pages, 4 figures, 1 table},
	file = {arXiv.org Snapshot:/home/jeroen/Zotero/storage/6JZN4CA5/2104.html:text/html;Full Text PDF:/home/jeroen/Zotero/storage/SJQW33NS/Tassel et al. - 2021 - A Reinforcement Learning Environment For Job-Shop .pdf:application/pdf},
}

@article{fischetti_deep_2018,
	title = {Deep neural networks and mixed integer linear optimization},
	volume = {23},
	issn = {1572-9354},
	url = {https://doi.org/10.1007/s10601-018-9285-6},
	doi = {10.1007/s10601-018-9285-6},
	abstract = {Deep Neural Networks (DNNs) are very popular these days, and are the subject of a very intense investigation. A DNN is made up of layers of internal units (or neurons), each of which computes an affine combination of the output of the units in the previous layer, applies a nonlinear operator, and outputs the corresponding value (also known as activation). A commonly-used nonlinear operator is the so-called rectified linear unit (ReLU), whose output is just the maximum between its input value and zero. In this (and other similar cases like max pooling, where the max operation involves more than one input value), for fixed parameters one can model the DNN as a 0-1 Mixed Integer Linear Program (0-1 MILP) where the continuous variables correspond to the output values of each unit, and a binary variable is associated with each ReLU to model its yes/no nature. In this paper we discuss the peculiarity of this kind of 0-1 MILP models, and describe an effective bound-tightening technique intended to ease its solution. We also present possible applications of the 0-1 MILP model arising in feature visualization and in the construction of adversarial examples. Computational results are reported, aimed at investigating (on small DNNs) the computational performance of a state-of-the-art MILP solver when applied to a known test case, namely, hand-written digit recognition.},
	language = {en},
	number = {3},
	urldate = {2023-09-27},
	journal = {Constraints},
	author = {Fischetti, Matteo and Jo, Jason},
	month = jul,
	year = {2018},
	keywords = {Deep neural networks, Computational experiments, Deep learning, Mathematical optimization, Mixed-integer programming},
	pages = {296--309},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/3IUMWN5Z/Fischetti and Jo - 2018 - Deep neural networks and mixed integer linear opti.pdf:application/pdf},
}

@article{shengren_optimal_2023,
	title = {Optimal energy system scheduling using a constraint-aware reinforcement learning algorithm},
	volume = {152},
	issn = {01420615},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0142061523002879},
	doi = {10.1016/j.ijepes.2023.109230},
	abstract = {The massive integration of renewable-based distributed energy resources (DERs) inherently increases the energy system’s complexity, especially when it comes to defining its operational schedule. Deep reinforcement learning (DRL) algorithms arise as a promising solution due to their data-driven and model-free features. However, current DRL algorithms fail to enforce rigorous operational constraints (e.g., power balance, ramping up or down constraints) limiting their implementation in real systems. To overcome this, in this paper, a DRL algorithm (namely MIP-DQN) is proposed, capable of strictly enforcing all operational constraints in the action space, ensuring the feasibility of the defined schedule in real-time operation. This is done by leveraging recent optimization advances for deep neural networks (DNNs) that allow their representation as a MIP formulation, enabling further consideration of any action space constraints. Comprehensive numerical simulations show that the proposed algorithm outperforms existing state-of-the-art DRL algorithms, obtaining a lower error when compared with the optimal global solution (upper boundary) obtained after solving a mathematical programming formulation with perfect forecast information; while strictly enforcing all operational constraints (even in unseen test days).},
	language = {en},
	urldate = {2023-09-27},
	journal = {International Journal of Electrical Power \& Energy Systems},
	author = {Shengren, Hou and Vergara, Pedro P. and Salazar Duque, Edgar Mauricio and Palensky, Peter},
	month = oct,
	year = {2023},
	pages = {109230},
	file = {Shengren et al. - 2023 - Optimal energy system scheduling using a constrain.pdf:/home/jeroen/Zotero/storage/ZDQYZ7TW/Shengren et al. - 2023 - Optimal energy system scheduling using a constrain.pdf:application/pdf},
}

@misc{noauthor_multi-agent_nodate,
	title = {Multi-{Agent} {Reinforcement} {Learning}: {Foundations} and {Modern} {Approaches}},
	shorttitle = {Multi-{Agent} {Reinforcement} {Learning}},
	url = {https://www.marl-book.com/},
	abstract = {Textbook published by MIT Press},
	language = {en},
	urldate = {2023-09-27},
	file = {Snapshot:/home/jeroen/Zotero/storage/HXKEIUWE/www.marl-book.com.html:text/html},
}

@article{busoniu_comprehensive_2008,
	title = {A {Comprehensive} {Survey} of {Multiagent} {Reinforcement} {Learning}},
	volume = {38},
	issn = {1094-6977, 1558-2442},
	url = {https://ieeexplore.ieee.org/document/4445757/},
	doi = {10.1109/TSMCC.2007.913919},
	abstract = {Multi-agent systems are rapidly ﬁnding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difﬁcult to solve with preprogrammed agent behaviors. The agents must instead discover a solution on their own, using learning. A signiﬁcant part of the research on multi-agent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multi-agent reinforcement learning (MARL). A central issue in the ﬁeld is the formal statement of the multi-agent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents’ learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim—either explicitly or implicitly—at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the speciﬁc issues that arise in each category. Additionally, the beneﬁts and challenges of MARL are described along with some of the problem domains where MARL techniques have been applied. Finally, an outlook for the ﬁeld is provided.},
	language = {en},
	number = {2},
	urldate = {2023-09-27},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
	month = mar,
	year = {2008},
	keywords = {MARL},
	pages = {156--172},
	file = {Busoniu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement.pdf:/home/jeroen/Zotero/storage/29BVG529/Busoniu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement.pdf:application/pdf},
}

@misc{kaelbling_reinforcement_1996,
	title = {Reinforcement {Learning}: {A} {Survey}},
	shorttitle = {Reinforcement {Learning}},
	url = {http://arxiv.org/abs/cs/9605103},
	abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
	urldate = {2023-09-27},
	publisher = {arXiv},
	author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
	month = apr,
	year = {1996},
	note = {arXiv:cs/9605103},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: See http://www.jair.org/ for any accompanying files},
	file = {arXiv.org Snapshot:/home/jeroen/Zotero/storage/LASUXHLE/9605103.html:text/html;Full Text PDF:/home/jeroen/Zotero/storage/L7PTKLKD/Kaelbling et al. - 1996 - Reinforcement Learning A Survey.pdf:application/pdf},
}

@article{glaubius_real-time_nodate,
	title = {Real-{Time} {Scheduling} via {Reinforcement} {Learning}},
	abstract = {Cyber-physical systems, such as mobile robots, must respond adaptively to dynamic operating conditions. Eﬀective operation of these systems requires that sensing and actuation tasks are performed in a timely manner. Additionally, execution of mission speciﬁc tasks such as imaging a room must be balanced against the need to perform more general tasks such as obstacle avoidance. This problem has been addressed by maintaining relative utilization of shared resources among tasks near a user-speciﬁed target level. Producing optimal scheduling strategies requires complete prior knowledge of task behavior, which is unlikely to be available in practice. Instead, suitable scheduling strategies must be learned online through interaction with the system. We consider the sample complexity of reinforcement learning in this domain, and demonstrate that while the problem state space is countably inﬁnite, we may leverage the problem’s structure to guarantee eﬃcient learning.},
	language = {en},
	author = {Glaubius, Robert and Tidwell, Terry and Gill, Christopher and Smart, William D},
	file = {Glaubius et al. - Real-Time Scheduling via Reinforcement Learning.pdf:/home/jeroen/Zotero/storage/GXTLU4S6/Glaubius et al. - Real-Time Scheduling via Reinforcement Learning.pdf:application/pdf},
}

@article{shyalika_reinforcement_2020,
	title = {Reinforcement {Learning} in {Dynamic} {Task} {Scheduling}: {A} {Review}},
	volume = {1},
	issn = {2661-8907},
	shorttitle = {Reinforcement {Learning} in {Dynamic} {Task} {Scheduling}},
	url = {https://doi.org/10.1007/s42979-020-00326-5},
	doi = {10.1007/s42979-020-00326-5},
	abstract = {Scheduling is assigning shared resources over time to efficiently complete the tasks over a given period of time. The term is applied separately for tasks and resources correspondingly in task scheduling and resource allocation. Scheduling is a popular topic in operational management and computer science. Effective schedules ensure system efficiency, effective decision making, minimize resource wastage and cost, and enhance overall productivity. It is generally a tedious task to choose the most accurate resources in performing work items and schedules in both computing and business process execution. Especially in real-world dynamic systems where multiple agents involve in scheduling various dynamic tasks is a challenging issue. Reinforcement Learning is an emergent technology which has been able to solve the problem of the optimal task and resource scheduling dynamically. This review paper is about a research study that focused on Reinforcement Learning techniques that have been used for dynamic task scheduling. The paper addresses the results of the study by means of the state-of-the-art on Reinforcement learning techniques used in dynamic task scheduling and a comparative review of those techniques.},
	language = {en},
	number = {6},
	urldate = {2023-09-27},
	journal = {SN Computer Science},
	author = {Shyalika, Chathurangi and Silva, Thushari and Karunananda, Asoka},
	month = sep,
	year = {2020},
	keywords = {Reinforcement learning, Dynamic, Environment uncertainty, Multi-agent, Task scheduling},
	pages = {306},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/P83QVV5V/Shyalika et al. - 2020 - Reinforcement Learning in Dynamic Task Scheduling.pdf:application/pdf},
}

@inproceedings{jin_common_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Common {Structures} in {Resource} {Management} as {Driver} for {Reinforcement} {Learning}: {A} {Survey} and {Research} {Tracks}},
	isbn = {978-3-030-19945-6},
	shorttitle = {Common {Structures} in {Resource} {Management} as {Driver} for {Reinforcement} {Learning}},
	doi = {10.1007/978-3-030-19945-6_8},
	abstract = {In the era of growing digitalization, dynamic resource management becomes one of the critical problems in many application fields where, due to the permanently evolving environment, the trade-off between cost and system performance needs to be continuously adapted. While traditional approaches based on prior system specification or model learning are challenged by the complexity and the dynamicity of these systems, a new paradigm of learning in interaction brings a strong promise - based on the toolset of model-free Reinforcement Learning (RL) and its great success stories in various domains. However, current RL methods still struggle to learn rapidly in incremental, online settings, which is a barrier to deal with many practical problems. To address the slow convergence issue, one approach consists in exploiting the system’s structural properties, instead of acting in full model-free mode. In this paper, we review the existing resource management systems and unveil their common structural properties. We propose a meta-model and discuss the tracks on how these properties can enhance general purpose RL algorithms.},
	language = {en},
	booktitle = {Machine {Learning} for {Networking}},
	publisher = {Springer International Publishing},
	author = {Jin, Yue and Kostadinov, Dimitre and Bouzid, Makram and Aghasaryan, Armen},
	editor = {Renault, Éric and Mühlethaler, Paul and Boumerdassi, Selma},
	year = {2019},
	keywords = {Capacity management, Learning through interactions, Resource management, RL},
	pages = {117--132},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/TBHHD5TB/Jin et al. - 2019 - Common Structures in Resource Management as Driver.pdf:application/pdf},
}

@inproceedings{kuyer_multiagent_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multiagent {Reinforcement} {Learning} for {Urban} {Traffic} {Control} {Using} {Coordination} {Graphs}},
	isbn = {978-3-540-87479-9},
	doi = {10.1007/978-3-540-87479-9_61},
	abstract = {Since traffic jams are ubiquitous in the modern world, optimizing the behavior of traffic lights for efficient traffic flow is a critically important goal. Though most current traffic lights use simple heuristic protocols, more efficient controllers can be discovered automatically via multiagent reinforcement learning, where each agent controls a single traffic light. However, in previous work on this approach, agents select only locally optimal actions without coordinating their behavior. This paper extends this approach to include explicit coordination between neighboring traffic lights. Coordination is achieved using the max-plus algorithm, which estimates the optimal joint action by sending locally optimized messages among connected agents. This paper presents the first application of max-plus to a large-scale problem and thus verifies its efficacy in realistic settings. It also provides empirical evidence that max-plus performs well on cyclic graphs, though it has been proven to converge only for tree-structured graphs. Furthermore, it provides a new understanding of the properties a traffic network must have for such coordination to be beneficial and shows that max-plus outperforms previous methods on networks that possess those properties.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Kuyer, Lior and Whiteson, Shimon and Bakker, Bram and Vlassis, Nikos},
	editor = {Daelemans, Walter and Goethals, Bart and Morik, Katharina},
	year = {2008},
	keywords = {coordination graphs, max-plus, multiagent systems, reinforcement learning, traffic control},
	pages = {656--671},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/V7P2JNL8/Kuyer et al. - 2008 - Multiagent Reinforcement Learning for Urban Traffi.pdf:application/pdf},
}

@article{yau_survey_2018,
	title = {A {Survey} on {Reinforcement} {Learning} {Models} and {Algorithms} for {Traffic} {Signal} {Control}},
	volume = {50},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3068287},
	doi = {10.1145/3068287},
	abstract = {Traffic congestion has become a vexing and complex issue in many urban areas. Of particular interest are the intersections where traffic bottlenecks are known to occur despite being traditionally signalized. Reinforcement learning (RL), which is an artificial intelligence approach, has been adopted in traffic signal control for monitoring and ameliorating traffic congestion. RL enables autonomous decision makers (e.g., traffic signal controllers) to observe, learn, and select the optimal action (e.g., determining the appropriate traffic phase and its timing) to manage traffic such that system performance is improved. This article reviews various RL models and algorithms applied to traffic signal control in the aspects of the representations of the RL model (i.e., state, action, and reward), performance measures, and complexity to establish a foundation for further investigation in this research field. Open issues are presented toward the end of this article to discover new research areas with the objective to spark new interest in this research field.},
	language = {en},
	number = {3},
	urldate = {2023-09-26},
	journal = {ACM Computing Surveys},
	author = {Yau, Kok-Lim Alvin and Qadir, Junaid and Khoo, Hooi Ling and Ling, Mee Hong and Komisarczuk, Peter},
	month = may,
	year = {2018},
	pages = {1--38},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/4UJSP6QX/Yau et al. - 2018 - A Survey on Reinforcement Learning Models and Algo.pdf:application/pdf},
}

@article{bazzan_opportunities_2009,
	title = {Opportunities for multiagent systems and multiagent reinforcement learning in traffic control},
	volume = {18},
	issn = {1573-7454},
	url = {https://doi.org/10.1007/s10458-008-9062-9},
	doi = {10.1007/s10458-008-9062-9},
	abstract = {The increasing demand for mobility in our society poses various challenges to traffic engineering, computer science in general, and artificial intelligence and multiagent systems in particular. As it is often the case, it is not possible to provide additional capacity, so that a more efficient use of the available transportation infrastructure is necessary. This relates closely to multiagent systems as many problems in traffic management and control are inherently distributed. Also, many actors in a transportation system fit very well the concept of autonomous agents: the driver, the pedestrian, the traffic expert; in some cases, also the intersection and the traffic signal controller can be regarded as an autonomous agent. However, the “agentification” of a transportation system is associated with some challenging issues: the number of agents is high, typically agents are highly adaptive, they react to changes in the environment at individual level but cause an unpredictable collective pattern, and act in a highly coupled environment. Therefore, this domain poses many challenges for standard techniques from multiagent systems such as coordination and learning. This paper has two main objectives: (i) to present problems, methods, approaches and practices in traffic engineering (especially regarding traffic signal control); and (ii) to highlight open problems and challenges so that future research in multiagent systems can address them.},
	language = {en},
	number = {3},
	urldate = {2023-09-26},
	journal = {Autonomous Agents and Multi-Agent Systems},
	author = {Bazzan, Ana L. C.},
	month = jun,
	year = {2009},
	keywords = {Reinforcement learning, Coordination of agents, Game-theory, Multiagent learning, Multiagent systems, Traffic signal control},
	pages = {342--375},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/JXBA9ZN2/Bazzan - 2009 - Opportunities for multiagent systems and multiagen.pdf:application/pdf},
}

@book{oliehoek_concise_2016,
	address = {Cham},
	series = {{SpringerBriefs} in {Intelligent} {Systems}},
	title = {A {Concise} {Introduction} to {Decentralized} {POMDPs}},
	isbn = {978-3-319-28927-4 978-3-319-28929-8},
	url = {http://link.springer.com/10.1007/978-3-319-28929-8},
	language = {en},
	urldate = {2023-09-26},
	publisher = {Springer International Publishing},
	author = {Oliehoek, Frans A. and Amato, Christopher},
	year = {2016},
	doi = {10.1007/978-3-319-28929-8},
	file = {Oliehoek and Amato - 2016 - A Concise Introduction to Decentralized POMDPs.pdf:/home/jeroen/Zotero/storage/AVTJKWEE/Oliehoek and Amato - 2016 - A Concise Introduction to Decentralized POMDPs.pdf:application/pdf},
}

@article{antes_information_2022-1,
	series = {The 13th {International} {Conference} on {Ambient} {Systems}, {Networks} and {Technologies} ({ANT}) / {The} 5th {International} {Conference} on {Emerging} {Data} and {Industry} 4.0 ({EDI40})},
	title = {Information upwards, recommendation downwards: reinforcement learning with hierarchy for traffic signal control},
	volume = {201},
	issn = {1877-0509},
	shorttitle = {Information upwards, recommendation downwards},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050922004185},
	doi = {10.1016/j.procs.2022.03.006},
	abstract = {Traffic signal control (TSC) is a practical solution to the major problem of congestion in metropolitan areas. Reinforcement Learning (RL) techniques present powerful frameworks for optimizing traffic signal controllers that learn to respond to real-time traffic changes. Multiagent RL (MARL) techniques have been showing better results over centralized techniques (RL-based or not), where local intersection agents have partial observation of and control over the environment. Since in TSC the best decision does not depend only on local information, in the present paper we aim at increasing agents’ views by using a hierarchical approach, where information is passed upwards, is then aggregated forming recommendations that are sent downwards. We divide the transportation network into regions, each controlled by a region agent; this is done at different hierarchical levels. The traffic signal controllers, located at the intersections, are the local agents at the hierarchy’s bottom. Region agents can supervise intersection agents or other region agents. Evaluation of this approach in a synthetic traffic grid shows that the proposed hierarchical organization outperforms a fixed-time approach and an RL-based approach without hierarchy.},
	urldate = {2023-09-26},
	journal = {Procedia Computer Science},
	author = {Antes, Taylor de O. and Bazzan, Ana L. C. and Tavares, Anderson Rocha},
	month = jan,
	year = {2022},
	keywords = {Reinforcement Learning, Intelligent Transportation Systems, Multiagent Systems, Smart Cities},
	pages = {24--31},
	file = {ScienceDirect Full Text PDF:/home/jeroen/Zotero/storage/T5W7ATZD/Antes et al. - 2022 - Information upwards, recommendation downwards rei.pdf:application/pdf;ScienceDirect Snapshot:/home/jeroen/Zotero/storage/6IRNMEHS/S1877050922004185.html:text/html},
}

@article{de_almeida_multiagent_nodate,
	title = {Multiagent {Reinforcement} {Learning} for {Traffic} {Signal} {Control}: a k-{Nearest} {Neighbors} {Based} {Approach}},
	abstract = {The increasing demand for mobility in our society poses various challenges to traffic engineering, computer science in general, and artificial intelligence in particular. As it is often the case, it is not possible to increase the capacity of road networks, therefore a more efficient use of the available transportation infrastructure is required. This relates closely to multiagent systems and to multiagent reinforcement learning, as many problems in traffic management and control are inherently distributed. However, one of the main challenges of this domain is that the state space is large and continuous, which makes it difficult to properly discretize states and also causes many RL algorithms to converge more slowly. To address these issues, a multiagent system with agents learning independently via a temporal difference learning algorithm based on k-nearest neighbors is presented as an option to control traffic signals in real-time. Our results show that the proposed method is both effective (reduces the average waiting time of vehicles in a traffic network) and efficient (the learning task is significantly accelerated), when compared to a baseline reported in the literature.},
	language = {en},
	author = {de Almeida, Vicente N and Bazzan, Ana L C and Abdoos, Monireh},
	file = {de Almeida et al. - Multiagent Reinforcement Learning for Traffic Sign.pdf:/home/jeroen/Zotero/storage/AZ4W2BV2/de Almeida et al. - Multiagent Reinforcement Learning for Traffic Sign.pdf:application/pdf},
}

@book{noauthor_multi-agent_nodate-1,
	title = {Multi-{Agent} {Reinforcement} {Learning}: {Foundations} and {Modern} {Approaches}},
	shorttitle = {Multi-{Agent} {Reinforcement} {Learning}},
	url = {https://www.marl-book.com/},
	abstract = {Textbook published by MIT Press},
	language = {en},
	urldate = {2023-09-25},
	file = {Multi-Agent Reinforcement Learning Foundations an.pdf:/home/jeroen/Zotero/storage/J6TYKEA5/Multi-Agent Reinforcement Learning Foundations an.pdf:application/pdf;Snapshot:/home/jeroen/Zotero/storage/6787IHI8/www.marl-book.com.html:text/html},
}

@article{fleuren_optimizing_2017,
	title = {Optimizing pre-timed control at isolated intersections},
	url = {https://pure.tue.nl/ws/portalfiles/portal/52367681/20170119_Fleuren.pdf},
	urldate = {2023-01-06},
	author = {Fleuren, Stijn},
	year = {2017},
	keywords = {MILP},
	file = {20170119_Fleuren.pdf:/home/jeroen/Zotero/storage/JFULKAUB/20170119_Fleuren.pdf:application/pdf},
}

@article{li_point_2022,
	title = {{POINT}: {Partially} {Observable} {Imitation} {Network} for {Traffic} {Signal} {Control}},
	volume = {76},
	issn = {2210-6707},
	shorttitle = {{POINT}},
	url = {https://www.sciencedirect.com/science/article/pii/S2210670721007344},
	doi = {10.1016/j.scs.2021.103461},
	abstract = {Smart traffic signals bring together transportation infrastructure and advance technologies to improve the mobility and efficiency of urban transportation network. Adaptive traffic signal control studies can be categorized into modeling-based approaches and learning-based approaches. In order to take advantages of these two systems, this study developed an offline-online combined Partial Observable Imitation Network for Traffic signal control (POINT). In the offline system, the traffic signal timing optimization problem was formulated as a Mixed Integer Nonlinear Programming (MINLP) given complete traffic information, i.e., second-by-second speeds and locations of all vehicles. The objective of MINLP is to minimize total travel delays considering individual vehicle trajectories under Connected Vehicle (CV) environment. The calculated optimal solutions under various traffic conditions were considered as the ”expert” decisions. In the online system, an imitation neural network model was developed to learn the ”expert” signal plans generated from offline system. Given partial observable traffic conditions in real time, e.g., the aggregate-level of traffic volume, the POINT model can compute the signal timing parameters in the online system. The numerical results demonstrated that the proposed method outperformed other state-of-the-art signal control method under high and unbalanced traffic demand levels in terms of reducing travel delays and queue length.},
	language = {en},
	urldate = {2023-01-20},
	journal = {Sustainable Cities and Society},
	author = {Li, Wan and Wang, Boyu and Liu, Zhanlin and Li, Qiang and Qi, Guo-Jun},
	month = jan,
	year = {2022},
	keywords = {MILP, Adaptive traffic signal control system, Connected vehicle, Imitation network, immitation learning, Vehicle trajectories},
	pages = {103461},
	file = {ScienceDirect Full Text PDF:/home/jeroen/Zotero/storage/LXKSI7DN/Li et al. - 2022 - POINT Partially Observable Imitation Network for .pdf:application/pdf;ScienceDirect Snapshot:/home/jeroen/Zotero/storage/LRPEMQ3X/S2210670721007344.html:text/html},
}

@article{he_pamscod_2012,
	title = {{PAMSCOD}: {Platoon}-based arterial multi-modal signal control with online data},
	volume = {20},
	issn = {0968090X},
	shorttitle = {{PAMSCOD}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0968090X11000775},
	doi = {10.1016/j.trc.2011.05.007},
	abstract = {A uniﬁed platoon-based mathematical formulation called PAMSCOD is presented to perform arterial (network) trafﬁc signal control while considering multiple travel modes in a vehicle-to-infrastructure communications environment. First, a headway-based platoon recognition algorithm is developed to identify pseudo-platoons given probe vehicles’ online information. It is assumed that passenger vehicles constitute a signiﬁcant majority of the vehicles in the network. This algorithm identiﬁes existing queues and signiﬁcant platoons approaching each intersection. Second, a mixed-integer linear program (MILP) is solved to determine future optimal signal plans based on the current trafﬁc controller status, online platoon data and priority requests from special vehicles, such as transit buses. Deviating from the traditional common network cycle length, PAMSCOD aims to provide multi-modal dynamical progression (MDP) on the arterial based on the probe information. Microscopic simulation using VISSIM shows that PAMSCOD can easily handle two common trafﬁc modes, transit buses and automobiles, and signiﬁcantly reduce delays for both modes under both non-saturated and oversaturated trafﬁc conditions as compared to traditional state-of-practice coordinated-actuated signal control with timings optimized by SYNCHRO.},
	language = {en},
	number = {1},
	urldate = {2023-01-20},
	journal = {Transportation Research Part C: Emerging Technologies},
	author = {He, Qing and Head, K. Larry and Ding, Jun},
	month = feb,
	year = {2012},
	keywords = {MILP},
	pages = {164--184},
	file = {He et al. - 2012 - PAMSCOD Platoon-based arterial multi-modal signal.pdf:/home/jeroen/Zotero/storage/FNICQCLU/He et al. - 2012 - PAMSCOD Platoon-based arterial multi-modal signal.pdf:application/pdf},
}

@inproceedings{ashtiani_multi-intersection_2018,
	title = {Multi-{Intersection} {Traffic} {Management} for {Autonomous} {Vehicles} via {Distributed} {Mixed} {Integer} {Linear} {Programming}},
	url = {http://arxiv.org/abs/2007.06639},
	doi = {10.23919/ACC.2018.8431656},
	abstract = {This paper extends our previous work in [1],[2], on optimal scheduling of autonomous vehicle arrivals at intersections, from one to a grid of intersections. A scalable distributed Mixed Integer Linear Program (MILP) is devised that solves the scheduling problem for a grid of intersections. A computational control node is allocated to each intersection and regularly receives position and velocity information from subscribed vehicles. Each node assigns an intersection access time to every subscribed vehicle by solving a local MILP. Neighboring intersections will coordinate with each other in real-time by sharing their solutions for vehicles' access times with each other. Our proposed approach is applied to a grid of nine intersections and its positive impact on traffic flow and vehicles' fuel economy is demonstrated in comparison to conventional intersection control scenarios.},
	urldate = {2023-09-14},
	booktitle = {2018 {Annual} {American} {Control} {Conference} ({ACC})},
	author = {Ashtiani, Faraz and Fayazi, S. Alireza and Vahidi, Ardalan},
	month = jun,
	year = {2018},
	note = {arXiv:2007.06639 [cs, eess, math]},
	keywords = {Mathematics - Optimization and Control, Electrical Engineering and Systems Science - Systems and Control, MILP},
	pages = {6341--6346},
	annote = {Comment: 6 pages, 6 figures, 2018 American Control Conference},
	file = {arXiv.org Snapshot:/home/jeroen/Zotero/storage/3QDH5MYN/2007.html:text/html;Full Text PDF:/home/jeroen/Zotero/storage/YGWRNS8U/Ashtiani et al. - 2018 - Multi-Intersection Traffic Management for Autonomo.pdf:application/pdf},
}

@inproceedings{shu_lin_model_2010,
	address = {Baltimore, MD},
	title = {Model {Predictive} {Control} for urban traffic networks via {MILP}},
	isbn = {978-1-4244-7427-1 978-1-4244-7426-4 978-1-4244-7425-7},
	url = {http://ieeexplore.ieee.org/document/5530534/},
	doi = {10.1109/ACC.2010.5530534},
	abstract = {Model Predictive Control (MPC) is an advanced control strategy that can easily coordinate urban trafﬁc networks. But, due to the nonlinearity of the trafﬁc model, the optimization problem of the MPC controller will become intractable in practice when the scale of the controlled trafﬁc network grows larger. To solve this problem, the nonlinear trafﬁc model is reformulated into a model with only linear equations and inequalities. Mixed-Integer Linear Programming (MILP) algorithms can efﬁciently solve the reformulated optimization problem, and guarantee the global optimum at the same time. Moreover, the MILP optimization problem is further relaxed by model reduction and adding upper bound constraints.},
	language = {en},
	urldate = {2023-09-14},
	booktitle = {Proceedings of the 2010 {American} {Control} {Conference}},
	publisher = {IEEE},
	author = {{Shu Lin} and De Schutter, B and {Yugeng Xi} and Hellendoorn, H},
	month = jun,
	year = {2010},
	keywords = {MILP},
	pages = {2272--2277},
	file = {Shu Lin et al. - 2010 - Model Predictive Control for urban traffic network.pdf:/home/jeroen/Zotero/storage/BTB9ZU5I/Shu Lin et al. - 2010 - Model Predictive Control for urban traffic network.pdf:application/pdf},
}

@article{hegemanand_intersection_nodate,
	title = {Intersection {Control} of the future},
	language = {en},
	author = {Hegemanand, Toon},
	keywords = {MILP},
	file = {Hegemanand - Intersection Control of the future.pdf:/home/jeroen/Zotero/storage/AU58ZIKR/Hegemanand - Intersection Control of the future.pdf:application/pdf},
}

@article{li_connected_2019,
	title = {Connected {Vehicles} {Based} {Traffic} {Signal} {Timing} {Optimization}},
	volume = {20},
	issn = {1524-9050, 1558-0016},
	url = {https://ieeexplore.ieee.org/document/8588385/},
	doi = {10.1109/TITS.2018.2883572},
	abstract = {We study the trafﬁc signal control problem with connected vehicles by assuming a ﬁxed cycle length so that the proposed model can be extended readily for the coordination of multiple signals. The problem can be ﬁrst formulated as a mixed-integer nonlinear program, by considering the information of individual vehicle’s trajectories (i.e., second-by-second vehicle locations and speeds) and their realistic driving/car-following behaviors. The objective function is to minimize the weighted sum of total fuel consumption and travel time. Due to the large dimension of the problem and the complexity of the nonlinear car-following model, solving the nonlinear program directly is challenging. We then reformulate the problem as a dynamic programming model by dividing the timing decisions into stages (one stage for a signal phase) and approximating the fuel consumption and travel time of a stage as functions of the state and decision variables of the stage. We also propose a twostep method to make sure that the obtained optimal solution can lead to the ﬁxed cycle length. Numerical experiments are provided to test the performance of the proposed model using data generated by trafﬁc simulation.},
	language = {en},
	number = {12},
	urldate = {2023-01-20},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Li, Wan and Ban, Xuegang},
	month = dec,
	year = {2019},
	keywords = {MILP},
	pages = {4354--4366},
	file = {Li and Ban - 2019 - Connected Vehicles Based Traffic Signal Timing Opt.pdf:/home/jeroen/Zotero/storage/CG5SBAC6/Li and Ban - 2019 - Connected Vehicles Based Traffic Signal Timing Opt.pdf:application/pdf},
}

@article{li_connected_2020,
	title = {Connected {Vehicle}-{Based} {Traffic} {Signal} {Coordination}},
	volume = {6},
	issn = {2095-8099},
	url = {https://www.sciencedirect.com/science/article/pii/S2095809920303015},
	doi = {10.1016/j.eng.2020.10.009},
	abstract = {This study presents a connected vehicles (CVs)-based traffic signal optimization framework for a coordinated arterial corridor. The signal optimization and coordination problem are first formulated in a centralized scheme as a mixed-integer nonlinear program (MINLP). The optimal phase durations and offsets are solved together by minimizing fuel consumption and travel time considering an individual vehicle’s trajectories. Due to the complexity of the model, we decompose the problem into two levels: an intersection level to optimize phase durations using dynamic programming (DP), and a corridor level to optimize the offsets of all intersections. In order to solve the two-level model, a prediction-based solution technique is developed. The proposed models are tested using traffic simulation under various scenarios. Compared with the traditional actuated signal timing and coordination plan, the signal timing plans generated by solving the MINLP and the two-level model can reasonably improve the signal control performance. When considering varies vehicle types under high demand levels, the proposed two-level model reduced the total system cost by 3.8\% comparing to baseline actuated plan. MINLP reduced the system cost by 5.9\%. It also suggested that coordination scheme was beneficial to corridors with relatively high demand levels. For intersections with major and minor street, coordination conducted for major street had little impacts on the vehicles at the minor street.},
	number = {12},
	urldate = {2023-09-25},
	journal = {Engineering},
	author = {Li, Wan and Ban, Xuegang},
	month = dec,
	year = {2020},
	keywords = {MILP, Connected vehicles, Dynamic programming, Mixed-integer nonlinear program, Traffic signal coordination, Two-level optimization},
	pages = {1463--1472},
	file = {ScienceDirect Full Text PDF:/home/jeroen/Zotero/storage/2EAQX3JD/Li and Ban - 2020 - Connected Vehicle-Based Traffic Signal Coordinatio.pdf:application/pdf},
}

@article{sommer_bidirectionally_2011,
	title = {Bidirectionally {Coupled} {Network} and {Road} {Traffic} {Simulation} for {Improved} {IVC} {Analysis}},
	volume = {10},
	issn = {1536-1233},
	shorttitle = {veins},
	url = {http://ieeexplore.ieee.org/document/5510240/},
	doi = {10.1109/TMC.2010.133},
	number = {1},
	urldate = {2023-01-20},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Sommer, C and German, R and Dressler, F},
	month = jan,
	year = {2011},
	pages = {3--15},
	file = {Submitted Version:/home/jeroen/Zotero/storage/M2QZETM7/Sommer et al. - 2011 - Bidirectionally Coupled Network and Road Traffic S.pdf:application/pdf},
}

@phdthesis{sarrafzadeh_offline_1990,
	title = {Offline {Policy}-search in {Bayesian} {Reinforcement} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/378886.380416},
	language = {en},
	urldate = {2022-12-08},
	author = {Sarrafzadeh, M.},
	month = jun,
	year = {1990},
	file = {Sarrafzadeh - 1990 - Department of electrical engineering and computer .pdf:/home/jeroen/Zotero/storage/ECQNPT29/Sarrafzadeh - 1990 - Department of electrical engineering and computer .pdf:application/pdf},
}

@article{boon_networks_2018,
	title = {Networks of fixed-cycle intersections},
	volume = {117},
	issn = {01912615},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0191261517303934},
	doi = {10.1016/j.trb.2018.08.019},
	language = {en},
	urldate = {2023-09-25},
	journal = {Transportation Research Part B: Methodological},
	author = {Boon, Marko A.A. and Van Leeuwaarden, Johan S.H.},
	month = nov,
	year = {2018},
	pages = {254--271},
	file = {Boon and Van Leeuwaarden - 2018 - Networks of fixed-cycle intersections.pdf:/home/jeroen/Zotero/storage/WMHHVYFC/Boon and Van Leeuwaarden - 2018 - Networks of fixed-cycle intersections.pdf:application/pdf},
}

@article{grooten_deep_2021,
	title = {Deep {Reinforcement} {Learning} for the cooperative card game {Hanabi}},
	language = {en},
	author = {Grooten, Bram},
	month = sep,
	year = {2021},
	file = {Grooten - Deep Reinforcement Learning for the cooperative ca.pdf:/home/jeroen/Zotero/storage/H8M7CX2Y/Grooten - Deep Reinforcement Learning for the cooperative ca.pdf:application/pdf},
}

@misc{tang_reinforcement_2020,
	title = {Reinforcement {Learning} for {Integer} {Programming}: {Learning} to {Cut}},
	shorttitle = {Reinforcement {Learning} for {Integer} {Programming}},
	url = {http://arxiv.org/abs/1906.04859},
	doi = {10.48550/arXiv.1906.04859},
	abstract = {Integer programming (IP) is a general optimization framework widely applicable to a variety of unstructured and structured problems arising in, e.g., scheduling, production planning, and graph optimization. As IP models many provably hard to solve problems, modern IP solvers rely on many heuristics. These heuristics are usually human-designed, and naturally prone to suboptimality. The goal of this work is to show that the performance of those solvers can be greatly enhanced using reinforcement learning (RL). In particular, we investigate a specific methodology for solving IPs, known as the Cutting Plane Method. This method is employed as a subroutine by all modern IP solvers. We present a deep RL formulation, network architecture, and algorithms for intelligent adaptive selection of cutting planes (aka cuts). Across a wide range of IP tasks, we show that the trained RL agent significantly outperforms human-designed heuristics, and effectively generalizes to 10X larger instances and across IP problem classes. The trained agent is also demonstrated to benefit the popular downstream application of cutting plane methods in Branch-and-Cut algorithm, which is the backbone of state-of-the-art commercial IP solvers.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Tang, Yunhao and Agrawal, Shipra and Faenza, Yuri},
	month = jul,
	year = {2020},
	note = {arXiv:1906.04859 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	annote = {Comment: Accepted at International Conference on Machine Learning (ICML) 2020},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/ZYWC4E8Z/Tang et al. - 2020 - Reinforcement Learning for Integer Programming Le.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/ZHQ44AZ7/1906.html:text/html},
}

@inproceedings{zhang_cityflow_2019,
	title = {{CityFlow}: {A} {Multi}-{Agent} {Reinforcement} {Learning} {Environment} for {Large} {Scale} {City} {Traffic} {Scenario}},
	shorttitle = {{CityFlow}},
	url = {http://arxiv.org/abs/1905.05217},
	doi = {10.1145/3308558.3314139},
	abstract = {Traffic signal control is an emerging application scenario for reinforcement learning. Besides being as an important problem that affects people's daily life in commuting, traffic signal control poses its unique challenges for reinforcement learning in terms of adapting to dynamic traffic environment and coordinating thousands of agents including vehicles and pedestrians. A key factor in the success of modern reinforcement learning relies on a good simulator to generate a large number of data samples for learning. The most commonly used open-source traffic simulator SUMO is, however, not scalable to large road network and large traffic flow, which hinders the study of reinforcement learning on traffic scenarios. This motivates us to create a new traffic simulator CityFlow with fundamentally optimized data structures and efficient algorithms. CityFlow can support flexible definitions for road network and traffic flow based on synthetic and real-world data. It also provides user-friendly interface for reinforcement learning. Most importantly, CityFlow is more than twenty times faster than SUMO and is capable of supporting city-wide traffic simulation with an interactive render for monitoring. Besides traffic signal control, CityFlow could serve as the base for other transportation studies and can create new possibilities to test machine learning methods in the intelligent transportation domain.},
	urldate = {2023-09-25},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	author = {Zhang, Huichu and Feng, Siyuan and Liu, Chang and Ding, Yaoyao and Zhu, Yichen and Zhou, Zihan and Zhang, Weinan and Yu, Yong and Jin, Haiming and Li, Zhenhui},
	month = may,
	year = {2019},
	note = {arXiv:1905.05217 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	pages = {3620--3624},
	annote = {Comment: WWW 2019 Demo Paper},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/L9EDGC59/Zhang et al. - 2019 - CityFlow A Multi-Agent Reinforcement Learning Env.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/VGJXZ99T/1905.html:text/html},
}

@inproceedings{wei_presslight_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {{PressLight}: {Learning} {Max} {Pressure} {Control} to {Coordinate} {Traffic} {Signals} in {Arterial} {Network}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {{PressLight}},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330949},
	doi = {10.1145/3292500.3330949},
	abstract = {Traffic signal control is essential for transportation efficiency in road networks. It has been a challenging problem because of the complexity in traffic dynamics. Conventional transportation research suffers from the incompetency to adapt to dynamic traffic situations. Recent studies propose to use reinforcement learning (RL) to search for more efficient traffic signal plans. However, most existing RL-based studies design the key elements - reward and state - in a heuristic way. This results in highly sensitive performances and a long learning process. To avoid the heuristic design of RL elements, we propose to connect RL with recent studies in transportation research. Our method is inspired by the state-of-the-art method max pressure (MP) in the transportation field. The reward design of our method is well supported by the theory in MP, which can be proved to be maximizing the throughput of the traffic network, i.e., minimizing the overall network travel time. We also show that our concise state representation can fully support the optimization of the proposed reward function. Through comprehensive experiments, we demonstrate that our method outperforms both conventional transportation approaches and existing learning-based methods.},
	urldate = {2023-09-25},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Wei, Hua and Chen, Chacha and Zheng, Guanjie and Wu, Kan and Gayah, Vikash and Xu, Kai and Li, Zhenhui},
	month = jul,
	year = {2019},
	keywords = {deep reinforcement learning, multi-agent system, traffic signal control},
	pages = {1290--1298},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/RV86D9SB/Wei et al. - 2019 - PressLight Learning Max Pressure Control to Coord.pdf:application/pdf},
}

@misc{bengio_machine_2020,
	title = {Machine {Learning} for {Combinatorial} {Optimization}: a {Methodological} {Tour} d'{Horizon}},
	shorttitle = {Machine {Learning} for {Combinatorial} {Optimization}},
	url = {http://arxiv.org/abs/1811.06128},
	doi = {10.48550/arXiv.1811.06128},
	abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
	month = mar,
	year = {2020},
	note = {arXiv:1811.06128 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/FB883LT7/Bengio et al. - 2020 - Machine Learning for Combinatorial Optimization a.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/2IHTDUK3/1811.html:text/html},
}

@article{mariani_coordination_2022,
	title = {Coordination of {Autonomous} {Vehicles}: {Taxonomy} and {Survey}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Coordination of {Autonomous} {Vehicles}},
	url = {https://dl.acm.org/doi/10.1145/3431231},
	doi = {10.1145/3431231},
	abstract = {In the near future, our streets will be populated by myriads of autonomous self-driving vehicles to serve our diverse mobility needs. This will raise the need to coordinate their movements in order to properly handle both access to shared resources (e.g., intersections and parking slots) and the execution of mobility tasks (e.g., platooning and ramp merging). The aim of this article is to provide a global view of the coordination issues and the related solutions in the field of autonomous vehicles. To this end, we firstly introduce the general problems associated with coordination of autonomous vehicles by identifying and framing the key classes of coordination problems. Then, we overview the different approaches that can be adopted to deal with such problems by classifying them in terms of the degree of autonomy in decision making that is left to autonomous vehicles during the coordination process. Finally, we overview some further research challenges to address before autonomous coordinated vehicles can safely hit our streets.},
	language = {en},
	number = {1},
	urldate = {2023-09-24},
	journal = {ACM Computing Surveys},
	author = {Mariani, Stefano and Cabri, Giacomo and Zambonelli, Franco},
	month = jan,
	year = {2022},
	pages = {1--33},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/TLS9NWCD/Mariani et al. - 2022 - Coordination of Autonomous Vehicles Taxonomy and .pdf:application/pdf},
}

@article{dai_queueing_2022,
	title = {Queueing {Network} {Controls} via {Deep} {Reinforcement} {Learning}},
	volume = {12},
	issn = {1946-5238},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/stsy.2021.0081},
	doi = {10.1287/stsy.2021.0081},
	abstract = {Novel advanced policy gradient (APG) methods, such as trust region policy optimization and proximal policy optimization (PPO), have become the dominant reinforcement learning algorithms because of their ease of implementation and good practical performance. A conventional setup for notoriously difficult queueing network control problems is a Markov decision problem (MDP) that has three features: infinite state space, unbounded costs, and long-run average cost objective. We extend the theoretical framework of these APG methods for such MDP problems. The resulting PPO algorithm is tested on a parallel-server system and large-size multiclass queueing networks. The algorithm consistently generates control policies that outperform state-of-art heuristics in literature in a variety of load conditions from light to heavy traffic. These policies are demonstrated to be near optimal when the optimal policy can be computed. A key to the successes of our PPO algorithm is the use of three variance reduction techniques in estimating the relative value function via sampling. First, we use a discounted relative value function as an approximation of the relative value function. Second, we propose regenerative simulation to estimate the discounted relative value function. Finally, we incorporate the approximating martingale-process method into the regenerative estimator.},
	number = {1},
	urldate = {2023-09-24},
	journal = {Stochastic Systems},
	author = {Dai, J. G. and Gluzman, Mark},
	month = mar,
	year = {2022},
	note = {Publisher: INFORMS},
	keywords = {reinforcement learning, control variate, long-run average cost, multiclass queueing network},
	pages = {30--67},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/IR7QHQGI/Dai and Gluzman - 2022 - Queueing Network Controls via Deep Reinforcement L.pdf:application/pdf},
}

@article{treca_green_nodate,
	title = {Green {Wave} {CoordinRaetiionnforFcoermTernat} {LceaSrignninagl} {Control} {Using} {Deep}},
	language = {en},
	author = {Tréca, Maxime and Zargayouna, Mahdi and Barth, Dominique and Garbiso, Julian},
	file = {Tréca et al. - Green Wave CoordinRaetiionnforFcoermTernatLceaSri.pdf:/home/jeroen/Zotero/storage/PDJIJI8C/Tréca et al. - Green Wave CoordinRaetiionnforFcoermTernatLceaSri.pdf:application/pdf},
}

@inproceedings{wei_intellilight_2018,
	address = {London United Kingdom},
	title = {{IntelliLight}: {A} {Reinforcement} {Learning} {Approach} for {Intelligent} {Traffic} {Light} {Control}},
	isbn = {978-1-4503-5552-0},
	shorttitle = {{IntelliLight}},
	url = {https://dl.acm.org/doi/10.1145/3219819.3220096},
	doi = {10.1145/3219819.3220096},
	language = {en},
	urldate = {2023-09-23},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Wei, Hua and Zheng, Guanjie and Yao, Huaxiu and Li, Zhenhui},
	month = jul,
	year = {2018},
	pages = {2496--2505},
	file = {Full Text PDF:/home/jeroen/Zotero/storage/NYXVAYX8/Wei et al. - 2018 - IntelliLight A Reinforcement Learning Approach fo.pdf:application/pdf},
}

@inproceedings{wei_colight_2019,
	title = {{CoLight}: {Learning} {Network}-level {Cooperation} for {Traffic} {Signal} {Control}},
	shorttitle = {{CoLight}},
	url = {http://arxiv.org/abs/1905.05717},
	doi = {10.1145/3357384.3357902},
	abstract = {Cooperation among the traffic signals enables vehicles to move through intersections more quickly. Conventional transportation approaches implement cooperation by pre-calculating the offsets between two intersections. Such pre-calculated offsets are not suitable for dynamic traffic environments. To enable cooperation of traffic signals, in this paper, we propose a model, CoLight, which uses graph attentional networks to facilitate communication. Specifically, for a target intersection in a network, CoLight can not only incorporate the temporal and spatial influences of neighboring intersections to the target intersection, but also build up index-free modeling of neighboring intersections. To the best of our knowledge, we are the first to use graph attentional networks in the setting of reinforcement learning for traffic signal control and to conduct experiments on the large-scale road network with hundreds of traffic signals. In experiments, we demonstrate that by learning the communication, the proposed model can achieve superior performance against the state-of-the-art methods.},
	urldate = {2023-09-23},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	author = {Wei, Hua and Xu, Nan and Zhang, Huichu and Zheng, Guanjie and Zang, Xinshi and Chen, Chacha and Zhang, Weinan and Zhu, Yanmin and Xu, Kai and Li, Zhenhui},
	month = nov,
	year = {2019},
	note = {arXiv:1905.05717 [cs]},
	keywords = {Computer Science - Machine Learning, 68Txx, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems},
	pages = {1913--1922},
	annote = {Comment: 10 pages. Proceedings of the 28th ACM International on Conference on Information and Knowledge Management. ACM, 2018},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/72C9U98C/Wei et al. - 2019 - CoLight Learning Network-level Cooperation for Tr.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/Z6M9HYCP/1905.html:text/html},
}

@article{wei_deep_2019,
	title = {Deep {Reinforcement} {Learning} for {Traffic} {Signal} {Control} along {Arterials}},
	abstract = {Arterial streets serve as the principal undertaker for urban mobility in a typical urban road network. In this paper, we propose a novel decentralized reinforcement learning method for multiintersection traffic signal control on arterial traffic, by applying reinforcement learning control agents in each intersection. While applying individual control to multi-intersection problems faces many challenges, two main adjustments are made to optimize the overall performance: 1) to provide simple yet novel contextual information to individual agents and 2) to train the RL agents in a transfer learning way. We test our method on synthetic data dataset and show that our proposed method outperforms the state-of-theart methods. We also interpret the policies learned by our method, which is the first time that the policy learned by the reinforcement learning control agents is interpreted using the traditional transportation coordination method on the arterial.},
	language = {en},
	author = {Wei, Hua and Chen, Chacha and Wu, Kan and Zheng, Guanjie and Yu, Zhengyao and Gayah, Vikash and Li, Zhenhui},
	year = {2019},
	file = {Wei et al. - 2019 - Deep Reinforcement Learning for Traffic Signal Con.pdf:/home/jeroen/Zotero/storage/46P599NU/Wei et al. - 2019 - Deep Reinforcement Learning for Traffic Signal Con.pdf:application/pdf},
}

@misc{nichol_point-e_2022,
	title = {Point-{E}: {A} {System} for {Generating} {3D} {Point} {Clouds} from {Complex} {Prompts}},
	shorttitle = {Point-{E}},
	url = {http://arxiv.org/abs/2212.08751},
	abstract = {While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.},
	urldate = {2023-02-18},
	publisher = {arXiv},
	author = {Nichol, Alex and Jun, Heewoo and Dhariwal, Prafulla and Mishkin, Pamela and Chen, Mark},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08751 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 8 pages, 11 figures},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/EGY8QG6C/Nichol et al. - 2022 - Point-E A System for Generating 3D Point Clouds f.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/ANINQA7V/2212.html:text/html},
}

@book{pearl_book_2018,
	address = {USA},
	edition = {1st},
	title = {The {Book} of {Why}: {The} {New} {Science} of {Cause} and {Effect}},
	isbn = {978-0-465-09760-9},
	shorttitle = {The {Book} of {Why}},
	abstract = {A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence"Correlation is not causation." This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality--the study of cause and effect--on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.},
	publisher = {Basic Books, Inc.},
	author = {Pearl, Judea and Mackenzie, Dana},
	year = {2018},
}

@book{jaynes2003probability,
	title = {Probability theory: {The} logic of science},
	publisher = {Cambridge university press},
	author = {Jaynes, Edwin T},
	year = {2003},
}

@misc{caticha_lectures_2008,
	title = {Lectures on {Probability}, {Entropy}, and {Statistical} {Physics}},
	url = {http://arxiv.org/abs/0808.0012},
	abstract = {These lectures deal with the problem of inductive inference, that is, the problem of reasoning under conditions of incomplete information. Is there a general method for handling uncertainty? Or, at least, are there rules that could in principle be followed by an ideally rational mind when discussing scientific matters? What makes one statement more plausible than another? How much more plausible? And then, when new information is acquired how do we change our minds? Or, to put it differently, are there rules for learning? Are there rules for processing information that are objective and consistent? Are they unique? And, come to think of it, what, after all, is information? It is clear that data contains or conveys information, but what does this precisely mean? Can information be conveyed in other ways? Is information physical? Can we measure amounts of information? Do we need to? Our goal is to develop the main tools for inductive inference--probability and entropy--from a thoroughly Bayesian point of view and to illustrate their use in physics with examples borrowed from the foundations of classical statistical physics.},
	urldate = {2023-02-18},
	publisher = {arXiv},
	author = {Caticha, Ariel},
	month = jul,
	year = {2008},
	note = {arXiv:0808.0012 [cond-mat, physics:physics, stat]},
	keywords = {Computer Science - Information Theory, Condensed Matter - Statistical Mechanics, Mathematics - Statistics Theory, Physics - Data Analysis, Statistics and Probability, Physics - General Physics},
	annote = {Comment: 170 pages. Invited lectures at MaxEnt 2008, the 28th International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering (July 8-13, 2008, Boraceia Beach, Sao Paulo, Brazil)},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/YN6PZ7FS/Caticha - 2008 - Lectures on Probability, Entropy, and Statistical .pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/DRQN8XWU/0808.html:text/html},
}

@article{kelly_citygen_nodate,
	title = {Citygen: {An} {Interactive} {System} for {Procedural} {City} {Generation}},
	abstract = {Contemporary 3D games are often situated within large urban environments. This necessitates a time-consuming and expensive content creation process involving the modelling of vast amounts of geometric detail: including terrain, roads, buildings, and other associated features. We present a system called Citygen that aims to automate as much of this as possible by employing procedural generation methods to rapidly create the urban geometry typical of a modern city. Procedural methods have long been used within the graphics and game development communities to generate natural phenomena such as plants and trees. We employ these methods to generate the underlying road networks that form the structure of cities and urban neighbourhoods. These road networks are automatically mapped to any supplied terrain model, and adapt themselves to the specific geometry of the underlying terrain. Building footprints are automatically extracted from the resulting model and buildings can then be inserted either procedurally or by hand. Our system is unique in that it is designed to allow developers hands-on interactive control over the generation process. We achieve this by providing an interface allowing the user to directly manipulate geometric elements such as road intersection nodes, and to directly control and specify many aspects of the procedural generation. The results are updated in real time, thus facilitating an interactive design process.},
	language = {en},
	author = {Kelly, George and McCabe, Hugh},
	file = {Kelly and McCabe - Citygen An Interactive System for Procedural City.pdf:/home/jeroen/Zotero/storage/ME84A9YC/Kelly and McCabe - Citygen An Interactive System for Procedural City.pdf:application/pdf},
}

@article{kelly_survey_2006,
	title = {A {Survey} of {Procedural} {Techniques} for {City} {Generation}},
	copyright = {Creative Commons Attribution-Noncommercial-Share Alike 3.0 License},
	url = {https://arrow.dit.ie/itbj/vol7/iss2/5/},
	doi = {10.21427/D76M9P},
	abstract = {The computer game industry requires a skilled workforce and this combined with the complexity of modern games, means that production costs are extremely high. One of the most time consuming aspects is the creation of game geometry, the virtual world which the players inhabit. Procedural techniques have been used within computer graphics to create natural textures, simulate special effects and generate complex natural models including trees and waterfalls. It is these procedural techniques that we intend to harness to generate geometry and textures suitable for a game situated in an urban environment. Procedural techniques can provide many benefits for computer graphics applications when the correct algorithm is used. An overview of several commonly used procedural techniques including fractals, L-systems, Perlin noise, tiling systems and cellular basis is provided. The function of each technique and the resulting output they create are discussed to better understand their characteristics, benefits and relevance to the city generation problem. City generation is the creation of an urban area which necessitates the creation of buildings, situated along streets and arranged in appropriate patterns. Some research has already taken place into recreating road network patterns and generating buildings that can vary in function and architectural style. We will study the main body of existing research into procedural city generation and provide an overview of their implementations and a critique of their functionality and results. Finally we present areas in which further research into the generation of cities is required and outline our research goals for city generation.},
	language = {en},
	urldate = {2023-02-11},
	author = {Kelly, George},
	year = {2006},
	note = {Publisher: Dublin Institute of Technology},
	file = {Kelly - 2006 - A Survey of Procedural Techniques for City Generat.pdf:/home/jeroen/Zotero/storage/PIKLABAL/Kelly - 2006 - A Survey of Procedural Techniques for City Generat.pdf:application/pdf},
}

@article{kelly_survey_2006-1,
	title = {A {Survey} of {Procedural} {Techniques} for {City} {Generation}},
	copyright = {Creative Commons Attribution-Noncommercial-Share Alike 3.0 License},
	url = {https://arrow.dit.ie/itbj/vol7/iss2/5/},
	doi = {10.21427/D76M9P},
	abstract = {The computer game industry requires a skilled workforce and this combined with the complexity of modern games, means that production costs are extremely high. One of the most time consuming aspects is the creation of game geometry, the virtual world which the players inhabit. Procedural techniques have been used within computer graphics to create natural textures, simulate special effects and generate complex natural models including trees and waterfalls. It is these procedural techniques that we intend to harness to generate geometry and textures suitable for a game situated in an urban environment. Procedural techniques can provide many benefits for computer graphics applications when the correct algorithm is used. An overview of several commonly used procedural techniques including fractals, L-systems, Perlin noise, tiling systems and cellular basis is provided. The function of each technique and the resulting output they create are discussed to better understand their characteristics, benefits and relevance to the city generation problem. City generation is the creation of an urban area which necessitates the creation of buildings, situated along streets and arranged in appropriate patterns. Some research has already taken place into recreating road network patterns and generating buildings that can vary in function and architectural style. We will study the main body of existing research into procedural city generation and provide an overview of their implementations and a critique of their functionality and results. Finally we present areas in which further research into the generation of cities is required and outline our research goals for city generation.},
	language = {en},
	urldate = {2023-02-11},
	author = {Kelly, George},
	year = {2006},
	note = {Publisher: Dublin Institute of Technology},
	file = {Kelly - 2006 - A Survey of Procedural Techniques for City Generat.pdf:/home/jeroen/Zotero/storage/S3ILDLCP/Kelly - 2006 - A Survey of Procedural Techniques for City Generat.pdf:application/pdf},
}

@article{kelly_survey_2006-2,
	title = {A {Survey} of {Procedural} {Techniques} for {City} {Generation}},
	volume = {14},
	doi = {10.21427/D76M9P},
	abstract = {The computer game industry requires a skilled workforce and this combined with the complexity of modern games, means that production costs are extremely high. One of the most time consuming aspects is the creation of game geometry, the virtual world which the players inhabit. Procedural techniques have been used within computer graphics to create natural textures, simulate special effects and generate complex natural models including trees and waterfalls. It is these procedural techniques that we intend to harness to generate geometry and textures suitable for a game situated in an urban environment. Procedural techniques can provide many benefits for computer graphics applications when the correct algorithm is used. An overview of several commonly used procedural techniques including fractals, L-systems, Perlin noise, tiling systems and cellular basis is provided. The function of each technique and the resulting output they create are discussed to better understand their characteristics, benefits and relevance to the city generation problem. City generation is the creation of an urban area which necessitates the creation of buildings, situated along streets and arranged in appropriate patterns. Some research has already taken place into recreating road network patterns and generating buildings that can vary in function and architectural style. We will study the main body of existing research into procedural city generation and provide an overview of their implementations and a critique of their functionality and results. Finally we present areas in which further research into the generation of cities is required and outline our research goals for city generation.},
	author = {Kelly, George and McCabe, Hugh},
	month = jan,
	year = {2006},
}

@article{kim_procedural_2018,
	title = {Procedural city generation beyond game development},
	volume = {10},
	url = {https://doi.org/10.1145/3292390.3292397},
	doi = {10.1145/3292390.3292397},
	abstract = {The common trend in the scientific inquiry of urban areas and their populations is to use real-world geographic and population data to understand, explain, and predict urban phenomena. We argue that this trend limits our understanding of urban areas as dealing with arbitrarily collected geographic data requires technical expertise to process; moreover, population data is often aggregated, sparsified, or anonymized for privacy reasons. We believe synthetic urban areas generated via procedural city generation, which is a technique mostly used in the gaming area, could help improve the state-of-the-art in many disciplines which study urban areas. In this paper, we describe a selection of research areas that could benefit from such synthetic urban data and show that the current research in procedurally generated cities needs to address specific issues (e.g., plausibility) to sufficiently capture real-world cities and thus take such data beyond gaming.},
	number = {2},
	urldate = {2023-02-11},
	journal = {SIGSPATIAL Special},
	author = {Kim, Joon-Seok and Kavak, Hamdi and Crooks, Andrew},
	month = nov,
	year = {2018},
	pages = {34--41},
	file = {Kim et al. - 2018 - Procedural city generation beyond game development.pdf:/home/jeroen/Zotero/storage/ZGQAJ62E/Kim et al. - 2018 - Procedural city generation beyond game development.pdf:application/pdf},
}

@article{kelly_survey_2006-3,
	title = {A {Survey} of {Procedural} {Techniques} for {City} {Generation}},
	volume = {14},
	doi = {10.21427/D76M9P},
	abstract = {The computer game industry requires a skilled workforce and this combined with the complexity of modern games, means that production costs are extremely high. One of the most time consuming aspects is the creation of game geometry, the virtual world which the players inhabit. Procedural techniques have been used within computer graphics to create natural textures, simulate special effects and generate complex natural models including trees and waterfalls. It is these procedural techniques that we intend to harness to generate geometry and textures suitable for a game situated in an urban environment. Procedural techniques can provide many benefits for computer graphics applications when the correct algorithm is used. An overview of several commonly used procedural techniques including fractals, L-systems, Perlin noise, tiling systems and cellular basis is provided. The function of each technique and the resulting output they create are discussed to better understand their characteristics, benefits and relevance to the city generation problem. City generation is the creation of an urban area which necessitates the creation of buildings, situated along streets and arranged in appropriate patterns. Some research has already taken place into recreating road network patterns and generating buildings that can vary in function and architectural style. We will study the main body of existing research into procedural city generation and provide an overview of their implementations and a critique of their functionality and results. Finally we present areas in which further research into the generation of cities is required and outline our research goals for city generation.},
	author = {Kelly, George and McCabe, Hugh},
	month = jan,
	year = {2006},
}

@article{laemmer2008a,
	title = {Self-control of traffic lights and vehicle flows in urban road networks},
	volume = {2008},
	language = {en},
	number = {04},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Lämmer, S. and Helbing, D.},
	year = {2008},
}

@misc{lillicrap_continuous_2019,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	doi = {10.48550/arXiv.1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = jul,
	year = {2019},
	note = {arXiv:1509.02971 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 10 pages + supplementary},
	file = {arXiv Fulltext PDF:/home/jeroen/Zotero/storage/AT28MPL2/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf:application/pdf;arXiv.org Snapshot:/home/jeroen/Zotero/storage/FNSFETWC/1509.html:text/html},
}

@misc{krauss1998a,
	title = {Microscopic modeling of traffic flow: investigation of collision free vehicle dynamics},
	language = {de},
	publisher = {DLR Deutsches Zentrum fur Luft– und Raumfahrt},
	author = {Krauss, S.},
	year = {1998},
	note = {Pages: 98–08
Type: Tech. rept.},
}

@article{richards1956a,
	title = {Shock waves on the highway},
	volume = {4},
	language = {en},
	number = {1},
	journal = {Operations Research},
	author = {Richards, P.I.},
	year = {1956},
	pages = {42--51},
}

@article{lighthill1955a,
	title = {On kinematic waves {II}. {A} theory of traffic flow on long crowded roads},
	volume = {229},
	language = {en},
	number = {1178},
	journal = {Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences},
	author = {Lighthill, M.J. and Whitham, G.B.},
	year = {1955},
	pages = {317--345},
}

@article{greenshields1935a,
	title = {A study of traffic capacity},
	language = {en},
	journal = {Proceedings of the 14th annual meeting of the highway research board},
	author = {Greenshields, B.D. and Bibbins, J.R. and Channing, W.S. and Miller, H.H.},
	year = {1935},
	pages = {448--477},
}

@article{greenshields1934a,
	title = {The photographic method of studying traffic behavior},
	language = {en},
	journal = {Proceedings of the 13th annual meeting of the highway research board},
	author = {Greenshields, B.D.},
	year = {1934},
	pages = {382--399},
}

@article{wageningen-kessels2015a,
	title = {Genealogy of traffic flow models},
	volume = {4},
	language = {en},
	number = {4},
	journal = {EURO Journal on Transportation and Logistics},
	author = {Wageningen-Kessels, F. and Lint, H. and Vuik, K. and Hoogendoorn, S.},
	year = {2015},
	pages = {445--473},
}

@article{daganzo_cell_1995,
	title = {The cell transmission model, part {II}: {Network} traffic},
	volume = {29},
	issn = {0191-2615},
	shorttitle = {The cell transmission model, part {II}},
	url = {https://www.sciencedirect.com/science/article/pii/019126159400022R},
	doi = {10.1016/0191-2615(94)00022-R},
	abstract = {This article shows how the evolution of multi-commodity traffic flows over complex networks can be predicted over time, based on a simple macroscopic computer representation of traffic flow that is consistent with the kinematic wave theory under all traffic conditions. The method does not use ad hoc procedures to treat special situations. After a brief review of the basic model for one link, the article describes how three-legged junctions can be modeled. It then introduces a numerical procedure for networks, assuming that a time-varying origin-destination (O-D) table is given and that the proportion of turns at every junction is known. These assumptions are reasonable for numerical analysis of disaster evacuation plans. The results are then extended to the case where, instead of the turning proportions, the best routes to each destination from every junction are known at all times. For technical reasons explained in the text, the procedure is more complicated in this case, requiring more computer memory and more time for execution. The effort is estimated to be about an order of magnitude greater than for the static traffic assignment problem on a network of the same size. The procedure is ideally suited for parallel computing. It is hoped that the results in the article will lead to more realistic models of freeway flow, disaster evacuations and dynamic traffic assignment for the evening commute.},
	language = {en},
	number = {2},
	urldate = {2023-01-20},
	journal = {Transportation Research Part B: Methodological},
	author = {Daganzo, Carlos F.},
	month = apr,
	year = {1995},
	pages = {79--93},
	file = {ScienceDirect Full Text PDF:/home/jeroen/Zotero/storage/BXTINP8V/Daganzo - 1995 - The cell transmission model, part II Network traf.pdf:application/pdf;ScienceDirect Snapshot:/home/jeroen/Zotero/storage/2IN935KE/019126159400022R.html:text/html},
}

@article{daganzo_cell_1994,
	title = {The cell transmission model: {A} dynamic representation of highway traffic consistent with the hydrodynamic theory},
	volume = {28},
	issn = {0191-2615},
	shorttitle = {The cell transmission model},
	url = {https://www.sciencedirect.com/science/article/pii/0191261594900027},
	doi = {10.1016/0191-2615(94)90002-7},
	abstract = {This paper presents a simple representation of traffic on a highway with a single entrance and exit. The representation can be used to predict traffic's evolution over time and space, including transient phenomena such as the building, propagation, and dissipation of queues. The easy-to-solve difference equations used to predict traffic's evolution are shown to be the discrete analog of the differential equations arising from a special case of the hydrodynamic model of traffic flow. The proposed method automatically generates appropriate changes in density at locations where the hydrodynamic theory would call for a shockwave; i.e., a jump in density such as those typically seen at the end of every queue. The complex side calculations required by classical methods to keep track of shockwaves are thus eliminated. The paper also shows how the equations can mimic the real-life development of stop-and-go traffic within moving queues.},
	language = {en},
	number = {4},
	urldate = {2023-01-20},
	journal = {Transportation Research Part B: Methodological},
	author = {Daganzo, Carlos F.},
	month = aug,
	year = {1994},
	pages = {269--287},
	file = {ScienceDirect Full Text PDF:/home/jeroen/Zotero/storage/9QHLKJCR/Daganzo - 1994 - The cell transmission model A dynamic representat.pdf:application/pdf;ScienceDirect Snapshot:/home/jeroen/Zotero/storage/7QWXPRCJ/0191261594900027.html:text/html},
}

@article{xu_leveraging_2022,
	title = {Leveraging {Transformer} {Model} to {Predict} {Vehicle} {Trajectories} in {Congested} {Urban} {Traffic}},
	issn = {0361-1981, 2169-4052},
	url = {http://journals.sagepub.com/doi/10.1177/03611981221109594},
	doi = {10.1177/03611981221109594},
	abstract = {Accurate vehicle trajectory prediction enables safe, comfortable, and optimal proactive motion planning for connected and autonomous vehicles (CAVs). Because of rapid advances in learning techniques and increasing access to massive amounts of data, deep learning techniques have been applied to predict vehicle trajectories, especially the long short-term memory (LSTM) model. However, the accurate prediction of vehicle trajectories for congested urban traffic remains problematic, as existing LSTM models do not perform well. To address this gap, this paper proposes to leverage an emerging deep learning technique—transformer—and utilizes a recently released dataset (pNEUMA) for predicting vehicle trajectories in congested urban traffic. The proposed transformer model uses the self-attention mechanism, which helps to identify dependencies within the model inputs, to systematically determine the impacts of vehicular interactions on the target vehicle’s future trajectory. The pNEUMA dataset, which provides drone-based large-scale data of congested urban traffic, is processed to fit a typical trajectory prediction scenario, and used to train the transformer model. Numerical studies are conducted to analyze the effectiveness of the proposed modeling approach. A comparison of the proposed model with representative LSTM models highlights the advantages of leveraging the transformer model characteristics for the vehicle trajectory prediction of congested urban traffic. By contrast, existing LSTM models may suffice for the trajectory prediction of freeway traffic. The results also indicate that, unlike for vehicle trajectory prediction for freeway traffic, a longer time window of inputs does not guarantee better prediction performance for congested urban traffic.},
	language = {en},
	urldate = {2023-01-20},
	journal = {Transportation Research Record: Journal of the Transportation Research Board},
	author = {Xu, Yufei and Wang, Yu and Peeta, Srinivas},
	month = aug,
	year = {2022},
	pages = {036119812211095},
	file = {Xu et al. - 2022 - Leveraging Transformer Model to Predict Vehicle Tr.pdf:/home/jeroen/Zotero/storage/73KTKYFW/Xu et al. - 2022 - Leveraging Transformer Model to Predict Vehicle Tr.pdf:application/pdf},
}

@misc{chen_s2tnet_2022,
	title = {{S2TNet}: {Spatio}-{Temporal} {Transformer} {Networks} for {Trajectory} {Prediction} in {Autonomous} {Driving}},
	shorttitle = {{S2TNet}},
	url = {http://arxiv.org/abs/2206.10902},
	abstract = {To safely and rationally participate in dense and heterogeneous traﬃc, autonomous vehicles require to suﬃciently analyze the motion patterns of surrounding traﬃc-agents and accurately predict their future trajectories. This is challenging because the trajectories of traﬃc-agents are not only inﬂuenced by the traﬃc-agents themselves but also by spatial interaction with each other. Previous methods usually rely on the sequential stepby-step processing of Long Short-Term Memory networks (LSTMs) and merely extract the interactions between spatial neighbors for single type traﬃc-agents. We propose the Spatio-Temporal Transformer Networks (S2TNet), which models the spatio-temporal interactions by spatio-temporal Transformer and deals with the temporel sequences by temporal Transformer. We input additional category, shape and heading information into our networks to handle the heterogeneity of traﬃc-agents. The proposed methods outperforms state-of-the-art methods on ApolloScape Trajectory dataset by more than 7\% on both the weighted sum of Average and Final Displacement Error. Our code is available at https://github.com/chenghuang66/s2tnet.},
	language = {en},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Chen, Weihuang and Wang, Fangfang and Sun, Hongbin},
	month = jun,
	year = {2022},
	note = {arXiv:2206.10902 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: Accepted by ACML2021},
	file = {Chen et al. - 2022 - S2TNet Spatio-Temporal Transformer Networks for T.pdf:/home/jeroen/Zotero/storage/TPL7UFL9/Chen et al. - 2022 - S2TNet Spatio-Temporal Transformer Networks for T.pdf:application/pdf},
}

@misc{quintanar_predicting_2021,
	title = {Predicting {Vehicles} {Trajectories} in {Urban} {Scenarios} with {Transformer} {Networks} and {Augmented} {Information}},
	url = {http://arxiv.org/abs/2106.00559},
	abstract = {Understanding the behavior of road users is of vital importance for the development of trajectory prediction systems. In this context, the latest advances have focused on recurrent structures, establishing the social interaction between the agents involved in the scene. More recently, simpler structures have also been introduced for predicting pedestrian trajectories, based on Transformer Networks, and using positional information [1]. They allow the individual modelling of each agent’s trajectory separately without any complex interaction terms. Our model exploits these simple structures by adding augmented data (position and heading), and adapting their use to the problem of vehicle trajectory prediction in urban scenarios in prediction horizons up to 5 seconds. In addition, a cross-performance analysis is performed between different types of scenarios, including highways, intersections and roundabouts, using recent datasets (inD, rounD, highD and INTERACTION). Our model achieves state-of-the-art results and proves to be ﬂexible and adaptable to different types of urban contexts.},
	language = {en},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Quintanar, A. and Fernández-Llorca, D. and Parra, I. and Izquierdo, R. and Sotelo, M. A.},
	month = jun,
	year = {2021},
	note = {arXiv:2106.00559 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: This work has been accepted for publication at IEEE Intelligent Vehicles Symposium 2021},
	file = {Quintanar et al. - 2021 - Predicting Vehicles Trajectories in Urban Scenario.pdf:/home/jeroen/Zotero/storage/Q92ZVIAZ/Quintanar et al. - 2021 - Predicting Vehicles Trajectories in Urban Scenario.pdf:application/pdf},
}

@article{taylor_transfer_nodate,
	title = {Transfer {Learning} for {Reinforcement} {Learning} {Domains}: {A} {Survey}},
	abstract = {The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While signiﬁcant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classiﬁes transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.},
	language = {en},
	author = {Taylor, Matthew E and Stone, Peter},
	file = {Taylor and Stone - Transfer Learning for Reinforcement Learning Domai.pdf:/home/jeroen/Zotero/storage/2P69864C/Taylor and Stone - Transfer Learning for Reinforcement Learning Domai.pdf:application/pdf},
}

@inproceedings{fayazi_optimal_2017,
	address = {Seattle, WA, USA},
	title = {Optimal scheduling of autonomous vehicle arrivals at intelligent intersections via {MILP}},
	isbn = {978-1-5090-5992-8},
	url = {https://ieeexplore.ieee.org/document/7963717/},
	doi = {10.23919/ACC.2017.7963717},
	abstract = {We propose optimal scheduling of autonomous vehicle arrivals at intersections, eliminating the need for physical trafﬁc signals. The proposed intersection control algorithm is assumed to have bi-directional communication links to approaching vehicles. After receiving subscription requests and status of approaching vehicles, the intersection control node calculates an arrival schedule that ensures safety while signiﬁcantly reducing number of stops and intersection delays. The vehicleintersection coordination problem is formulated as a MixedInteger Linear Program (MILP). A case study is presented and a customized trafﬁc microsimulation environment is developed to demonstrate the effectiveness of the proposed intersection control scheme.},
	language = {en},
	urldate = {2024-01-26},
	booktitle = {2017 {American} {Control} {Conference} ({ACC})},
	publisher = {IEEE},
	author = {Fayazi, S. Alireza and Vahidi, Ardalan and Luckow, Andre},
	month = may,
	year = {2017},
	pages = {4920--4925},
	file = {Fayazi et al. - 2017 - Optimal scheduling of autonomous vehicle arrivals .pdf:/home/jeroen/Zotero/storage/NEQUSM5Q/Fayazi et al. - 2017 - Optimal scheduling of autonomous vehicle arrivals .pdf:application/pdf},
}

@article{li_temporal-spatial_2019,
	title = {Temporal-spatial dimension extension-based intersection control formulation for connected and autonomous vehicle systems},
	volume = {104},
	issn = {0968-090X},
	url = {https://www.sciencedirect.com/science/article/pii/S0968090X18305436},
	doi = {10.1016/j.trc.2019.05.003},
	abstract = {Traffic congestion has become a serious issue worldwide due to the rapid increase in population and traffic demands. Advances in connected automated vehicle (CAV) technology demonstrate the potential to improve traffic mobility and safety performance at intersections. An advanced intersection control system is proposed in this study to coordinate vehicle trajectories and ensure safety and operation efficiency at intersections. A temporal-spatial dimension extension-based trajectory coordination model is developed by formulating all possible trajectories of vehicles at the intersection. Correspondingly, according to the trajectory coordination model, two signal-free control algorithms, including the priority-based algorithm and the Discrete Forward-Rolling Optimal Control (DFROC) algorithm are proposed in this study to manage vehicles at the intersection. These two algorithms, together with the FCFS policy, are compared with the conventional signal control method in a SUMO-based simulation platform. Experimental results indicate that the proposed algorithms outperform the signal control method in terms of reducing total traffic delays at intersections and increasing intersection capacity and operation efficiency.},
	urldate = {2024-01-26},
	journal = {Transportation Research Part C: Emerging Technologies},
	author = {Li, Zhenning and Wu, Qiong and Yu, Hao and Chen, Cong and Zhang, Guohui and Tian, Zong Z. and Prevedouros, Panos D.},
	month = jul,
	year = {2019},
	keywords = {Vehicle trajectories, Connected automated vehicles, Intersection management, Tabu search},
	pages = {234--248},
	file = {ScienceDirect Snapshot:/home/jeroen/Zotero/storage/LPAJRKJK/S0968090X18305436.html:text/html},
}

@article{chen_cooperative_2016,
	title = {Cooperative {Intersection} {Management}: {A} {Survey}},
	volume = {17},
	issn = {1558-0016},
	shorttitle = {Cooperative {Intersection} {Management}},
	url = {https://ieeexplore.ieee.org/document/7244203},
	doi = {10.1109/TITS.2015.2471812},
	abstract = {Intersection management is one of the most challenging problems within the transport system. Traffic light-based methods have been efficient but are not able to deal with the growing mobility and social challenges. On the other hand, the advancements of automation and communications have enabled cooperative intersection management, where road users, infrastructure, and traffic control centers are able to communicate and coordinate the traffic safely and efficiently. Major techniques and solutions for cooperative intersections are surveyed in this paper for both signalized and nonsignalized intersections, whereas focuses are put on the latter. Cooperative methods, including time slots and space reservation, trajectory planning, and virtual traffic lights, are discussed in detail. Vehicle collision warning and avoidance methods are discussed to deal with uncertainties. Concerning vulnerable road users, pedestrian collision avoidance methods are discussed. In addition, an introduction to major projects related to cooperative intersection management is presented. A further discussion of the presented works is given with highlights of future research topics. This paper serves as a comprehensive survey of the field, aiming at stimulating new methods and accelerating the advancement of automated and cooperative intersections.},
	number = {2},
	urldate = {2024-01-26},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Chen, Lei and Englund, Cristofer},
	month = feb,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {C-ITS, collision avoidance, Computer integrated manufacturing, Cooperative intelligent traffic systems, cooperative intersection management, mathematical optimization, motion planning, multi-agent system, Optimization, Planning, Roads, Safety, Trajectory, trajectory planning, V2I, V2P, V2V, V2X, VANET, Vehicles},
	pages = {570--586},
	file = {IEEE Xplore Abstract Record:/home/jeroen/Zotero/storage/UADN4VTY/7244203.html:text/html},
}

@article{gholamhosseinian_comprehensive_nodate,
	title = {A {Comprehensive} {Survey} on {Cooperative} {Intersection} {Management} for {Heterogeneous} {Connected} {Vehicles} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	volume = {Volume 10},
	url = {https://ieeexplore.ieee.org/document/9678327},
	urldate = {2024-01-26},
	journal = {IEEE Access},
	author = {Gholamhosseinian, Ashkan and Seitz, Jochen},
	file = {A Comprehensive Survey on Cooperative Intersection Management for Heterogeneous Connected Vehicles | IEEE Journals & Magazine | IEEE Xplore:/home/jeroen/Zotero/storage/64HIUTRM/9678327.html:text/html},
}

@article{fayazi_mixed-integer_2018,
	title = {Mixed-{Integer} {Linear} {Programming} for {Optimal} {Scheduling} of {Autonomous} {Vehicle} {Intersection} {Crossing}},
	volume = {3},
	issn = {2379-8904},
	url = {https://ieeexplore.ieee.org/document/8370718},
	doi = {10.1109/TIV.2018.2843163},
	abstract = {We propose an urban traffic management scheme for an all connected vehicle environment. If all the vehicles are autonomous, for example, in smart city projects or future's dense city centers, then such an environment does not need a physical traffic signal. Instead, an intersection control server processes data streams from approaching vehicles, periodically solves an optimization problem, and assigns to each vehicle an optimal arrival time that ensures safety while significantly reducing number of stops and intersection delays. The scheduling problem is formulated as a mixed-integer linear program (MILP), and is solved by IBM CPLEX optimization package. The optimization outputs (scheduled access/arrival times) are sent to all approaching vehicles. The autonomous vehicles adjust their speed accordingly by a proposed trajectory planning algorithm with the aim of accessing the intersection at their scheduled times. A customized traffic microsimulation environment is developed to determine the potentials of the proposed solution in comparison to two baseline scenarios. In addition, the proposed MILP-based intersection control scheme is modified and simulated for a mixed traffic consisting of autonomous and human-controlled vehicles, all connected through a wireless communication to the intersection controller of a signalized intersection.},
	number = {3},
	urldate = {2024-01-26},
	journal = {IEEE Transactions on Intelligent Vehicles},
	author = {Fayazi, Seyed Alireza and Vahidi, Ardalan},
	month = sep,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Intelligent Vehicles},
	keywords = {Autonomous vehicles, Intelligent transportation systems, connected and autonomous vehicles, intersection traffic management, mixed integer linear program, trajectory planning, traffic simulation and modeling, Optimal scheduling, Safety, Servers, Timing, Trajectory},
	pages = {287--299},
	file = {IEEE Xplore Abstract Record:/home/jeroen/Zotero/storage/CGTGJGNR/8370718.html:text/html},
}

@article{li_temporal-spatial_2019-1,
	title = {Temporal-spatial dimension extension-based intersection control formulation for connected and autonomous vehicle systems},
	volume = {104},
	issn = {0968-090X},
	url = {https://www.sciencedirect.com/science/article/pii/S0968090X18305436},
	doi = {10.1016/j.trc.2019.05.003},
	abstract = {Traffic congestion has become a serious issue worldwide due to the rapid increase in population and traffic demands. Advances in connected automated vehicle (CAV) technology demonstrate the potential to improve traffic mobility and safety performance at intersections. An advanced intersection control system is proposed in this study to coordinate vehicle trajectories and ensure safety and operation efficiency at intersections. A temporal-spatial dimension extension-based trajectory coordination model is developed by formulating all possible trajectories of vehicles at the intersection. Correspondingly, according to the trajectory coordination model, two signal-free control algorithms, including the priority-based algorithm and the Discrete Forward-Rolling Optimal Control (DFROC) algorithm are proposed in this study to manage vehicles at the intersection. These two algorithms, together with the FCFS policy, are compared with the conventional signal control method in a SUMO-based simulation platform. Experimental results indicate that the proposed algorithms outperform the signal control method in terms of reducing total traffic delays at intersections and increasing intersection capacity and operation efficiency.},
	urldate = {2024-01-26},
	journal = {Transportation Research Part C: Emerging Technologies},
	author = {Li, Zhenning and Wu, Qiong and Yu, Hao and Chen, Cong and Zhang, Guohui and Tian, Zong Z. and Prevedouros, Panos D.},
	month = jul,
	year = {2019},
	keywords = {Connected automated vehicles, Intersection management, Tabu search, Vehicle trajectories},
	pages = {234--248},
	file = {ScienceDirect Snapshot:/home/jeroen/Zotero/storage/AXY59E4Y/S0968090X18305436.html:text/html},
}

@article{feng_spatiotemporal_2018,
	title = {Spatiotemporal intersection control in a connected and automated vehicle environment},
	volume = {89},
	issn = {0968-090X},
	url = {https://www.sciencedirect.com/science/article/pii/S0968090X1830144X},
	doi = {10.1016/j.trc.2018.02.001},
	abstract = {Current research on traffic control has focused on the optimization of either traffic signals or vehicle trajectories. With the rapid development of connected and automated vehicle (CAV) technologies, vehicles equipped with dedicated short-range communications (DSRC) can communicate not only with other CAVs but also with infrastructure. Joint control of vehicle trajectories and traffic signals becomes feasible and may achieve greater benefits regarding system efficiency and environmental sustainability. Traffic control framework is expected to be extended from one dimension (either spatial or temporal) to two dimensions (spatiotemporal). This paper investigates a joint control framework for isolated intersections. The control framework is modeled as a two-stage optimization problem with signal optimization at the first stage and vehicle trajectory control at the second stage. The signal optimization is modeled as a dynamic programming (DP) problem with the objective to minimize vehicle delay. Optimal control theory is applied to the vehicle trajectory control problem with the objective to minimize fuel consumption and emissions. A simplified objective function is adopted to get analytical solutions to the optimal control problem so that the two-stage model is solved efficiently. Simulation results show that the proposed joint control framework is able to reduce both vehicle delay and emissions under a variety of demand levels compared to fixed-time and adaptive signal control when vehicle trajectories are not optimized. The reduced vehicle delay and CO2 emissions can be as much as 24.0\% and 13.8\%, respectively for a simple two-phase intersection. Sensitivity analysis suggests that maximum acceleration and deceleration rates have a significant impact on the performance regarding both vehicle delay and emission reduction. Further extension to a full eight-phase intersection shows a similar pattern of delay and emission reduction by the joint control framework.},
	urldate = {2024-01-26},
	journal = {Transportation Research Part C: Emerging Technologies},
	author = {Feng, Yiheng and Yu, Chunhui and Liu, Henry X.},
	month = apr,
	year = {2018},
	keywords = {Connected and automated vehicle, Delay and emission reduction, Traffic signal control, Vehicle trajectory control},
	pages = {364--383},
	file = {ScienceDirect Snapshot:/home/jeroen/Zotero/storage/UIUYKK85/S0968090X1830144X.html:text/html},
}

@inproceedings{guerreiro_online_2023,
	title = {Online {Scheduling}: {A} {Survey}},
	shorttitle = {Online {Scheduling}},
	url = {https://ieeexplore.ieee.org/document/10211826},
	doi = {10.23919/CISTI58278.2023.10211826},
	abstract = {In this article a deep search of the literature of online scheduling is conducted. This paper intends to assess the developments and solutions found for online scheduling problems. Online scheduling is a very important topic since most of the real scheduling problems have dynamic characteristics. First, it was developed a literature review about scheduling problems, dividing them in stochastic and deterministic problems as well as in online and offline problems. Then, a bibliometric analysis was performed. Finally, some case studies in the field of online scheduling were analyzed. Online Scheduling is mostly explored in industry and health areas. In some articles explored there is a rescheduling, and the sequence of task may change due to the arrival of new tasks. In other cases, the new tasks are introduced in blocks of time that do not affect the previous schedule. This last technique is limited, since, with the arrival of new tasks, the schedule is not re-evaluated. Therefore, it is thought that, in future work, within the scope of online scheduling, when new tasks or other significant changes enter the system, the system should be evaluated, allowing the necessary changes to be made to the existing schedule. The Industry 4.0 and the evolution of Internet of Things (IoT), Deep Learning and Machine Learning favours a continuous and real-time flow of information, which allows the implementation of real-time online scheduling. This is a branch that should be explored in future works.},
	urldate = {2024-01-26},
	booktitle = {2023 18th {Iberian} {Conference} on {Information} {Systems} and {Technologies} ({CISTI})},
	author = {Guerreiro, Rita and Santos, André S. and Tereso, Anabela},
	month = jun,
	year = {2023},
	note = {ISSN: 2166-0727},
	keywords = {Bibliometrics, Deep learning, Dynamic scheduling, heuristics, Industries, internet of things, Job shop scheduling, machine learning, online scheduling, Real-time systems, Schedules},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/jeroen/Zotero/storage/9FBJZKNU/10211826.html:text/html},
}
