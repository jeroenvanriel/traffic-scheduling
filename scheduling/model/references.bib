@misc{achiamConstrainedPolicyOptimization2017,
  title = {Constrained {{Policy Optimization}}},
  author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  year = {2017},
  month = may,
  number = {arXiv:1705.10528},
  eprint = {1705.10528},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-12-08},
  abstract = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/zotero/storage/LUQLNWJJ/Achiam et al. - 2017 - Constrained Policy Optimization.pdf}
}

@article{AdaptiveSamplingAlgorithm2022,
  title = {An {{Adaptive Sampling Algorithm}} for {{Solving Markov Decision Processes}}},
  year = {2022},
  pages = {15},
  langid = {english},
  file = {/home/jeroen/zotero/storage/BBJI4IYS/2022 - An Adaptive Sampling Algorithm for Solving Markov .pdf}
}

@article{agandEcoLightRewardShaping2021,
  title = {{{EcoLight}}: {{Reward Shaping}} in {{Deep Reinforcement Learning}} for {{Ergonomic Traffic Signal Control}}},
  author = {Agand, Pedram and Iskrov, Alexey},
  year = {2021},
  abstract = {Mobility, the environment, and human health are all harmed by sub-optimal control policies in transportation systems. Intersection traffic signal controllers are a crucial part of today's transportation infrastructure, as sub-optimal policies may lead to traffic jams and as a result increased levels of air pollution and wasted time. Many adaptive traffic signal controllers have been proposed in the literature, but research on their relative performance differences is limited. On the other hand, to the best of our knowledge there has been no work that directly targets CO2 emission reduction, even though pollution is currently a critical issue. In this paper, we propose a reward shaping scheme for various RL algorithms that not only produces lowers CO2 emissions, but also produces respectable outcomes in terms of other metrics such as travel time. We compare multiple RL algorithms \textemdash{} sarsa, and A2C \textemdash{} as well as diverse scenarios with a mix of different road users emitting varied amounts of pollution.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/JY2AYWIY/Agand and Iskrov - EcoLight Reward Shaping in Deep Reinforcement Lea.pdf}
}

@misc{agarwalProvableBenefitsRepresentational2022,
  title = {Provable {{Benefits}} of {{Representational Transfer}} in {{Reinforcement Learning}}},
  author = {Agarwal, Alekh and Song, Yuda and Sun, Wen and Wang, Kaiwen and Wang, Mengdi and Zhang, Xuezhou},
  year = {2022},
  month = may,
  number = {arXiv:2205.14571},
  eprint = {2205.14571},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-12-08},
  abstract = {We study the problem of representational transfer in RL, where an agent first pretrains in a number of source tasks to discover a shared representation, which is subsequently used to learn a good policy in a target task. We propose a new notion of task relatedness between source and target tasks, and develop a novel approach for representational transfer under this assumption. Concretely, we show that given a generative access to source tasks, we can discover a representation, using which subsequent linear RL techniques quickly converge to a near-optimal policy, with only online access to the target task. The sample complexity is close to knowing the ground truth features in the target task, and comparable to prior representation learning results in the source tasks. We complement our positive results with lower bounds without generative access, and validate our findings with empirical evaluation on rich observation MDPs that require deep exploration.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/jeroen/zotero/storage/5GURUTLZ/Agarwal et al. - 2022 - Provable Benefits of Representational Transfer in .pdf}
}

@article{alegreQuantifyingImpactNonstationarity2021,
  title = {Quantifying the Impact of Non-Stationarity in Reinforcement Learning-Based Traffic Signal Control},
  author = {Alegre, Lucas N. and Bazzan, Ana L.C. and {da Silva}, Bruno C.},
  year = {2021},
  month = may,
  journal = {PeerJ Computer Science},
  volume = {7},
  pages = {e575},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.575},
  urldate = {2022-12-21},
  abstract = {In reinforcement learning (RL), dealing with non-stationarity is a challenging issue. However, some domains such as traffic optimization are inherently non-stationary. Causes for and effects of this are manifold. In particular, when dealing with traffic signal controls, addressing non-stationarity is key since traffic conditions change over time and as a function of traffic control decisions taken in other parts of a network. In this paper we analyze the effects that different sources of non-stationarity have in a network of traffic signals, in which each signal is modeled as a learning agent. More precisely, we study both the effects of changing the context in which an agent learns (e.g., a change in flow rates experienced by it), as well as the effects of reducing agent observability of the true environment state. Partial observability may cause distinct states (in which distinct actions are optimal) to be seen as the same by the traffic signal agents. This, in turn, may lead to sub-optimal performance. We show that the lack of suitable sensors to provide a representative observation of the real state seems to affect the performance more drastically than the changes to the underlying traffic patterns.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/JB7J3AXL/Alegre et al. - 2021 - Quantifying the impact of non-stationarity in rein.pdf}
}

@book{altmanConstrainedMarkovDecision2021,
  title = {Constrained {{Markov Decision Processes}}: {{Stochastic Modeling}}},
  shorttitle = {Constrained {{Markov Decision Processes}}},
  author = {Altman, Eitan},
  year = {2021},
  month = dec,
  edition = {1},
  publisher = {{Routledge}},
  address = {{Boca Raton}},
  doi = {10.1201/9781315140223},
  urldate = {2022-12-13},
  isbn = {978-1-315-14022-3},
  langid = {english},
  file = {/home/jeroen/zotero/storage/5EM7MRDE/Altman - 2021 - Constrained Markov Decision Processes Stochastic .pdf}
}

@article{antesInformationUpwardsRecommendation2022,
  title = {Information Upwards, Recommendation Downwards: Reinforcement Learning with Hierarchy for Traffic Signal Control},
  shorttitle = {Information Upwards, Recommendation Downwards},
  author = {Antes, Taylor de O. and Bazzan, Ana L.C. and Tavares, Anderson Rocha},
  year = {2022},
  journal = {Procedia Computer Science},
  volume = {201},
  pages = {24--31},
  issn = {18770509},
  doi = {10.1016/j.procs.2022.03.006},
  urldate = {2022-12-21},
  langid = {english},
  file = {/home/jeroen/zotero/storage/AN5WPLVM/Antes et al. - 2022 - Information upwards, recommendation downwards rei.pdf}
}

@article{antesInformationUpwardsRecommendation2022a,
  title = {Information Upwards, Recommendation Downwards: Reinforcement Learning with Hierarchy for Traffic Signal Control},
  shorttitle = {Information Upwards, Recommendation Downwards},
  author = {Antes, Taylor de O. and Bazzan, Ana L. C. and Tavares, Anderson Rocha},
  year = {2022},
  month = jan,
  journal = {Procedia Computer Science},
  series = {The 13th {{International Conference}} on {{Ambient Systems}}, {{Networks}} and {{Technologies}} ({{ANT}}) / {{The}} 5th {{International Conference}} on {{Emerging Data}} and {{Industry}} 4.0 ({{EDI40}})},
  volume = {201},
  pages = {24--31},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2022.03.006},
  urldate = {2023-09-26},
  abstract = {Traffic signal control (TSC) is a practical solution to the major problem of congestion in metropolitan areas. Reinforcement Learning (RL) techniques present powerful frameworks for optimizing traffic signal controllers that learn to respond to real-time traffic changes. Multiagent RL (MARL) techniques have been showing better results over centralized techniques (RL-based or not), where local intersection agents have partial observation of and control over the environment. Since in TSC the best decision does not depend only on local information, in the present paper we aim at increasing agents' views by using a hierarchical approach, where information is passed upwards, is then aggregated forming recommendations that are sent downwards. We divide the transportation network into regions, each controlled by a region agent; this is done at different hierarchical levels. The traffic signal controllers, located at the intersections, are the local agents at the hierarchy's bottom. Region agents can supervise intersection agents or other region agents. Evaluation of this approach in a synthetic traffic grid shows that the proposed hierarchical organization outperforms a fixed-time approach and an RL-based approach without hierarchy.},
  keywords = {Intelligent Transportation Systems,Multiagent Systems,Reinforcement Learning,Smart Cities},
  file = {/home/jeroen/zotero/storage/T5W7ATZD/Antes et al. - 2022 - Information upwards, recommendation downwards rei.pdf;/home/jeroen/zotero/storage/6IRNMEHS/S1877050922004185.html}
}

@inproceedings{ashtianiMultiIntersectionTrafficManagement2018,
  title = {Multi-{{Intersection Traffic Management}} for {{Autonomous Vehicles}} via {{Distributed Mixed Integer Linear Programming}}},
  booktitle = {2018 {{Annual American Control Conference}} ({{ACC}})},
  author = {Ashtiani, Faraz and Fayazi, S. Alireza and Vahidi, Ardalan},
  year = {2018},
  month = jun,
  eprint = {2007.06639},
  primaryclass = {cs, eess, math},
  pages = {6341--6346},
  doi = {10.23919/ACC.2018.8431656},
  urldate = {2023-09-14},
  abstract = {This paper extends our previous work in [1],[2], on optimal scheduling of autonomous vehicle arrivals at intersections, from one to a grid of intersections. A scalable distributed Mixed Integer Linear Program (MILP) is devised that solves the scheduling problem for a grid of intersections. A computational control node is allocated to each intersection and regularly receives position and velocity information from subscribed vehicles. Each node assigns an intersection access time to every subscribed vehicle by solving a local MILP. Neighboring intersections will coordinate with each other in real-time by sharing their solutions for vehicles' access times with each other. Our proposed approach is applied to a grid of nine intersections and its positive impact on traffic flow and vehicles' fuel economy is demonstrated in comparison to conventional intersection control scenarios.},
  archiveprefix = {arxiv},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,MILP},
  file = {/home/jeroen/zotero/storage/YGWRNS8U/Ashtiani et al. - 2018 - Multi-Intersection Traffic Management for Autonomo.pdf;/home/jeroen/zotero/storage/3QDH5MYN/2007.html}
}

@article{aultReinforcementLearningBenchmarks,
  title = {Reinforcement {{Learning Benchmarks}} for {{Traffic Signal Control}}},
  author = {Ault, James and Sharon, Guni},
  abstract = {We propose a toolkit for developing and comparing reinforcement learning (RL)based traffic signal controllers. The toolkit includes implementation of state-of-theart deep-RL algorithms for signal control along with benchmark control problems that are based on realistic traffic scenarios. Importantly, the toolkit allows a firstof-its-kind comparison between state-of-the-art RL-based signal controllers while providing benchmarks for future comparisons. Consequently, we compare and report the relative performance of current RL algorithms. The experimental results suggest that previous algorithms are not robust to varying sensing assumptions and non-stylized intersection layouts. When more realistic signal layouts and advanced sensing capabilities are considered, a distributed deep Q-learning approach is shown to outperform previously reported state-of-the-art algorithms in many cases.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/XQ6XB54M/Ault and Sharon - Reinforcement Learning Benchmarks for Traffic Sign.pdf}
}

@article{baileyQueueingProcessesBulk1954,
  title = {On {{Queueing Processes}} with {{Bulk Service}}},
  author = {Bailey, Norman T. J.},
  year = {1954},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {16},
  number = {1},
  pages = {80--87},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1954.tb00149.x},
  urldate = {2023-01-10},
  langid = {english}
}

@article{balasOneMachineProblemDelayed1995,
  title = {The {{One-Machine Problem}} with {{Delayed Precedence Constraints}} and Its {{Use}} in {{Job Shop Scheduling}}},
  author = {Balas, Egon and Lenstra, Jan Karel and Vazacopoulos, Alkis},
  year = {1995},
  month = jan,
  journal = {Management Science},
  publisher = {{INFORMS}},
  doi = {10.1287/mnsc.41.1.94},
  urldate = {2023-10-20},
  abstract = {We study the one machine scheduling problem with release and delivery times and the minimum makespan objective, in the presence of constraints that for certain pairs of jobs require a delay between...},
  copyright = {\textcopyright{} 1995 INFORMS},
  langid = {english},
  file = {/home/jeroen/zotero/storage/IQSCZJN8/mnsc.41.1.html}
}

@article{bazzanOpportunitiesMultiagentSystems2009,
  title = {Opportunities for Multiagent Systems and Multiagent Reinforcement Learning in Traffic Control},
  author = {Bazzan, Ana L. C.},
  year = {2009},
  month = jun,
  journal = {Autonomous Agents and Multi-Agent Systems},
  volume = {18},
  number = {3},
  pages = {342--375},
  issn = {1573-7454},
  doi = {10.1007/s10458-008-9062-9},
  urldate = {2023-09-26},
  abstract = {The increasing demand for mobility in our society poses various challenges to traffic engineering, computer science in general, and artificial intelligence and multiagent systems in particular. As it is often the case, it is not possible to provide additional capacity, so that a more efficient use of the available transportation infrastructure is necessary. This relates closely to multiagent systems as many problems in traffic management and control are inherently distributed. Also, many actors in a transportation system fit very well the concept of autonomous agents: the driver, the pedestrian, the traffic expert; in some cases, also the intersection and the traffic signal controller can be regarded as an autonomous agent. However, the ``agentification'' of a transportation system is associated with some challenging issues: the number of agents is high, typically agents are highly adaptive, they react to changes in the environment at individual level but cause an unpredictable collective pattern, and act in a highly coupled environment. Therefore, this domain poses many challenges for standard techniques from multiagent systems such as coordination and learning. This paper has two main objectives: (i) to present problems, methods, approaches and practices in traffic engineering (especially regarding traffic signal control); and (ii) to highlight open problems and challenges so that future research in multiagent systems can address them.},
  langid = {english},
  keywords = {Coordination of agents,Game-theory,Multiagent learning,Multiagent systems,Reinforcement learning,Traffic signal control},
  file = {/home/jeroen/zotero/storage/JXBA9ZN2/Bazzan - 2009 - Opportunities for multiagent systems and multiagen.pdf}
}

@article{belouadahSchedulingReleaseDates1992,
  title = {Scheduling with Release Dates on a Single Machine to Minimize Total Weighted Completion Time},
  author = {Belouadah, H. and Posner, M. E. and Potts, C. N.},
  year = {1992},
  month = may,
  journal = {Discrete Applied Mathematics},
  volume = {36},
  number = {3},
  pages = {213--231},
  issn = {0166-218X},
  doi = {10.1016/0166-218X(92)90255-9},
  urldate = {2023-10-23},
  abstract = {This paper considers the problem of scheduling jobs with release dates on a single machine to minimize the total weighted completion time. A branch and bound algorithm is proposed which incorporates three special features that contribute to its efficiency. Firstly, quickly computed lower bounds are obtained using a procedure which is based on job splitting. The job splitting methodology is shown to be applicable to a range of total weighted completion time scheduling problems. Secondly, the branching rule includes a release date adjustment mechanism which increases release dates at certain nodes of the tree with a view to tightening lower bounds. Thirdly, the branch and bound algorithm includes a new dominance rule for eliminating nodes of the search tree. Computational experience on problems with up to 50 jobs indicates that the proposed algorithm is superior to other known algorithms.},
  file = {/home/jeroen/zotero/storage/ZBKRIBP9/Belouadah et al. - 1992 - Scheduling with release dates on a single machine .pdf;/home/jeroen/zotero/storage/NGL4CCBH/0166218X92902559.html}
}

@misc{bengioMachineLearningCombinatorial2020,
  title = {Machine {{Learning}} for {{Combinatorial Optimization}}: A {{Methodological Tour}} d'{{Horizon}}},
  shorttitle = {Machine {{Learning}} for {{Combinatorial Optimization}}},
  author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
  year = {2020},
  month = mar,
  number = {arXiv:1811.06128},
  eprint = {1811.06128},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1811.06128},
  urldate = {2023-09-24},
  abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/FB883LT7/Bengio et al. - 2020 - Machine Learning for Combinatorial Optimization a.pdf;/home/jeroen/zotero/storage/2IHTDUK3/1811.html}
}

@article{boonNetworksFixedcycleIntersections2018a,
  title = {Networks of Fixed-Cycle Intersections},
  author = {Boon, Marko A.A. and Van Leeuwaarden, Johan S.H.},
  year = {2018},
  month = nov,
  journal = {Transportation Research Part B: Methodological},
  volume = {117},
  pages = {254--271},
  issn = {01912615},
  doi = {10.1016/j.trb.2018.08.019},
  urldate = {2023-09-25},
  langid = {english},
  file = {/home/jeroen/zotero/storage/WMHHVYFC/Boon and Van Leeuwaarden - 2018 - Networks of fixed-cycle intersections.pdf}
}

@misc{boonOptimalCapacityAllocation2022,
  title = {Optimal Capacity Allocation for Heavy-Traffic Fixed-Cycle Traffic-Light Queues and Intersections},
  author = {Boon, Marko and Janssen, Guido and {van Leeuwaarden}, Johan and Timmerman, Rik},
  year = {2022},
  month = aug,
  number = {arXiv:2104.04303},
  eprint = {2104.04303},
  primaryclass = {math},
  publisher = {{arXiv}},
  urldate = {2022-12-21},
  abstract = {Setting traffic light signals is a classical topic in traffic engineering, and important in heavy-traffic conditions when green times become scarce and longer queues are inevitably formed. For the fixed-cycle traffic-light queue, an elementary queueing model for one traffic light with cyclic signaling, we obtain heavy-traffic limits that capture the long-term queue behavior. We leverage the limit theorems to obtain sharp performance approximations for one queue in heavy traffic. We also consider optimization problems that aim for optimal division of green times among multiple conflicting traffic streams. We show that inserting heavy-traffic approximations leads to tractable optimization problems and close-to-optimal signal prescriptions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Optimization and Control,Mathematics - Probability},
  file = {/home/jeroen/zotero/storage/NQIN56RM/Boon et al. - 2022 - Optimal capacity allocation for heavy-traffic fixe.pdf}
}

@inproceedings{bouderbaReinforcementLearningQLEARNING2019,
  title = {Reinforcement {{Learning}} ({{Q-LEARNING}}) Traffic Light Controller within Intersection Traffic System},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Big Data}} and {{Internet}} of {{Things}}},
  author = {Bouderba, Saif Islam and Moussa, Najem},
  year = {2019},
  month = oct,
  pages = {1--6},
  publisher = {{ACM}},
  address = {{Rabat Morocco}},
  doi = {10.1145/3372938.3372999},
  urldate = {2023-01-20},
  abstract = {In this paper we study the effect of signalized traffic intersection control strategies in a cellular automaton model for transportation in urban networks. Starting with a simple synchronized strategy, then a green wave which gave a surprising result. Finally, a reinforcement learning approach (Q-LEARNING) is presented to learn the traffic light controller how to interact with drivers with different situation. By keeping a belief the level of cooperation drivers, we improved the performance of Q-LEARNING algorithms. We show that our traffic light controller successfully learns how to manage the intersection with less deadlocks than without learning.},
  isbn = {978-1-4503-7240-4},
  langid = {english},
  file = {/home/jeroen/zotero/storage/7PLTGWMR/Bouderba and Moussa - 2019 - Reinforcement Learning (Q-LEARNING) traffic light .pdf}
}

@article{bruckerSchedulingChainsIdentical2006,
  title = {Scheduling Chains with Identical Jobs and Constant Delays on a Single Machine},
  author = {Brucker, Peter and Knust, Sigrid and O{\u g}uz, Ceyda},
  year = {2006},
  month = feb,
  journal = {Mathematical Methods of Operations Research},
  volume = {63},
  number = {1},
  pages = {63--75},
  issn = {1432-5217},
  doi = {10.1007/s00186-005-0014-8},
  urldate = {2023-10-20},
  abstract = {In this paper we study the single-machine problem 1|chains(l), pj= p|{$\sum$} Cjin which jobs with constant processing times and generalized precedence constraints in form of chains with constant delays are given. One has to schedule the jobs on a single machine such that all delays between consecutive jobs in a chain are satisfied and the sum of all completion times of the jobs is minimized. We show that this problem is polynomially solvable.},
  langid = {english},
  keywords = {Complexity results,Delays,Scheduling,Time-lags},
  file = {/home/jeroen/zotero/storage/YIUHYYQ7/Brucker et al. - 2006 - Scheduling chains with identical jobs and constant.pdf}
}

@article{bubeckRegretAnalysisStochastic2012,
  title = {Regret {{Analysis}} of {{Stochastic}} and {{Nonstochastic Multi-armed Bandit Problems}}},
  author = {Bubeck, S{\'e}bastien},
  year = {2012},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {5},
  number = {1},
  pages = {1--122},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000024},
  urldate = {2022-12-08},
  abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration\textendash exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the 1930s, exploration\textendash exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/I589D9ZR/Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf}
}

@article{busoniuComprehensiveSurveyMultiagent2008,
  title = {A {{Comprehensive Survey}} of {{Multiagent Reinforcement Learning}}},
  author = {Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  year = {2008},
  month = mar,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {38},
  number = {2},
  pages = {156--172},
  issn = {1094-6977, 1558-2442},
  doi = {10.1109/TSMCC.2007.913919},
  urldate = {2023-09-27},
  abstract = {Multi-agent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must instead discover a solution on their own, using learning. A significant part of the research on multi-agent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multi-agent reinforcement learning (MARL). A central issue in the field is the formal statement of the multi-agent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents' learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim\textemdash either explicitly or implicitly\textemdash at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges of MARL are described along with some of the problem domains where MARL techniques have been applied. Finally, an outlook for the field is provided.},
  langid = {english},
  keywords = {MARL},
  file = {/home/jeroen/zotero/storage/29BVG529/Busoniu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement.pdf}
}

@inproceedings{caiRealTimeBiddingReinforcement2017,
  title = {Real-{{Time Bidding}} by {{Reinforcement Learning}} in {{Display Advertising}}},
  booktitle = {Proceedings of the {{Tenth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Cai, Han and Ren, Kan and Zhang, Weinan and Malialis, Kleanthis and Wang, Jun and Yu, Yong and Guo, Defeng},
  year = {2017},
  month = feb,
  eprint = {1701.02490},
  primaryclass = {cs},
  pages = {661--670},
  doi = {10.1145/3018661.3018702},
  urldate = {2022-12-13},
  abstract = {The majority of online display ads are served through realtime bidding (RTB) \textemdash{} each ad display impression is auctioned off in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign's real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks. The empirical study on two large-scale real-world datasets and the live A/B testing on a commercial platform have demonstrated the superior performance and high efficiency compared to state-of-the-art methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {/home/jeroen/zotero/storage/8HEL4ZHB/Cai et al. - 2017 - Real-Time Bidding by Reinforcement Learning in Dis.pdf}
}

@misc{catichaLecturesProbabilityEntropy2008,
  title = {Lectures on {{Probability}}, {{Entropy}}, and {{Statistical Physics}}},
  author = {Caticha, Ariel},
  year = {2008},
  month = jul,
  number = {arXiv:0808.0012},
  eprint = {0808.0012},
  primaryclass = {cond-mat, physics:physics, stat},
  publisher = {{arXiv}},
  urldate = {2023-02-18},
  abstract = {These lectures deal with the problem of inductive inference, that is, the problem of reasoning under conditions of incomplete information. Is there a general method for handling uncertainty? Or, at least, are there rules that could in principle be followed by an ideally rational mind when discussing scientific matters? What makes one statement more plausible than another? How much more plausible? And then, when new information is acquired how do we change our minds? Or, to put it differently, are there rules for learning? Are there rules for processing information that are objective and consistent? Are they unique? And, come to think of it, what, after all, is information? It is clear that data contains or conveys information, but what does this precisely mean? Can information be conveyed in other ways? Is information physical? Can we measure amounts of information? Do we need to? Our goal is to develop the main tools for inductive inference--probability and entropy--from a thoroughly Bayesian point of view and to illustrate their use in physics with examples borrowed from the foundations of classical statistical physics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Statistics Theory,{Physics - Data Analysis, Statistics and Probability},Physics - General Physics},
  file = {/home/jeroen/zotero/storage/YN6PZ7FS/Caticha - 2008 - Lectures on Probability, Entropy, and Statistical .pdf;/home/jeroen/zotero/storage/DRQN8XWU/0808.html}
}

@misc{cesa-bianchiBoltzmannExplorationDone2017,
  title = {Boltzmann {{Exploration Done Right}}},
  author = {{Cesa-Bianchi}, Nicol{\`o} and Gentile, Claudio and Lugosi, G{\'a}bor and Neu, Gergely},
  year = {2017},
  month = nov,
  number = {arXiv:1705.10257},
  eprint = {1705.10257},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-12-09},
  abstract = {Boltzmann exploration is a classic strategy for sequential decision-making under uncertainty, and is one of the most standard tools in Reinforcement Learning (RL). Despite its widespread use, there is virtually no theoretical understanding about the limitations or the actual benefits of this exploration scheme. Does it drive exploration in a meaningful way? Is it prone to misidentifying the optimal actions or spending too much time exploring the suboptimal ones? What is the right tuning for the learning rate? In this paper, we address several of these questions in the classic setup of stochastic multi-armed bandits. One of our main results is showing that the Boltzmann exploration strategy with any monotone learning-rate sequence will induce suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that guarantees near-optimal performance, albeit only when given prior access to key problem parameters that are typically not available in practical situations (like the time horizon \$T\$ and the suboptimality gap \$\textbackslash Delta\$). More importantly, we propose a novel variant that uses different learning rates for different arms, and achieves a distribution-dependent regret bound of order \$\textbackslash frac\{K\textbackslash log\^2 T\}\{\textbackslash Delta\}\$ and a distribution-independent bound of order \$\textbackslash sqrt\{KT\}\textbackslash log K\$ without requiring such prior knowledge. To demonstrate the flexibility of our technique, we also propose a variant that guarantees the same performance bounds even if the rewards are heavy-tailed.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/J9KN3GKJ/Cesa-Bianchi et al. - 2017 - Boltzmann Exploration Done Right.pdf;/home/jeroen/zotero/storage/A9LIPILZ/1705.html}
}

@incollection{chaudhuriComparativeStudyAlgorithms2022,
  title = {A {{Comparative Study}} of {{Algorithms}} for {{Intelligent Traffic Signal Control}}},
  booktitle = {Machine {{Learning}} and {{Autonomous Systems}}},
  author = {Chaudhuri, Hrishit and Masti, Vibha and Veerendranath, Vishruth and Natarajan, S.},
  editor = {Chen, Joy Iong-Zong and Wang, Haoxiang and Du, Ke-Lin and Suma, V.},
  year = {2022},
  volume = {269},
  pages = {271--287},
  publisher = {{Springer Nature Singapore}},
  address = {{Singapore}},
  doi = {10.1007/978-981-16-7996-4_19},
  urldate = {2023-01-20},
  abstract = {In this paper, methods have been explored to effectively optimise traffic signal control to minimise waiting times and queue lengths, thereby increasing traffic flow. The traffic intersection was first defined as a Markov Decision Process, and a state representation, actions and rewards were chosen. Simulation of Urban MObility (SUMO) was used to simulate an intersection and then compare a Round Robin Scheduler, a Feedback Control mechanism and two Reinforcement Learning techniques - Deep Q Network (DQN) and Advantage Actor-Critic (A2C), as the policy for the traffic signal in the simulation under different scenarios. Finally, the methods were tested on a simulation of a real-world intersection in Bengaluru, India.},
  isbn = {9789811679957 9789811679964},
  langid = {english},
  file = {/home/jeroen/zotero/storage/BYK43CTD/Chaudhuri et al. - 2022 - A Comparative Study of Algorithms for Intelligent .pdf}
}

@misc{chenS2TNetSpatioTemporalTransformer2022,
  title = {{{S2TNet}}: {{Spatio-Temporal Transformer Networks}} for {{Trajectory Prediction}} in {{Autonomous Driving}}},
  shorttitle = {{{S2TNet}}},
  author = {Chen, Weihuang and Wang, Fangfang and Sun, Hongbin},
  year = {2022},
  month = jun,
  number = {arXiv:2206.10902},
  eprint = {2206.10902},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-01-20},
  abstract = {To safely and rationally participate in dense and heterogeneous traffic, autonomous vehicles require to sufficiently analyze the motion patterns of surrounding traffic-agents and accurately predict their future trajectories. This is challenging because the trajectories of traffic-agents are not only influenced by the traffic-agents themselves but also by spatial interaction with each other. Previous methods usually rely on the sequential stepby-step processing of Long Short-Term Memory networks (LSTMs) and merely extract the interactions between spatial neighbors for single type traffic-agents. We propose the Spatio-Temporal Transformer Networks (S2TNet), which models the spatio-temporal interactions by spatio-temporal Transformer and deals with the temporel sequences by temporal Transformer. We input additional category, shape and heading information into our networks to handle the heterogeneity of traffic-agents. The proposed methods outperforms state-of-the-art methods on ApolloScape Trajectory dataset by more than 7\% on both the weighted sum of Average and Final Displacement Error. Our code is available at https://github.com/chenghuang66/s2tnet.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/jeroen/zotero/storage/TPL7UFL9/Chen et al. - 2022 - S2TNet Spatio-Temporal Transformer Networks for T.pdf}
}

@article{chouAlgorithmsSingleMachine2009,
  title = {Algorithms for the Single Machine Total Weighted Completion Time Scheduling Problem with Release Times and Sequence-Dependent Setups},
  author = {Chou, Fuh-Der and Wang, Hui-Mei and Chang, Tzu-Yun},
  year = {2009},
  month = aug,
  journal = {The International Journal of Advanced Manufacturing Technology},
  volume = {43},
  number = {7-8},
  pages = {810--821},
  issn = {0268-3768, 1433-3015},
  doi = {10.1007/s00170-008-1762-4},
  urldate = {2023-10-20},
  abstract = {This paper considers the problem of scheduling n jobs on a single machine to minimize the total weighted completion time in the presence of sequence-dependent setup times and release times. To the best of our knowledge, little research has been devoted to this scheduling problem. Therefore, we developed two exact algorithms, including a constraint programming model and a branch-and-bound method for small problems. The obtained optimal solutions can be used as a benchmark for evaluating the performance of heuristics. With the complexity in mind, two heuristics, including a best index dispatch (BID) and a modified weighted shortest processing time (MWSPT) based on non-delay concepts are also proposed for large problems. The time complexities of the two proposed heuristics are O(n4) and O(n3), respectively. The computational results showed that the branch-andbound method could solve most instances with 40 jobs under the time limit of 7,200 s. The BID heuristic is superior to the MWSPT in solution quality, although both can efficiently and effectively obtain near-optimal solutions for large instances.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/9SEVG8S6/Chou et al. - 2009 - Algorithms for the single machine total weighted c.pdf}
}

@article{cireMultivaluedDecisionDiagrams,
  title = {Multivalued {{Decision Diagrams}} for {{Sequencing Problems}}},
  author = {Cire, Andre A and {van Hoeve}, Willem-Jan},
  abstract = {Sequencing problems are among the most prominent problems studied in operations research, with primary application in, e.g., scheduling and routing. We propose a novel approach to solving generic sequencing problems using multivalued decision diagrams (MDDs). Because an MDD representation may grow exponentially large, we apply MDDs of limited size as a discrete relaxation to the problem. We show that MDDs can be used to represent a wide range of sequencing problems with various side constraints and objective functions, and demonstrate how MDDs can be added to existing constraint-based scheduling systems. Our computational results indicate that the additional inference obtained by our MDDs can speed up a state-of-the art solver by several orders of magnitude, for a range of different problem classes.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/VEJBLVRL/Cire and van Hoeve - Multivalued Decision Diagrams for Sequencing Probl.pdf}
}

@article{daganzoCellTransmissionModel1994,
  title = {The Cell Transmission Model: {{A}} Dynamic Representation of Highway Traffic Consistent with the Hydrodynamic Theory},
  shorttitle = {The Cell Transmission Model},
  author = {Daganzo, Carlos F.},
  year = {1994},
  month = aug,
  journal = {Transportation Research Part B: Methodological},
  volume = {28},
  number = {4},
  pages = {269--287},
  issn = {0191-2615},
  doi = {10.1016/0191-2615(94)90002-7},
  urldate = {2023-01-20},
  abstract = {This paper presents a simple representation of traffic on a highway with a single entrance and exit. The representation can be used to predict traffic's evolution over time and space, including transient phenomena such as the building, propagation, and dissipation of queues. The easy-to-solve difference equations used to predict traffic's evolution are shown to be the discrete analog of the differential equations arising from a special case of the hydrodynamic model of traffic flow. The proposed method automatically generates appropriate changes in density at locations where the hydrodynamic theory would call for a shockwave; i.e., a jump in density such as those typically seen at the end of every queue. The complex side calculations required by classical methods to keep track of shockwaves are thus eliminated. The paper also shows how the equations can mimic the real-life development of stop-and-go traffic within moving queues.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/9QHLKJCR/Daganzo - 1994 - The cell transmission model A dynamic representat.pdf;/home/jeroen/zotero/storage/7QWXPRCJ/0191261594900027.html}
}

@article{daganzoCellTransmissionModel1995,
  title = {The Cell Transmission Model, Part {{II}}: {{Network}} Traffic},
  shorttitle = {The Cell Transmission Model, Part {{II}}},
  author = {Daganzo, Carlos F.},
  year = {1995},
  month = apr,
  journal = {Transportation Research Part B: Methodological},
  volume = {29},
  number = {2},
  pages = {79--93},
  issn = {0191-2615},
  doi = {10.1016/0191-2615(94)00022-R},
  urldate = {2023-01-20},
  abstract = {This article shows how the evolution of multi-commodity traffic flows over complex networks can be predicted over time, based on a simple macroscopic computer representation of traffic flow that is consistent with the kinematic wave theory under all traffic conditions. The method does not use ad hoc procedures to treat special situations. After a brief review of the basic model for one link, the article describes how three-legged junctions can be modeled. It then introduces a numerical procedure for networks, assuming that a time-varying origin-destination (O-D) table is given and that the proportion of turns at every junction is known. These assumptions are reasonable for numerical analysis of disaster evacuation plans. The results are then extended to the case where, instead of the turning proportions, the best routes to each destination from every junction are known at all times. For technical reasons explained in the text, the procedure is more complicated in this case, requiring more computer memory and more time for execution. The effort is estimated to be about an order of magnitude greater than for the static traffic assignment problem on a network of the same size. The procedure is ideally suited for parallel computing. It is hoped that the results in the article will lead to more realistic models of freeway flow, disaster evacuations and dynamic traffic assignment for the evening commute.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/BXTINP8V/Daganzo - 1995 - The cell transmission model, part II Network traf.pdf;/home/jeroen/zotero/storage/2IN935KE/019126159400022R.html}
}

@article{daiQueueingNetworkControls2022,
  title = {Queueing {{Network Controls}} via {{Deep Reinforcement Learning}}},
  author = {Dai, J. G. and Gluzman, Mark},
  year = {2022},
  month = mar,
  journal = {Stochastic Systems},
  volume = {12},
  number = {1},
  pages = {30--67},
  publisher = {{INFORMS}},
  issn = {1946-5238},
  doi = {10.1287/stsy.2021.0081},
  urldate = {2023-09-24},
  abstract = {Novel advanced policy gradient (APG) methods, such as trust region policy optimization and proximal policy optimization (PPO), have become the dominant reinforcement learning algorithms because of their ease of implementation and good practical performance. A conventional setup for notoriously difficult queueing network control problems is a Markov decision problem (MDP) that has three features: infinite state space, unbounded costs, and long-run average cost objective. We extend the theoretical framework of these APG methods for such MDP problems. The resulting PPO algorithm is tested on a parallel-server system and large-size multiclass queueing networks. The algorithm consistently generates control policies that outperform state-of-art heuristics in literature in a variety of load conditions from light to heavy traffic. These policies are demonstrated to be near optimal when the optimal policy can be computed. A key to the successes of our PPO algorithm is the use of three variance reduction techniques in estimating the relative value function via sampling. First, we use a discounted relative value function as an approximation of the relative value function. Second, we propose regenerative simulation to estimate the discounted relative value function. Finally, we incorporate the approximating martingale-process method into the regenerative estimator.},
  keywords = {control variate,long-run average cost,multiclass queueing network,reinforcement learning},
  file = {/home/jeroen/zotero/storage/IR7QHQGI/Dai and Gluzman - 2022 - Queueing Network Controls via Deep Reinforcement L.pdf}
}

@article{darrochTrafficlightQueue1964,
  title = {On the {{Traffic-light Queue}}},
  author = {Darroch, J. N.},
  year = {1964},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {35},
  number = {1},
  pages = {380--388},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177703761},
  urldate = {2023-01-10},
  langid = {english},
  file = {/home/jeroen/zotero/storage/IPWE9LMT/Darroch - 1964 - On the Traffic-light Queue.pdf}
}

@article{dealmeidaMultiagentReinforcementLearning,
  title = {Multiagent {{Reinforcement Learning}} for {{Traffic Signal Control}}: A k-{{Nearest Neighbors Based Approach}}},
  author = {{de Almeida}, Vicente N and Bazzan, Ana L C and Abdoos, Monireh},
  abstract = {The increasing demand for mobility in our society poses various challenges to traffic engineering, computer science in general, and artificial intelligence in particular. As it is often the case, it is not possible to increase the capacity of road networks, therefore a more efficient use of the available transportation infrastructure is required. This relates closely to multiagent systems and to multiagent reinforcement learning, as many problems in traffic management and control are inherently distributed. However, one of the main challenges of this domain is that the state space is large and continuous, which makes it difficult to properly discretize states and also causes many RL algorithms to converge more slowly. To address these issues, a multiagent system with agents learning independently via a temporal difference learning algorithm based on k-nearest neighbors is presented as an option to control traffic signals in real-time. Our results show that the proposed method is both effective (reduces the average waiting time of vehicles in a traffic network) and efficient (the learning task is significantly accelerated), when compared to a baseline reported in the literature.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/AZ4W2BV2/de Almeida et al. - Multiagent Reinforcement Learning for Traffic Sign.pdf}
}

@article{el-tantawyDesignReinforcementLearning2014,
  title = {Design of {{Reinforcement Learning Parameters}} for {{Seamless Application}} of {{Adaptive Traffic Signal Control}}},
  author = {{El-Tantawy}, Samah and Abdulhai, Baher and Abdelgawad, Hossam},
  year = {2014},
  month = jul,
  journal = {Journal of Intelligent Transportation Systems},
  volume = {18},
  number = {3},
  pages = {227--245},
  issn = {1547-2450, 1547-2442},
  doi = {10.1080/15472450.2013.810991},
  urldate = {2023-01-06},
  langid = {english},
  file = {/home/jeroen/zotero/storage/34WWS5TM/El-Tantawy et al. - 2014 - Design of Reinforcement Learning Parameters for Se.pdf}
}

@article{el-tantawyMultiagentReinforcementLearning2013,
  title = {Multiagent {{Reinforcement Learning}} for {{Integrated Network}} of {{Adaptive Traffic Signal Controllers}} ({{MARLIN-ATSC}}): {{Methodology}} and {{Large-Scale Application}} on {{Downtown Toronto}}},
  shorttitle = {Multiagent {{Reinforcement Learning}} for {{Integrated Network}} of {{Adaptive Traffic Signal Controllers}} ({{MARLIN-ATSC}})},
  author = {{El-Tantawy}, Samah and Abdulhai, Baher and Abdelgawad, Hossam},
  year = {2013},
  month = sep,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {14},
  number = {3},
  pages = {1140--1150},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2013.2255286},
  urldate = {2023-01-06},
  file = {/home/jeroen/zotero/storage/7MIEKXXA/El-Tantawy et al. - 2013 - Multiagent Reinforcement Learning for Integrated N.pdf}
}

@article{fischettiDeepNeuralNetworks2018,
  title = {Deep Neural Networks and Mixed Integer Linear Optimization},
  author = {Fischetti, Matteo and Jo, Jason},
  year = {2018},
  month = jul,
  journal = {Constraints},
  volume = {23},
  number = {3},
  pages = {296--309},
  issn = {1572-9354},
  doi = {10.1007/s10601-018-9285-6},
  urldate = {2023-09-27},
  abstract = {Deep Neural Networks (DNNs) are very popular these days, and are the subject of a very intense investigation. A DNN is made up of layers of internal units (or neurons), each of which computes an affine combination of the output of the units in the previous layer, applies a nonlinear operator, and outputs the corresponding value (also known as activation). A commonly-used nonlinear operator is the so-called rectified linear unit (ReLU), whose output is just the maximum between its input value and zero. In this (and other similar cases like max pooling, where the max operation involves more than one input value), for fixed parameters one can model the DNN as a 0-1 Mixed Integer Linear Program (0-1 MILP) where the continuous variables correspond to the output values of each unit, and a binary variable is associated with each ReLU to model its yes/no nature. In this paper we discuss the peculiarity of this kind of 0-1 MILP models, and describe an effective bound-tightening technique intended to ease its solution. We also present possible applications of the 0-1 MILP model arising in feature visualization and in the construction of adversarial examples. Computational results are reported, aimed at investigating (on small DNNs) the computational performance of a state-of-the-art MILP solver when applied to a known test case, namely, hand-written digit recognition.},
  langid = {english},
  keywords = {Computational experiments,Deep learning,Deep neural networks,Mathematical optimization,Mixed-integer programming},
  file = {/home/jeroen/zotero/storage/3IUMWN5Z/Fischetti and Jo - 2018 - Deep neural networks and mixed integer linear opti.pdf}
}

@article{fleurenOptimizingPretimedControl2017,
  title = {Optimizing Pre-Timed Control at Isolated Intersections},
  author = {Fleuren, Stijn},
  year = {2017},
  urldate = {2023-01-06},
  keywords = {MILP},
  file = {/home/jeroen/zotero/storage/JFULKAUB/20170119_Fleuren.pdf}
}

@article{garciaComprehensiveSurveySafe,
  title = {A {{Comprehensive Survey}} on {{Safe Reinforcement Learning}}},
  author = {Garc{\i}a, Javier and Fernandez, Fernando},
  pages = {44},
  abstract = {Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/H467LGLD/Garca and Fernandez - A Comprehensive Survey on Safe Reinforcement Learn.pdf}
}

@article{ghanadbashiUsingOntologyGuide2022,
  title = {Using Ontology to Guide Reinforcement Learning Agents in Unseen Situations: {{A}} Traffic Signal Control System Case Study},
  shorttitle = {Using Ontology to Guide Reinforcement Learning Agents in Unseen Situations},
  author = {Ghanadbashi, Saeedeh and Golpayegani, Fatemeh},
  year = {2022},
  month = jan,
  journal = {Applied Intelligence},
  volume = {52},
  number = {2},
  pages = {1808--1824},
  issn = {0924-669X, 1573-7497},
  doi = {10.1007/s10489-021-02449-5},
  urldate = {2022-12-21},
  abstract = {In multi-agent systems, goal achievement is challenging when agents operate in ever-changing environments and face unseen situations, where not all the goals are known or predefined. In such cases, agents need to identify the changes and adapt their behaviour, by evolving their goals or even generating new goals to address the emerging requirements. Learning and practical reasoning techniques have been used to enable agents with limited knowledge to adapt to new circumstances. However, they depend on the availability of large amounts of data, require long exploration periods, and cannot help agents to set new goals. Furthermore, the accuracy of agents' actions is improved by introducing added intelligence through integrating conceptual features extracted from ontologies. However, the concerns related to taking suitable actions when unseen situations occur are not addressed. This paper proposes a new Automatic Goal Generation Model (AGGM) that enables agents to create new goals to handle unseen situations and to adapt to their ever-changing environment on a real-time basis. AGGM is compared to Q-learning, SARSA, and Deep Q Network in a Traffic Signal Control System case study. The results show that AGGM outperforms the baseline algorithms in unseen situations while handling the seen situations as well as the baseline algorithms.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/ABJVYHG8/Ghanadbashi and Golpayegani - 2022 - Using ontology to guide reinforcement learning age.pdf}
}

@article{ghavamzadehBayesianReinforcementLearning2015,
  title = {Bayesian {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Bayesian {{Reinforcement Learning}}},
  author = {Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
  year = {2015},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {8},
  number = {5-6},
  eprint = {1609.04436},
  primaryclass = {cs, stat},
  pages = {359--483},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000049},
  urldate = {2022-12-08},
  abstract = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/8MG2AF4R/Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf}
}

@article{glaubiusRealTimeSchedulingReinforcement,
  title = {Real-{{Time Scheduling}} via {{Reinforcement Learning}}},
  author = {Glaubius, Robert and Tidwell, Terry and Gill, Christopher and Smart, William D},
  abstract = {Cyber-physical systems, such as mobile robots, must respond adaptively to dynamic operating conditions. Effective operation of these systems requires that sensing and actuation tasks are performed in a timely manner. Additionally, execution of mission specific tasks such as imaging a room must be balanced against the need to perform more general tasks such as obstacle avoidance. This problem has been addressed by maintaining relative utilization of shared resources among tasks near a user-specified target level. Producing optimal scheduling strategies requires complete prior knowledge of task behavior, which is unlikely to be available in practice. Instead, suitable scheduling strategies must be learned online through interaction with the system. We consider the sample complexity of reinforcement learning in this domain, and demonstrate that while the problem state space is countably infinite, we may leverage the problem's structure to guarantee efficient learning.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/GXTLU4S6/Glaubius et al. - Real-Time Scheduling via Reinforcement Learning.pdf}
}

@incollection{grahamOptimizationApproximationDeterministic1979,
  title = {Optimization and {{Approximation}} in {{Deterministic Sequencing}} and {{Scheduling}}: A {{Survey}}},
  shorttitle = {Optimization and {{Approximation}} in {{Deterministic Sequencing}} and {{Scheduling}}},
  booktitle = {Annals of {{Discrete Mathematics}}},
  author = {Graham, R. L. and Lawler, E. L. and Lenstra, J. K. and Kan, A. H. G. Rinnooy},
  editor = {Hammer, P. L. and Johnson, E. L. and Korte, B. H.},
  year = {1979},
  month = jan,
  series = {Discrete {{Optimization II}}},
  volume = {5},
  pages = {287--326},
  publisher = {{Elsevier}},
  doi = {10.1016/S0167-5060(08)70356-X},
  urldate = {2023-10-23},
  abstract = {The theory of deterministic sequencing and scheduling has expanded rapidly during the past years. In this paper we survey the state of the art with respect to optimization and approximation algorithms and interpret these in terms of computational complexity theory. Special cases considered are single machine scheduling, identical, uniform and unrelated parallel machine scheduling, and open shop, flow shop and job shop scheduling. We indicate some problems for future research and include a selective bibliography.},
  file = {/home/jeroen/zotero/storage/6INJSUCP/Graham et al. - 1979 - Optimization and Approximation in Deterministic Se.pdf}
}

@article{greenshields1934a,
  title = {The Photographic Method of Studying Traffic Behavior},
  author = {Greenshields, B.D.},
  year = {1934},
  journal = {Proceedings of the 13th annual meeting of the highway research board},
  pages = {382--399},
  langid = {english}
}

@article{greenshields1935a,
  title = {A Study of Traffic Capacity},
  author = {Greenshields, B.D. and Bibbins, J.R. and Channing, W.S. and Miller, H.H.},
  year = {1935},
  journal = {Proceedings of the 14th annual meeting of the highway research board},
  pages = {448--477},
  langid = {english}
}

@article{grootenDeepReinforcementLearning2021,
  title = {Deep {{Reinforcement Learning}} for the Cooperative Card Game {{Hanabi}}},
  author = {Grooten, Bram},
  year = {2021},
  month = sep,
  langid = {english},
  file = {/home/jeroen/zotero/storage/H8M7CX2Y/Grooten - Deep Reinforcement Learning for the cooperative ca.pdf}
}

@misc{hanResearchAdaptiveJob,
  title = {Research on {{Adaptive Job Shop Scheduling Problems Based}} on {{Dueling Double DQN}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  author = {Han, Bao-An and Yang, Jian-Jun},
  urldate = {2023-10-14},
  howpublished = {https://ieeexplore.ieee.org/document/9218934?denied=}
}

@article{haydariDeepReinforcementLearning2022,
  title = {Deep {{Reinforcement Learning}} for {{Intelligent Transportation Systems}}: {{A Survey}}},
  shorttitle = {Deep {{Reinforcement Learning}} for {{Intelligent Transportation Systems}}},
  author = {Haydari, Ammar and Yilmaz, Yasin},
  year = {2022},
  month = jan,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {1},
  pages = {11--32},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2020.3008612},
  urldate = {2023-01-20},
  abstract = {Latest technological improvements increased the quality of transportation. New data-driven approaches bring out a new research direction for all control-based systems, e.g., in transportation, robotics, IoT and power systems. Combining data-driven applications with transportation systems plays a key role in recent transportation applications. In this paper, the latest deep reinforcement learning (RL) based traffic control applications are surveyed. Specifically, traffic signal control (TSC) applications based on (deep) RL, which have been studied extensively in the literature, are discussed in detail. Different problem formulations, RL parameters, and simulation environments for TSC are discussed comprehensively. In the literature, there are also several autonomous driving applications studied with deep RL models. Our survey extensively summarizes existing works in this field by categorizing them with respect to application types, control models and studied algorithms. In the end, we discuss the challenges and open questions regarding deep RL-based transportation applications.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/ZAD9J8I2/Haydari and Yilmaz - 2022 - Deep Reinforcement Learning for Intelligent Transp.pdf}
}

@article{hegemanandIntersectionControlFuture,
  title = {Intersection {{Control}} of the Future},
  author = {Hegemanand, Toon},
  langid = {english},
  keywords = {MILP},
  file = {/home/jeroen/zotero/storage/AU58ZIKR/Hegemanand - Intersection Control of the future.pdf}
}

@misc{heOnlinePlanningPOMDPs2022,
  title = {Online {{Planning}} in {{POMDPs}} with {{Self-Improving Simulators}}},
  author = {He, Jinke and Suau, Miguel and Baier, Hendrik and Kaisers, Michael and Oliehoek, Frans A.},
  year = {2022},
  month = dec,
  number = {arXiv:2201.11404},
  eprint = {2201.11404},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.11404},
  urldate = {2023-09-27},
  abstract = {How can we plan efficiently in a large and complex environment when the time budget is limited? Given the original simulator of the environment, which may be computationally very demanding, we propose to learn online an approximate but much faster simulator that improves over time. To plan reliably and efficiently while the approximate simulator is learning, we develop a method that adaptively decides which simulator to use for every simulation, based on a statistic that measures the accuracy of the approximate simulator. This allows us to use the approximate simulator to replace the original simulator for faster simulations when it is accurate enough under the current context, thus trading off simulation speed and accuracy. Experimental results in two large domains show that when integrated with POMCP, our approach allows to plan with improving efficiency over time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/jeroen/zotero/storage/6CY9E44N/He et al. - 2022 - Online Planning in POMDPs with Self-Improving Simu.pdf;/home/jeroen/zotero/storage/427D6ZVR/2201.html}
}

@article{hePAMSCODPlatoonbasedArterial2012,
  title = {{{PAMSCOD}}: {{Platoon-based}} Arterial Multi-Modal Signal Control with Online Data},
  shorttitle = {{{PAMSCOD}}},
  author = {He, Qing and Head, K. Larry and Ding, Jun},
  year = {2012},
  month = feb,
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {20},
  number = {1},
  pages = {164--184},
  issn = {0968090X},
  doi = {10.1016/j.trc.2011.05.007},
  urldate = {2023-01-20},
  abstract = {A unified platoon-based mathematical formulation called PAMSCOD is presented to perform arterial (network) traffic signal control while considering multiple travel modes in a vehicle-to-infrastructure communications environment. First, a headway-based platoon recognition algorithm is developed to identify pseudo-platoons given probe vehicles' online information. It is assumed that passenger vehicles constitute a significant majority of the vehicles in the network. This algorithm identifies existing queues and significant platoons approaching each intersection. Second, a mixed-integer linear program (MILP) is solved to determine future optimal signal plans based on the current traffic controller status, online platoon data and priority requests from special vehicles, such as transit buses. Deviating from the traditional common network cycle length, PAMSCOD aims to provide multi-modal dynamical progression (MDP) on the arterial based on the probe information. Microscopic simulation using VISSIM shows that PAMSCOD can easily handle two common traffic modes, transit buses and automobiles, and significantly reduce delays for both modes under both non-saturated and oversaturated traffic conditions as compared to traditional state-of-practice coordinated-actuated signal control with timings optimized by SYNCHRO.},
  langid = {english},
  keywords = {MILP},
  file = {/home/jeroen/zotero/storage/FNICQCLU/He et al. - 2012 - PAMSCOD Platoon-based arterial multi-modal signal.pdf}
}

@article{hoogendoornModelbasedStochasticControl,
  title = {Model-Based {{Stochastic Control}} of {{Traffic Networks}}},
  author = {Hoogendoorn, S P and Knoop, V L and {van Zuylen}, H J},
  abstract = {Uncertainty of traffic network operations has been a subject of lively debate in the last decade. However, little efforts have been put in developing control frameworks that are not only aimed at improving the mean performance of the system, but also at improving the system robustness and reliability. In fact, it can be argued that most of the current control approaches are only aimed at improving the efficiency, which can even be counterproductive from a robustness point of view.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/LZMGC6NL/Hoogendoorn et al. - Model-based Stochastic Control of Traffic Networks.pdf}
}

@article{hullermeierAleatoricEpistemicUncertainty2021,
  title = {Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods},
  shorttitle = {Aleatoric and Epistemic Uncertainty in Machine Learning},
  author = {H{\"u}llermeier, Eyke and Waegeman, Willem},
  year = {2021},
  month = mar,
  journal = {Machine Learning},
  volume = {110},
  number = {3},
  pages = {457--506},
  issn = {1573-0565},
  doi = {10.1007/s10994-021-05946-3},
  urldate = {2023-10-13},
  abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
  langid = {english},
  keywords = {Bayesian inference,Calibration,Conformal prediction,Credal sets and classifiers,Deep neural networks,Ensembles,Epistemic uncertainty,Gaussian processes,Generative models,Likelihood-based methods,Probability,Set-valued prediction,Uncertainty,Version space learning},
  file = {/home/jeroen/zotero/storage/JUPP7XHF/Hllermeier and Waegeman - 2021 - Aleatoric and epistemic uncertainty in machine lea.pdf}
}

@misc{jannerSequenceModelingSolutions2021,
  title = {Sequence {{Modeling Solutions}} for {{Reinforcement Learning Problems}}},
  author = {Janner, Michael},
  year = {2021},
  month = nov
}

@book{jaynes2003probability,
  title = {Probability Theory: {{The}} Logic of Science},
  author = {Jaynes, Edwin T},
  year = {2003},
  publisher = {{Cambridge university press}}
}

@inproceedings{jinCommonStructuresResource2019,
  title = {Common {{Structures}} in {{Resource Management}} as {{Driver}} for {{Reinforcement Learning}}: {{A Survey}} and {{Research Tracks}}},
  shorttitle = {Common {{Structures}} in {{Resource Management}} as {{Driver}} for {{Reinforcement Learning}}},
  booktitle = {Machine {{Learning}} for {{Networking}}},
  author = {Jin, Yue and Kostadinov, Dimitre and Bouzid, Makram and Aghasaryan, Armen},
  editor = {Renault, {\'E}ric and M{\"u}hlethaler, Paul and Boumerdassi, Selma},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {117--132},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-19945-6_8},
  abstract = {In the era of growing digitalization, dynamic resource management becomes one of the critical problems in many application fields where, due to the permanently evolving environment, the trade-off between cost and system performance needs to be continuously adapted. While traditional approaches based on prior system specification or model learning are challenged by the complexity and the dynamicity of these systems, a new paradigm of learning in interaction brings a strong promise - based on the toolset of model-free Reinforcement Learning (RL) and its great success stories in various domains. However, current RL methods still struggle to learn rapidly in incremental, online settings, which is a barrier to deal with many practical problems. To address the slow convergence issue, one approach consists in exploiting the system's structural properties, instead of acting in full model-free mode. In this paper, we review the existing resource management systems and unveil their common structural properties. We propose a meta-model and discuss the tracks on how these properties can enhance general purpose RL algorithms.},
  isbn = {978-3-030-19945-6},
  langid = {english},
  keywords = {Capacity management,Learning through interactions,Resource management,RL},
  file = {/home/jeroen/zotero/storage/TBHHD5TB/Jin et al. - 2019 - Common Structures in Resource Management as Driver.pdf}
}

@misc{kaelblingReinforcementLearningSurvey1996a,
  title = {Reinforcement {{Learning}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning}}},
  author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
  year = {1996},
  month = apr,
  number = {arXiv:cs/9605103},
  eprint = {cs/9605103},
  publisher = {{arXiv}},
  urldate = {2023-09-27},
  abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/jeroen/zotero/storage/L7PTKLKD/Kaelbling et al. - 1996 - Reinforcement Learning A Survey.pdf;/home/jeroen/zotero/storage/LASUXHLE/9605103.html}
}

@article{kellyCitygenInteractiveSystem,
  title = {Citygen: {{An Interactive System}} for {{Procedural City Generation}}},
  author = {Kelly, George and McCabe, Hugh},
  abstract = {Contemporary 3D games are often situated within large urban environments. This necessitates a time-consuming and expensive content creation process involving the modelling of vast amounts of geometric detail: including terrain, roads, buildings, and other associated features. We present a system called Citygen that aims to automate as much of this as possible by employing procedural generation methods to rapidly create the urban geometry typical of a modern city. Procedural methods have long been used within the graphics and game development communities to generate natural phenomena such as plants and trees. We employ these methods to generate the underlying road networks that form the structure of cities and urban neighbourhoods. These road networks are automatically mapped to any supplied terrain model, and adapt themselves to the specific geometry of the underlying terrain. Building footprints are automatically extracted from the resulting model and buildings can then be inserted either procedurally or by hand. Our system is unique in that it is designed to allow developers hands-on interactive control over the generation process. We achieve this by providing an interface allowing the user to directly manipulate geometric elements such as road intersection nodes, and to directly control and specify many aspects of the procedural generation. The results are updated in real time, thus facilitating an interactive design process.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/ME84A9YC/Kelly and McCabe - Citygen An Interactive System for Procedural City.pdf}
}

@article{kellySurveyProceduralTechniques2006,
  title = {A {{Survey}} of {{Procedural Techniques}} for {{City Generation}}},
  author = {Kelly, George and McCabe, Hugh},
  year = {2006},
  month = jan,
  volume = {14},
  doi = {10.21427/D76M9P},
  abstract = {The computer game industry requires a skilled workforce and this combined with the complexity of modern games, means that production costs are extremely high. One of the most time consuming aspects is the creation of game geometry, the virtual world which the players inhabit. Procedural techniques have been used within computer graphics to create natural textures, simulate special effects and generate complex natural models including trees and waterfalls. It is these procedural techniques that we intend to harness to generate geometry and textures suitable for a game situated in an urban environment. Procedural techniques can provide many benefits for computer graphics applications when the correct algorithm is used. An overview of several commonly used procedural techniques including fractals, L-systems, Perlin noise, tiling systems and cellular basis is provided. The function of each technique and the resulting output they create are discussed to better understand their characteristics, benefits and relevance to the city generation problem. City generation is the creation of an urban area which necessitates the creation of buildings, situated along streets and arranged in appropriate patterns. Some research has already taken place into recreating road network patterns and generating buildings that can vary in function and architectural style. We will study the main body of existing research into procedural city generation and provide an overview of their implementations and a critique of their functionality and results. Finally we present areas in which further research into the generation of cities is required and outline our research goals for city generation.}
}

@article{kellySurveyProceduralTechniques2006a,
  title = {A {{Survey}} of {{Procedural Techniques}} for {{City Generation}}},
  author = {Kelly, George and McCabe, Hugh},
  year = {2006},
  month = jan,
  volume = {14},
  doi = {10.21427/D76M9P},
  abstract = {The computer game industry requires a skilled workforce and this combined with the complexity of modern games, means that production costs are extremely high. One of the most time consuming aspects is the creation of game geometry, the virtual world which the players inhabit. Procedural techniques have been used within computer graphics to create natural textures, simulate special effects and generate complex natural models including trees and waterfalls. It is these procedural techniques that we intend to harness to generate geometry and textures suitable for a game situated in an urban environment. Procedural techniques can provide many benefits for computer graphics applications when the correct algorithm is used. An overview of several commonly used procedural techniques including fractals, L-systems, Perlin noise, tiling systems and cellular basis is provided. The function of each technique and the resulting output they create are discussed to better understand their characteristics, benefits and relevance to the city generation problem. City generation is the creation of an urban area which necessitates the creation of buildings, situated along streets and arranged in appropriate patterns. Some research has already taken place into recreating road network patterns and generating buildings that can vary in function and architectural style. We will study the main body of existing research into procedural city generation and provide an overview of their implementations and a critique of their functionality and results. Finally we present areas in which further research into the generation of cities is required and outline our research goals for city generation.}
}

@article{kellySurveyProceduralTechniques2006b,
  title = {A {{Survey}} of {{Procedural Techniques}} for {{City Generation}}},
  author = {Kelly, George},
  year = {2006},
  publisher = {{Dublin Institute of Technology}},
  doi = {10.21427/D76M9P},
  urldate = {2023-02-11},
  abstract = {The computer game industry requires a skilled workforce and this combined with the complexity of modern games, means that production costs are extremely high. One of the most time consuming aspects is the creation of game geometry, the virtual world which the players inhabit. Procedural techniques have been used within computer graphics to create natural textures, simulate special effects and generate complex natural models including trees and waterfalls. It is these procedural techniques that we intend to harness to generate geometry and textures suitable for a game situated in an urban environment. Procedural techniques can provide many benefits for computer graphics applications when the correct algorithm is used. An overview of several commonly used procedural techniques including fractals, L-systems, Perlin noise, tiling systems and cellular basis is provided. The function of each technique and the resulting output they create are discussed to better understand their characteristics, benefits and relevance to the city generation problem. City generation is the creation of an urban area which necessitates the creation of buildings, situated along streets and arranged in appropriate patterns. Some research has already taken place into recreating road network patterns and generating buildings that can vary in function and architectural style. We will study the main body of existing research into procedural city generation and provide an overview of their implementations and a critique of their functionality and results. Finally we present areas in which further research into the generation of cities is required and outline our research goals for city generation.},
  copyright = {Creative Commons Attribution-Noncommercial-Share Alike 3.0 License},
  langid = {english},
  file = {/home/jeroen/zotero/storage/S3ILDLCP/Kelly - 2006 - A Survey of Procedural Techniques for City Generat.pdf}
}

@article{kellySurveyProceduralTechniques2006c,
  title = {A {{Survey}} of {{Procedural Techniques}} for {{City Generation}}},
  author = {Kelly, George},
  year = {2006},
  publisher = {{Dublin Institute of Technology}},
  doi = {10.21427/D76M9P},
  urldate = {2023-02-11},
  abstract = {The computer game industry requires a skilled workforce and this combined with the complexity of modern games, means that production costs are extremely high. One of the most time consuming aspects is the creation of game geometry, the virtual world which the players inhabit. Procedural techniques have been used within computer graphics to create natural textures, simulate special effects and generate complex natural models including trees and waterfalls. It is these procedural techniques that we intend to harness to generate geometry and textures suitable for a game situated in an urban environment. Procedural techniques can provide many benefits for computer graphics applications when the correct algorithm is used. An overview of several commonly used procedural techniques including fractals, L-systems, Perlin noise, tiling systems and cellular basis is provided. The function of each technique and the resulting output they create are discussed to better understand their characteristics, benefits and relevance to the city generation problem. City generation is the creation of an urban area which necessitates the creation of buildings, situated along streets and arranged in appropriate patterns. Some research has already taken place into recreating road network patterns and generating buildings that can vary in function and architectural style. We will study the main body of existing research into procedural city generation and provide an overview of their implementations and a critique of their functionality and results. Finally we present areas in which further research into the generation of cities is required and outline our research goals for city generation.},
  copyright = {Creative Commons Attribution-Noncommercial-Share Alike 3.0 License},
  langid = {english},
  file = {/home/jeroen/zotero/storage/PIKLABAL/Kelly - 2006 - A Survey of Procedural Techniques for City Generat.pdf}
}

@misc{kidambiMOReLModelBasedOffline2021,
  title = {{{MOReL}} : {{Model-Based Offline Reinforcement Learning}}},
  shorttitle = {{{MOReL}}},
  author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  year = {2021},
  month = mar,
  number = {arXiv:2005.05951},
  eprint = {2005.05951},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-12-13},
  abstract = {In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. The ability to train RL policies offline can greatly expand the applicability of RL, its data efficiency, and its experimental velocity. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; and (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP. This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied offline RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g. generative modeling, uncertainty estimation, planning etc.) to directly translate into advances for offline RL.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/IV2THQ4G/Kidambi et al. - 2021 - MOReL  Model-Based Offline Reinforcement Learning.pdf;/home/jeroen/zotero/storage/SGVCJAZB/2005.html}
}

@article{kimProceduralCityGeneration2018,
  title = {Procedural City Generation beyond Game Development},
  author = {Kim, Joon-Seok and Kavak, Hamdi and Crooks, Andrew},
  year = {2018},
  month = nov,
  journal = {SIGSPATIAL Special},
  volume = {10},
  number = {2},
  pages = {34--41},
  doi = {10.1145/3292390.3292397},
  urldate = {2023-02-11},
  abstract = {The common trend in the scientific inquiry of urban areas and their populations is to use real-world geographic and population data to understand, explain, and predict urban phenomena. We argue that this trend limits our understanding of urban areas as dealing with arbitrarily collected geographic data requires technical expertise to process; moreover, population data is often aggregated, sparsified, or anonymized for privacy reasons. We believe synthetic urban areas generated via procedural city generation, which is a technique mostly used in the gaming area, could help improve the state-of-the-art in many disciplines which study urban areas. In this paper, we describe a selection of research areas that could benefit from such synthetic urban data and show that the current research in procedurally generated cities needs to address specific issues (e.g., plausibility) to sufficiently capture real-world cities and thus take such data beyond gaming.},
  file = {/home/jeroen/zotero/storage/ZGQAJ62E/Kim et al. - 2018 - Procedural city generation beyond game development.pdf}
}

@misc{krauss1998a,
  type = {{Tech. rept.}},
  title = {{Microscopic modeling of traffic flow: investigation of collision free vehicle dynamics}},
  author = {Krauss, S.},
  year = {1998},
  pages = {98--08},
  publisher = {{DLR Deutsches Zentrum fur Luft\textendash{} und Raumfahrt}},
  langid = {ngerman}
}

@inproceedings{kuyerMultiagentReinforcementLearning2008,
  title = {Multiagent {{Reinforcement Learning}} for {{Urban Traffic Control Using Coordination Graphs}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Kuyer, Lior and Whiteson, Shimon and Bakker, Bram and Vlassis, Nikos},
  editor = {Daelemans, Walter and Goethals, Bart and Morik, Katharina},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {656--671},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87479-9_61},
  abstract = {Since traffic jams are ubiquitous in the modern world, optimizing the behavior of traffic lights for efficient traffic flow is a critically important goal. Though most current traffic lights use simple heuristic protocols, more efficient controllers can be discovered automatically via multiagent reinforcement learning, where each agent controls a single traffic light. However, in previous work on this approach, agents select only locally optimal actions without coordinating their behavior. This paper extends this approach to include explicit coordination between neighboring traffic lights. Coordination is achieved using the max-plus algorithm, which estimates the optimal joint action by sending locally optimized messages among connected agents. This paper presents the first application of max-plus to a large-scale problem and thus verifies its efficacy in realistic settings. It also provides empirical evidence that max-plus performs well on cyclic graphs, though it has been proven to converge only for tree-structured graphs. Furthermore, it provides a new understanding of the properties a traffic network must have for such coordination to be beneficial and shows that max-plus outperforms previous methods on networks that possess those properties.},
  isbn = {978-3-540-87479-9},
  langid = {english},
  keywords = {coordination graphs,max-plus,multiagent systems,reinforcement learning,traffic control},
  file = {/home/jeroen/zotero/storage/V7P2JNL8/Kuyer et al. - 2008 - Multiagent Reinforcement Learning for Urban Traffi.pdf}
}

@article{laemmer2008a,
  title = {Self-Control of Traffic Lights and Vehicle Flows in Urban Road Networks},
  author = {L{\"a}mmer, S. and Helbing, D.},
  year = {2008},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2008},
  number = {04},
  langid = {english}
}

@article{lamorgeseExactDecompositionApproach2015,
  title = {An {{Exact Decomposition Approach}} for the {{Real-Time Train Dispatching Problem}}},
  author = {Lamorgese, Leonardo and Mannino, Carlo},
  year = {2015},
  month = feb,
  journal = {Operations Research},
  volume = {63},
  number = {1},
  pages = {48--64},
  publisher = {{INFORMS}},
  issn = {0030-364X},
  doi = {10.1287/opre.2014.1327},
  urldate = {2023-10-16},
  abstract = {Trains' movements on a railway network are regulated by official timetables. Deviations and delays occur quite often in practice, demanding fast rescheduling and rerouting decisions in order to avoid conflicts and minimize overall delay. This is the real-time train dispatching problem. In contrast with the classic ``holistic'' approach, we show how to decompose the problem into smaller subproblems associated with the line and the stations. This decomposition is the basis for a master-slave solution algorithm, in which the master problem is associated with the line and the slave problem is associated with the stations. The two subproblems are modeled as mixed integer linear programs, with specific sets of variables and constraints. Similarly to the classical Benders' decomposition approach, slave and master communicate through suitable feasibility cuts in the variables of the master. Extensive tests on real-life instances from single and double-track lines in Italy showed significant improvements over current dispatching performances. A decision support system based on this exact approach has been in operation in Norway since February 2014 and represents one of the first operative applications of mathematical optimization to train dispatching.},
  keywords = {discrete mathematics,railway optimization,train dispatching},
  file = {/home/jeroen/zotero/storage/K7SJ2Z57/Lamorgese and Mannino - 2015 - An Exact Decomposition Approach for the Real-Time .pdf}
}

@article{lamorgeseNoncompactFormulationJobShop2019,
  title = {A {{Noncompact Formulation}} for {{Job-Shop Scheduling Problems}} in {{Traffic Management}}},
  author = {Lamorgese, Leonardo and Mannino, Carlo},
  year = {2019},
  month = nov,
  journal = {Operations Research},
  volume = {67},
  number = {6},
  pages = {1586--1609},
  publisher = {{INFORMS}},
  issn = {0030-364X},
  doi = {10.1287/opre.2018.1837},
  urldate = {2023-10-12},
  abstract = {A central problem in traffic management is that of scheduling the movements of vehicles so as to minimize the cost of the schedule. It arises in important applications such as train timetabling, rescheduling, delay and disruption management, airplane surface routing, runway scheduling, air-traffic control, and more. This problem can be modeled as a job-shop scheduling problem. We introduce a new mixed-integer linear program (MILP) formulation for job-shop scheduling, which is an alternative to classical approaches, namely, big-M and time-indexed formulations. It does not make use of artificially large coefficients, and its constraints correspond to basic graph structures, such as paths, cycles, and trees. The new formulation can be obtained by strengthening and lifting the constraints of a classical Benders' reformulation. Tests on a large set of real-life instances from train rescheduling show that the new approach performs on average better than our previous approaches based on big-M formulations and particularly better on a class of instances with nonconvex costs very common in the practice.},
  keywords = {algorithms,Benders' decomposition,Integer programming,scheduling,Transportation,Transportation technology,{transportation: scheduling, vehicles}},
  file = {/home/jeroen/zotero/storage/HASPKTYU/Lamorgese and Mannino - 2019 - A Noncompact Formulation for Job-Shop Scheduling P.pdf}
}

@article{lamorgeseOptimalTrainDispatching2016,
  title = {Optimal {{Train Dispatching}} by {{Benders}}'-{{Like Reformulation}}},
  author = {Lamorgese, Leonardo and Mannino, Carlo and Piacentini, Mauro},
  year = {2016},
  month = aug,
  journal = {Transportation Science},
  volume = {50},
  number = {3},
  pages = {910--925},
  publisher = {{INFORMS}},
  issn = {0041-1655},
  doi = {10.1287/trsc.2015.0605},
  urldate = {2023-10-16},
  abstract = {Train movements on railway lines are generally controlled by human dispatchers. Because disruptions often occur, dispatchers make real-time scheduling and routing decisions in an attempt to minimize deviations from the official timetable. This optimization problem is called train dispatching. We represent it as a mixed integer linear programming model, and solve it with a Benders'-like decomposition within a suitable master/slave scheme. Interestingly, the master and the slave problems correspond to a macroscopic and microscopic representation of the railway, recently exploited in heuristic approaches to the problem. The decomposition, along with some new modeling ideas, allowed us to solve real-life instances of practical interest to optimality. Automatic dispatching systems based on our macro/micro decomposition\textemdash in which both master and slave are solved heuristically\textemdash have been in operation in several Italian lines since 2011. The exact approach described in this paper outperforms such systems on our test bed of real-life instances. Furthermore, a system based on another version of the exact decomposition approach has been in operation since February 2014 on a line in Norway.},
  keywords = {integer programming,logic Benders' decomposition,railway optimization},
  file = {/home/jeroen/zotero/storage/9NLH7JB7/Lamorgese et al. - 2016 - Optimal Train Dispatching by Benders-Like Reformu.pdf}
}

@misc{leBatchPolicyLearning2019,
  title = {Batch {{Policy Learning}} under {{Constraints}}},
  author = {Le, Hoang M. and Voloshin, Cameron and Yue, Yisong},
  year = {2019},
  month = mar,
  number = {arXiv:1903.08738},
  eprint = {1903.08738},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2022-12-13},
  abstract = {When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. To certify constraint satisfaction, we propose a new and simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/5XG8FNW3/Le et al. - 2019 - Batch Policy Learning under Constraints.pdf}
}

@incollection{lenstraComplexityMachineScheduling1977,
  title = {Complexity of {{Machine Scheduling Problems}}},
  booktitle = {Annals of {{Discrete Mathematics}}},
  author = {Lenstra, J. K. and Rinnooy Kan, A. H. G. and Brucker, P.},
  editor = {Hammer, P. L. and Johnson, E. L. and Korte, B. H. and Nemhauser, G. L.},
  year = {1977},
  month = jan,
  series = {Studies in {{Integer Programming}}},
  volume = {1},
  pages = {343--362},
  publisher = {{Elsevier}},
  doi = {10.1016/S0167-5060(08)70743-X},
  urldate = {2023-10-23},
  abstract = {We survey and extend the results on the complexity of machine scheduling problems. After a brief review of the central concept of NP-completeness we give a classification of scheduling problems on single, different and identical machines and study the influence of various parameters on their complexity. The problems for which a polynomial-bounded algorithm is available are listed and NP-completeness is established for a large number of other machine scheduling problems. We finally discuss some questions that remain unanswered.},
  file = {/home/jeroen/zotero/storage/NPNFKNAU/Lenstra et al. - 1977 - Complexity of Machine Scheduling Problems.pdf;/home/jeroen/zotero/storage/6NLB2NZ4/S016750600870743X.html}
}

@misc{levineOfflineReinforcementLearning2020,
  title = {Offline {{Reinforcement Learning}}: {{Tutorial}}, {{Review}}, and {{Perspectives}} on {{Open Problems}}},
  shorttitle = {Offline {{Reinforcement Learning}}},
  author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  year = {2020},
  month = nov,
  number = {arXiv:2005.01643},
  eprint = {2005.01643},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-12-08},
  abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/JQVHKDUG/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf}
}

@article{liConnectedVehicleBasedTraffic2020,
  title = {Connected {{Vehicle-Based Traffic Signal Coordination}}},
  author = {Li, Wan and Ban, Xuegang},
  year = {2020},
  month = dec,
  journal = {Engineering},
  volume = {6},
  number = {12},
  pages = {1463--1472},
  issn = {2095-8099},
  doi = {10.1016/j.eng.2020.10.009},
  urldate = {2023-09-25},
  abstract = {This study presents a connected vehicles (CVs)-based traffic signal optimization framework for a coordinated arterial corridor. The signal optimization and coordination problem are first formulated in a centralized scheme as a mixed-integer nonlinear program (MINLP). The optimal phase durations and offsets are solved together by minimizing fuel consumption and travel time considering an individual vehicle's trajectories. Due to the complexity of the model, we decompose the problem into two levels: an intersection level to optimize phase durations using dynamic programming (DP), and a corridor level to optimize the offsets of all intersections. In order to solve the two-level model, a prediction-based solution technique is developed. The proposed models are tested using traffic simulation under various scenarios. Compared with the traditional actuated signal timing and coordination plan, the signal timing plans generated by solving the MINLP and the two-level model can reasonably improve the signal control performance. When considering varies vehicle types under high demand levels, the proposed two-level model reduced the total system cost by 3.8\% comparing to baseline actuated plan. MINLP reduced the system cost by 5.9\%. It also suggested that coordination scheme was beneficial to corridors with relatively high demand levels. For intersections with major and minor street, coordination conducted for major street had little impacts on the vehicles at the minor street.},
  keywords = {Connected vehicles,Dynamic programming,MILP,Mixed-integer nonlinear program,Traffic signal coordination,Two-level optimization},
  file = {/home/jeroen/zotero/storage/2EAQX3JD/Li and Ban - 2020 - Connected Vehicle-Based Traffic Signal Coordinatio.pdf}
}

@article{liConnectedVehiclesBased2019,
  title = {Connected {{Vehicles Based Traffic Signal Timing Optimization}}},
  author = {Li, Wan and Ban, Xuegang},
  year = {2019},
  month = dec,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {20},
  number = {12},
  pages = {4354--4366},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2018.2883572},
  urldate = {2023-01-20},
  abstract = {We study the traffic signal control problem with connected vehicles by assuming a fixed cycle length so that the proposed model can be extended readily for the coordination of multiple signals. The problem can be first formulated as a mixed-integer nonlinear program, by considering the information of individual vehicle's trajectories (i.e., second-by-second vehicle locations and speeds) and their realistic driving/car-following behaviors. The objective function is to minimize the weighted sum of total fuel consumption and travel time. Due to the large dimension of the problem and the complexity of the nonlinear car-following model, solving the nonlinear program directly is challenging. We then reformulate the problem as a dynamic programming model by dividing the timing decisions into stages (one stage for a signal phase) and approximating the fuel consumption and travel time of a stage as functions of the state and decision variables of the stage. We also propose a twostep method to make sure that the obtained optimal solution can lead to the fixed cycle length. Numerical experiments are provided to test the performance of the proposed model using data generated by traffic simulation.},
  langid = {english},
  keywords = {MILP},
  file = {/home/jeroen/zotero/storage/CG5SBAC6/Li and Ban - 2019 - Connected Vehicles Based Traffic Signal Timing Opt.pdf}
}

@article{lighthill1955a,
  title = {On Kinematic Waves {{II}}. {{A}} Theory of Traffic Flow on Long Crowded Roads},
  author = {Lighthill, M.J. and Whitham, G.B.},
  year = {1955},
  journal = {Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences},
  volume = {229},
  number = {1178},
  pages = {317--345},
  langid = {english}
}

@misc{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  month = jul,
  number = {arXiv:1509.02971},
  eprint = {1509.02971},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1509.02971},
  urldate = {2023-01-20},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/AT28MPL2/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf;/home/jeroen/zotero/storage/FNSFETWC/1509.html}
}

@article{linSingleMachineScheduling2022,
  title = {Single Machine Scheduling Problems with Sequence-Dependent Setup Times and Precedence Delays},
  author = {Lin, Shih-Wei and Ying, Kuo-Ching},
  year = {2022},
  month = jun,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {9430},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-13278-y},
  urldate = {2023-10-20},
  abstract = {Sequence-dependent setup times and precedence delays occur frequently in various production environments. This study investigates the single machine scheduling problem with setup times and precedence delays that occur in an amplifier assembly company. This study proposes a novel mixed-integer linear programming model and a lean iterated greedy algorithm to minimize the makespan for this problem. Based on the property of delayed precedence constraints, the lean iterated greedy (LIG) algorithm uses a simple but effective lean construction mechanism that can discard infeasible solutions to reduce the waste of unnecessary searches and quickly converge to the (near) global optimum. The computational results show that LIG significantly outperforms the state-of-the-art algorithm in terms of solution quality and computational efficiency. This study mainly contributes to providing a simple, effective, and efficient algorithm that can facilitate industrial applications and serve as a new benchmark approach for future research.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Engineering,Mathematics and computing},
  file = {/home/jeroen/zotero/storage/5X5QHDPC/Lin and Ying - 2022 - Single machine scheduling problems with sequence-d.pdf}
}

@article{liPOINTPartiallyObservable2022,
  title = {{{POINT}}: {{Partially Observable Imitation Network}} for {{Traffic Signal Control}}},
  shorttitle = {{{POINT}}},
  author = {Li, Wan and Wang, Boyu and Liu, Zhanlin and Li, Qiang and Qi, Guo-Jun},
  year = {2022},
  month = jan,
  journal = {Sustainable Cities and Society},
  volume = {76},
  pages = {103461},
  issn = {2210-6707},
  doi = {10.1016/j.scs.2021.103461},
  urldate = {2023-01-20},
  abstract = {Smart traffic signals bring together transportation infrastructure and advance technologies to improve the mobility and efficiency of urban transportation network. Adaptive traffic signal control studies can be categorized into modeling-based approaches and learning-based approaches. In order to take advantages of these two systems, this study developed an offline-online combined Partial Observable Imitation Network for Traffic signal control (POINT). In the offline system, the traffic signal timing optimization problem was formulated as a Mixed Integer Nonlinear Programming (MINLP) given complete traffic information, i.e., second-by-second speeds and locations of all vehicles. The objective of MINLP is to minimize total travel delays considering individual vehicle trajectories under Connected Vehicle (CV) environment. The calculated optimal solutions under various traffic conditions were considered as the ''expert'' decisions. In the online system, an imitation neural network model was developed to learn the ''expert'' signal plans generated from offline system. Given partial observable traffic conditions in real time, e.g., the aggregate-level of traffic volume, the POINT model can compute the signal timing parameters in the online system. The numerical results demonstrated that the proposed method outperformed other state-of-the-art signal control method under high and unbalanced traffic demand levels in terms of reducing travel delays and queue length.},
  langid = {english},
  keywords = {Adaptive traffic signal control system,Connected vehicle,Imitation network,immitation learning,MILP,Vehicle trajectories},
  file = {/home/jeroen/zotero/storage/LXKSI7DN/Li et al. - 2022 - POINT Partially Observable Imitation Network for .pdf;/home/jeroen/zotero/storage/LRPEMQ3X/S2210670721007344.html}
}

@article{littleVersatileProgramSetting,
  title = {A {{Versatile Program}} for {{Setting Signals}} on {{Arteries}} and {{Triangular Networks}}},
  author = {Little, John D C and Kelson, Mark D and Gartner, Nathan H},
  langid = {english},
  file = {/home/jeroen/zotero/storage/5UZLGK52/Little et al. - A Versatile Program for Setting Signals on Arterie.pdf}
}

@inproceedings{liuPolicyLearningConstraints2021,
  title = {Policy {{Learning}} with {{Constraints}} in {{Model-free Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Policy {{Learning}} with {{Constraints}} in {{Model-free Reinforcement Learning}}},
  booktitle = {Proceedings of the {{Thirtieth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Liu, Yongshuai and Halev, Avishai and Liu, Xin},
  year = {2021},
  month = aug,
  pages = {4508--4515},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Montreal, Canada}},
  doi = {10.24963/ijcai.2021/614},
  urldate = {2022-12-08},
  abstract = {Reinforcement Learning (RL) algorithms have had tremendous success in simulated domains. These algorithms, however, often cannot be directly applied to physical systems, especially in cases where there are constraints to satisfy (e.g. to ensure safety or limit resource consumption). In standard RL, the agent is incentivized to explore any policy with the sole goal of maximizing reward; in the real world, however, ensuring satisfaction of certain constraints in the process is also necessary and essential. In this article, we overview existing approaches addressing constraints in model-free reinforcement learning. We model the problem of learning with constraints as a Constrained Markov Decision Process and consider two main types of constraints: cumulative and instantaneous. We summarize existing approaches and discuss their pros and cons. To evaluate policy performance under constraints, we introduce a set of standard benchmarks and metrics. We also summarize limitations of current methods and present open questions for future research.},
  isbn = {978-0-9992411-9-6},
  langid = {english},
  file = {/home/jeroen/zotero/storage/NLYBQNWC/Liu et al. - 2021 - Policy Learning with Constraints in Model-free Rei.pdf}
}

@inproceedings{lopezMicroscopicTrafficSimulation2018,
  title = {Microscopic {{Traffic Simulation}} Using {{SUMO}}},
  booktitle = {2018 21st {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Lopez, Pablo Alvarez and Wiessner, Evamarie and Behrisch, Michael and {Bieker-Walz}, Laura and Erdmann, Jakob and Flotterod, Yun-Pang and Hilbrich, Robert and Lucken, Leonhard and Rummel, Johannes and Wagner, Peter},
  year = {2018},
  month = nov,
  pages = {2575--2582},
  publisher = {{IEEE}},
  address = {{Maui, HI}},
  doi = {10.1109/ITSC.2018.8569938},
  urldate = {2023-01-10},
  isbn = {978-1-72810-321-1 978-1-72810-323-5},
  file = {/home/jeroen/zotero/storage/KV3FY2AH/Lopez et al. - 2018 - Microscopic Traffic Simulation using SUMO.pdf}
}

@article{manninoPathCycleFormulation2018,
  title = {The {{Path}}\&{{Cycle Formulation}} for the {{Hotspot Problem}} in {{Air Traffic Management}}},
  author = {Mannino, Carlo and Sartor, Giorgio},
  year = {2018},
  pages = {11 pages},
  publisher = {{Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany}},
  doi = {10.4230/OASICS.ATMOS.2018.14},
  urldate = {2023-10-12},
  abstract = {The Hotspot Problem in Air Traffic Management consists of optimally rescheduling a set of airplanes that are forecast to occupy an overcrowded region of the airspace, should they follow their original schedule. We first provide a MILP model for the Hotspot Problem using a standard big-M formulation. Then, we present a novel MILP model that gets rid of the big-M coefficients. The new formulation contains only simple combinatorial constraints, corresponding to paths and cycles in an associated disjunctive graph. We report computational results on a set of randomly generated instances. In the experiments, the new formulation consistently outperforms the big-M formulation, both in terms of running times and number of branching nodes.},
  collaborator = {Wagner, Michael},
  copyright = {Creative Commons Attribution 3.0 Unported license (CC-BY 3.0)},
  langid = {english},
  keywords = {{000 Computer science, knowledge, general works},Computer Science},
  file = {/home/jeroen/zotero/storage/PKMK754N/Mannino and Sartor - 2018 - The Path&amp;Cycle Formulation for the Hotspot Pro.pdf}
}

@article{marianiCoordinationAutonomousVehicles2022,
  title = {Coordination of {{Autonomous Vehicles}}: {{Taxonomy}} and {{Survey}}},
  shorttitle = {Coordination of {{Autonomous Vehicles}}},
  author = {Mariani, Stefano and Cabri, Giacomo and Zambonelli, Franco},
  year = {2022},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {1},
  pages = {1--33},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3431231},
  urldate = {2023-09-24},
  abstract = {In the near future, our streets will be populated by myriads of autonomous self-driving vehicles to serve our diverse mobility needs. This will raise the need to coordinate their movements in order to properly handle both access to shared resources (e.g., intersections and parking slots) and the execution of mobility tasks (e.g., platooning and ramp merging). The aim of this article is to provide a global view of the coordination issues and the related solutions in the field of autonomous vehicles. To this end, we firstly introduce the general problems associated with coordination of autonomous vehicles by identifying and framing the key classes of coordination problems. Then, we overview the different approaches that can be adopted to deal with such problems by classifying them in terms of the degree of autonomy in decision making that is left to autonomous vehicles during the coordination process. Finally, we overview some further research challenges to address before autonomous coordinated vehicles can safely hit our streets.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/TLS9NWCD/Mariani et al. - 2022 - Coordination of Autonomous Vehicles Taxonomy and .pdf}
}

@book{mcshaneTrafficEngineering1990,
  title = {Traffic Engineering},
  author = {McShane, William R. and Roess, Roger P.},
  year = {1990},
  series = {Prentice {{Hall}} Polytechnic Series in Traffic Engineering},
  publisher = {{Prentice-Hall}},
  address = {{Englewood Cliffs, N.J}},
  isbn = {978-0-13-926148-0},
  lccn = {HE355 .M43 1990},
  keywords = {Traffic engineering,United States}
}

@misc{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  number = {arXiv:1312.5602},
  eprint = {1312.5602},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-01-10},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/zotero/storage/9KEM54CE/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/jeroen/zotero/storage/THQC2DX6/1312.html}
}

@misc{moldovanSafeExplorationMarkov2012,
  title = {Safe {{Exploration}} in {{Markov Decision Processes}}},
  author = {Moldovan, Teodor Mihai and Abbeel, Pieter},
  year = {2012},
  month = jul,
  number = {arXiv:1205.4810},
  eprint = {1205.4810},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-12-08},
  abstract = {In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/zotero/storage/MBKV6JAT/Moldovan and Abbeel - 2012 - Safe Exploration in Markov Decision Processes.pdf}
}

@article{monchDistributedShiftingBottleneck2005,
  title = {A Distributed Shifting Bottleneck Heuristic for Complex Job Shops},
  author = {M{\"o}nch, Lars and Drie{\ss}el, Ren{\'e}},
  year = {2005},
  month = nov,
  journal = {Computers \& Industrial Engineering},
  volume = {49},
  number = {3},
  pages = {363--380},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2005.06.004},
  urldate = {2023-10-12},
  abstract = {In this paper, we consider distributed versions of a modified shifting bottleneck heuristic for complex job shops. The considered job shop environment contains parallel batching machines, machines with sequence-dependent setup times and reentrant process flows. Semiconductor wafer fabrication facilities are typical examples for manufacturing systems with these characteristics. The used performance measure is total weighted tardiness (TWT). We suggest a two-layer hierarchical approach in order to decompose the overall scheduling problem. The upper (or top) layer works on an aggregated model. Based on appropriately aggregated routes it determines start dates and planned due dates for the jobs within each single work area, where a work area is defined as a set of parallel machine groups. The lower (or base) layer uses the start dates and planned due dates in order to apply shifting bottleneck heuristic type solution approaches for the jobs in each single work area. We conduct simulation experiments in a dynamic job shop environment in order to assess the performance of the heuristic. It turns out that the suggested approach outperforms a pure First In First Out (FIFO) dispatching scheme and provides a similar solution quality as the original modified shifting bottleneck heuristic.},
  keywords = {Complex job shops,Computational experiments,Distributed scheduling,Hierarchical production control,Shifting bottleneck heuristic},
  file = {/home/jeroen/zotero/storage/EESQXWLU/Mnch and Drieel - 2005 - A distributed shifting bottleneck heuristic for co.pdf;/home/jeroen/zotero/storage/RPCP6CZG/S0360835205000793.html}
}

@article{morimuraNonparametricReturnDistribution,
  title = {Nonparametric {{Return Distribution Approximation}}  for {{Reinforcement Learning}}},
  author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
  pages = {8},
  abstract = {Standard Reinforcement Learning (RL) aims to optimize decision-making rules in terms of the expected return. However, especially for risk-management purposes, other criteria such as the expected shortfall are sometimes preferred. Here, we describe a method of approximating the distribution of returns, which allows us to derive various kinds of information about the returns. We first show that the Bellman equation, which is a recursive formula for the expected return, can be extended to the cumulative return distribution. Then we derive a nonparametric return distribution estimator with particle smoothing based on this extended Bellman equation. A key aspect of the proposed algorithm is to represent the recursion relation in the extended Bellman equation by a simple replacement procedure of particles associated with a state by using those of the successor state. We show that our algorithm leads to a risksensitive RL paradigm. The usefulness of the proposed approach is demonstrated through numerical experiments.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/ZF9RBJ2V/Morimura et al. - Nonparametric Return Distribution Approximation  f.pdf}
}

@misc{morimuraParametricReturnDensity2012,
  title = {Parametric {{Return Density Estimation}} for {{Reinforcement Learning}}},
  author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
  year = {2012},
  month = mar,
  number = {arXiv:1203.3497},
  eprint = {1203.3497},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-12-09},
  abstract = {Most conventional Reinforcement Learning (RL) algorithms aim to optimize decision-making rules in terms of the expected returns. However, especially for risk management purposes, other risk-sensitive criteria such as the value-at-risk or the expected shortfall are sometimes preferred in real applications. Here, we describe a parametric method for estimating density of the returns, which allows us to handle various criteria in a unified manner. We first extend the Bellman equation for the conditional expected return to cover a conditional probability density of the returns. Then we derive an extension of the TD-learning algorithm for estimating the return densities in an unknown environment. As test instances, several parametric density estimation algorithms are presented for the Gaussian, Laplace, and skewed Laplace distributions. We show that these algorithms lead to risk-sensitive as well as robust RL paradigms through numerical experiments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/RABI4SWB/Morimura et al. - 2012 - Parametric Return Density Estimation for Reinforce.pdf;/home/jeroen/zotero/storage/YFDU5G5F/1203.html}
}

@book{MultiAgentReinforcementLearning,
  title = {Multi-{{Agent Reinforcement Learning}}: {{Foundations}} and {{Modern Approaches}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  urldate = {2023-09-25},
  abstract = {Textbook published by MIT Press},
  langid = {english},
  file = {/home/jeroen/zotero/storage/J6TYKEA5/Multi-Agent Reinforcement Learning Foundations an.pdf;/home/jeroen/zotero/storage/6787IHI8/www.marl-book.com.html}
}

@misc{MultiAgentReinforcementLearninga,
  title = {Multi-{{Agent Reinforcement Learning}}: {{Foundations}} and {{Modern Approaches}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  urldate = {2023-09-27},
  abstract = {Textbook published by MIT Press},
  howpublished = {https://www.marl-book.com/},
  langid = {english},
  file = {/home/jeroen/zotero/storage/HXKEIUWE/www.marl-book.com.html}
}

@article{nagelCellularAutomatonModel1992,
  title = {A Cellular Automaton Model for Freeway Traffic},
  author = {Nagel, Kai and Schreckenberg, Michael},
  year = {1992},
  month = dec,
  journal = {Journal de Physique I},
  volume = {2},
  number = {12},
  pages = {2221--2229},
  issn = {1155-4304, 1286-4862},
  doi = {10.1051/jp1:1992277},
  urldate = {2023-01-10}
}

@misc{nicholPointESystemGenerating2022,
  title = {Point-{{E}}: {{A System}} for {{Generating 3D Point Clouds}} from {{Complex Prompts}}},
  shorttitle = {Point-{{E}}},
  author = {Nichol, Alex and Jun, Heewoo and Dhariwal, Prafulla and Mishkin, Pamela and Chen, Mark},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08751},
  eprint = {2212.08751},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-18},
  abstract = {While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jeroen/zotero/storage/EGY8QG6C/Nichol et al. - 2022 - Point-E A System for Generating 3D Point Clouds f.pdf;/home/jeroen/zotero/storage/ANINQA7V/2212.html}
}

@article{noaeenReinforcementLearningUrban2022,
  title = {Reinforcement Learning in Urban Network Traffic Signal Control: {{A}} Systematic Literature Review},
  shorttitle = {Reinforcement Learning in Urban Network Traffic Signal Control},
  author = {Noaeen, Mohammad and Naik, Atharva and Goodman, Liana and Crebo, Jared and Abrar, Taimoor and Abad, Zahra Shakeri Hossein and Bazzan, Ana L.C. and Far, Behrouz},
  year = {2022},
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {199},
  pages = {116830},
  issn = {09574174},
  doi = {10.1016/j.eswa.2022.116830},
  urldate = {2023-01-20},
  abstract = {Improvement of traffic signal control (TSC) efficiency has been found to lead to improved urban transportation and enhanced quality of life. Recently, the use of reinforcement learning (RL) in various areas of TSC has gained significant traction; thus, we conducted a systematic literature review as a systematic, comprehensive, and reproducible review to dissect all the existing research that applied RL in the network-level TSC domain, called as RL in NTSC or RL-NTSC for brevity. The review only targeted the network-level articles that tested the proposed methods in networks with two or more intersections. This review covers 160 peer-reviewed articles from 30 countries published from 1994 to March 2020. The goal of this study is to provide the research community with statistical and conceptual knowledge, summarize existence evidence, characterize RL applications in NTSC domains, explore all applied methods and major first events in the defined scope, and identify areas for further research based on the explored research problems in current research. We analyzed the extracted data from the included articles in the following seven categories: (i) publication and authors' data, (ii) method identification and analysis, (iii) environment attributes and traffic simulation, (iv) application domains of RL-NTSC, (v) major first events of RL-NTSC and authors' key statements, (vi) code availability, and (vii) evaluation. This paper provides a comprehensive view of the past 26 years of research on applying RL to NTSC. It also reveals the role of advancing deep learning methods in the revival of the research area, the rise of using non-commercial microscopic traffic simulators, a lack of interaction between traffic and transportation engineering practitioners and researchers, and a lack of proposal and creation of testbeds which can likely bring different communities together around common goals.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/37SRIMR9/Noaeen et al. - 2022 - Reinforcement learning in urban network traffic si.pdf}
}

@phdthesis{oblakovaQueueingModelsUrban2019,
  title = {Queueing Models for Urban Traffic Networks},
  author = {Oblakova, A.},
  year = {2019},
  month = sep,
  address = {{Enschede, The Netherlands}},
  doi = {10.3990/1.9789036548472},
  urldate = {2023-01-20},
  isbn = {9789036548472},
  langid = {english},
  school = {University of Twente},
  file = {/home/jeroen/zotero/storage/KSB4JAIW/Oblakova - 2019 - Queueing models for urban traffic networks.pdf}
}

@book{oliehoekConciseIntroductionDecentralized2016,
  title = {A {{Concise Introduction}} to {{Decentralized POMDPs}}},
  author = {Oliehoek, Frans A. and Amato, Christopher},
  year = {2016},
  series = {{{SpringerBriefs}} in {{Intelligent Systems}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-28929-8},
  urldate = {2023-09-26},
  isbn = {978-3-319-28927-4 978-3-319-28929-8},
  langid = {english},
  file = {/home/jeroen/zotero/storage/AVTJKWEE/Oliehoek and Amato - 2016 - A Concise Introduction to Decentralized POMDPs.pdf}
}

@misc{ortegaThompsonSamplingBayesian2010,
  title = {Thompson {{Sampling}} \& {{Bayesian Control Rule}}},
  author = {Ortega, P.A.},
  year = {2010}
}

@article{panSurveyTransferLearning2010,
  title = {A {{Survey}} on {{Transfer Learning}}},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  year = {2010},
  month = oct,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {22},
  number = {10},
  pages = {1345--1359},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2009.191},
  urldate = {2023-01-20},
  abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/TVZKQKGH/Pan and Yang - 2010 - A Survey on Transfer Learning.pdf}
}

@misc{pavseReducingSamplingError2020,
  title = {Reducing {{Sampling Error}} in {{Batch Temporal Difference Learning}}},
  author = {Pavse, Brahma and Durugkar, Ishan and Hanna, Josiah and Stone, Peter},
  year = {2020},
  month = aug,
  number = {arXiv:2008.06738},
  eprint = {2008.06738},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-12-08},
  abstract = {Temporal difference (TD) learning is one of the main foundations of modern reinforcement learning. This paper studies the use of TD(0), a canonical TD algorithm, to estimate the value function of a given policy from a batch of data. In this batch setting, we show that TD(0) may converge to an inaccurate value function because the update following an action is weighted according to the number of times that action occurred in the batch \textendash{} not the true probability of the action under the given policy. To address this limitation, we introduce policy sampling error corrected-TD(0) (PSEC-TD(0)). PSEC-TD(0) first estimates the empirical distribution of actions in each state in the batch and then uses importance sampling to correct for the mismatch between the empirical weighting and the correct weighting for updates following each action. We refine the concept of a certainty-equivalence estimate and argue that PSEC-TD(0) is a more data efficient estimator than TD(0) for a fixed batch of data. Finally, we conduct an empirical evaluation of PSEC-TD(0) on three batch value function learning tasks, with a hyperparameter sensitivity analysis, and show that PSEC-TD(0) produces value function estimates with lower mean squared error than TD(0).},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/LLG3CN5B/Pavse et al. - 2020 - Reducing Sampling Error in Batch Temporal Differen.pdf}
}

@book{pearlBookWhyNew2018,
  title = {The {{Book}} of {{Why}}: {{The New Science}} of {{Cause}} and {{Effect}}},
  shorttitle = {The {{Book}} of {{Why}}},
  author = {Pearl, Judea and Mackenzie, Dana},
  year = {2018},
  edition = {1st},
  publisher = {{Basic Books, Inc.}},
  address = {{USA}},
  abstract = {A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence"Correlation is not causation." This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality--the study of cause and effect--on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.},
  isbn = {978-0-465-09760-9}
}

@book{pinedoScheduling2016,
  title = {Scheduling},
  author = {Pinedo, Michael L.},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-26580-3},
  urldate = {2023-10-18},
  isbn = {978-3-319-26578-0 978-3-319-26580-3},
  langid = {english},
  keywords = {Applications of schedule,Decision-making,Deterministic models,Scheduling,Stochastic models},
  file = {/home/jeroen/zotero/storage/QZV8HWL5/Pinedo - 2016 - Scheduling.pdf}
}

@inproceedings{Pol2016CoordinatedDR,
  title = {Coordinated Deep Reinforcement Learners for Traffic Light Control},
  author = {{van der Pol}, Elise and Oliehoek, Frans A.},
  year = {2016},
  file = {/home/jeroen/zotero/storage/ASFV2PZ9/van der Pol and Oliehoek - 2016 - Coordinated deep reinforcement learners for traffi.pdf}
}

@misc{provodinEmpiricalEvaluationPosterior2022,
  title = {An {{Empirical Evaluation}} of {{Posterior Sampling}} for {{Constrained Reinforcement Learning}}},
  author = {Provodin, Danil and Gajane, Pratik and Pechenizkiy, Mykola and Kaptein, Maurits},
  year = {2022},
  month = sep,
  number = {arXiv:2209.03596},
  eprint = {2209.03596},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-12-08},
  abstract = {We study a posterior sampling approach to efficient exploration in constrained reinforcement learning. Alternatively to existing algorithms, we propose two simple algorithms that are more efficient statistically, simpler to implement and computationally cheaper. The first algorithm is based on a linear formulation of CMDP, and the second algorithm leverages the saddle-point formulation of CMDP. Our empirical results demonstrate that, despite its simplicity, posterior sampling achieves state-of-the-art performance and, in some cases, significantly outperforms optimistic algorithms.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeroen/zotero/storage/ZWRQ4UPN/Provodin et al. - 2022 - An Empirical Evaluation of Posterior Sampling for .pdf}
}

@misc{provodinImpactBatchLearning2021,
  title = {The {{Impact}} of {{Batch Learning}} in {{Stochastic Bandits}}},
  author = {Provodin, Danil and Gajane, Pratik and Pechenizkiy, Mykola and Kaptein, Maurits},
  year = {2021},
  month = nov,
  number = {arXiv:2111.02071},
  eprint = {2111.02071},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-12-08},
  abstract = {We consider a special case of bandit problems, namely batched bandits. Motivated by natural restrictions of recommender systems and e-commerce platforms, we assume that a learning agent observes responses batched in groups over a certain time period. Unlike previous work, we consider a more practically relevant batchcentric scenario of batch learning. We provide a policy-agnostic regret analysis and demonstrate upper and lower bounds for the regret of a candidate policy. Our main theoretical results show that the impact of batch learning can be measured in terms of online behavior. Finally, we demonstrate the consistency of theoretical results by conducting empirical experiments and reflect on the optimal batch size choice.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/KV2PV2UT/Provodin et al. - 2021 - The Impact of Batch Learning in Stochastic Bandits.pdf}
}

@misc{provodinImpactBatchLearning2021a,
  title = {The {{Impact}} of {{Batch Learning}} in {{Stochastic Bandits}}},
  author = {Provodin, Danil and Gajane, Pratik and Pechenizkiy, Mykola and Kaptein, Maurits},
  year = {2021},
  month = nov,
  number = {arXiv:2111.02071},
  eprint = {2111.02071},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-12-13},
  abstract = {We consider a special case of bandit problems, namely batched bandits. Motivated by natural restrictions of recommender systems and e-commerce platforms, we assume that a learning agent observes responses batched in groups over a certain time period. Unlike previous work, we consider a more practically relevant batchcentric scenario of batch learning. We provide a policy-agnostic regret analysis and demonstrate upper and lower bounds for the regret of a candidate policy. Our main theoretical results show that the impact of batch learning can be measured in terms of online behavior. Finally, we demonstrate the consistency of theoretical results by conducting empirical experiments and reflect on the optimal batch size choice.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/KAMGEHJ9/Provodin et al. - 2021 - The Impact of Batch Learning in Stochastic Bandits.pdf}
}

@misc{prudencioSurveyOfflineReinforcement2022,
  title = {A {{Survey}} on {{Offline Reinforcement Learning}}: {{Taxonomy}}, {{Review}}, and {{Open Problems}}},
  shorttitle = {A {{Survey}} on {{Offline Reinforcement Learning}}},
  author = {Prudencio, Rafael Figueiredo and Maximo, Marcos R. O. A. and Colombini, Esther Luna},
  year = {2022},
  month = mar,
  number = {arXiv:2203.01387},
  eprint = {2203.01387},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-12-08},
  abstract = {With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications such as education, healthcare, and robotics. In this work, we propose a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field, and a review of existing benchmarks' properties and shortcomings. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/P2YTCKFY/Prudencio et al. - 2022 - A Survey on Offline Reinforcement Learning Taxono.pdf}
}

@misc{quintanarPredictingVehiclesTrajectories2021,
  title = {Predicting {{Vehicles Trajectories}} in {{Urban Scenarios}} with {{Transformer Networks}} and {{Augmented Information}}},
  author = {Quintanar, A. and {Fern{\'a}ndez-Llorca}, D. and Parra, I. and Izquierdo, R. and Sotelo, M. A.},
  year = {2021},
  month = jun,
  number = {arXiv:2106.00559},
  eprint = {2106.00559},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-01-20},
  abstract = {Understanding the behavior of road users is of vital importance for the development of trajectory prediction systems. In this context, the latest advances have focused on recurrent structures, establishing the social interaction between the agents involved in the scene. More recently, simpler structures have also been introduced for predicting pedestrian trajectories, based on Transformer Networks, and using positional information [1]. They allow the individual modelling of each agent's trajectory separately without any complex interaction terms. Our model exploits these simple structures by adding augmented data (position and heading), and adapting their use to the problem of vehicle trajectory prediction in urban scenarios in prediction horizons up to 5 seconds. In addition, a cross-performance analysis is performed between different types of scenarios, including highways, intersections and roundabouts, using recent datasets (inD, rounD, highD and INTERACTION). Our model achieves state-of-the-art results and proves to be flexible and adaptable to different types of urban contexts.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/jeroen/zotero/storage/Q92ZVIAZ/Quintanar et al. - 2021 - Predicting Vehicles Trajectories in Urban Scenario.pdf}
}

@book{reckerScientificResearchInformation2013,
  title = {Scientific {{Research}} in {{Information Systems}}},
  author = {Recker, Jan},
  year = {2013},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-30048-6},
  urldate = {2022-12-08},
  isbn = {978-3-642-30047-9 978-3-642-30048-6},
  langid = {english},
  file = {/home/jeroen/zotero/storage/AKZWNSY2/Recker - 2013 - Scientific Research in Information Systems.pdf}
}

@article{richards1956a,
  title = {Shock Waves on the Highway},
  author = {Richards, P.I.},
  year = {1956},
  journal = {Operations Research},
  volume = {4},
  number = {1},
  pages = {42--51},
  langid = {english}
}

@phdthesis{sarrafzadehOfflinePolicysearchBayesian1990,
  title = {Offline {{Policy-search}} in {{Bayesian Reinforcement Learning}}},
  author = {Sarrafzadeh, M.},
  year = {1990},
  month = jun,
  urldate = {2022-12-08},
  langid = {english},
  file = {/home/jeroen/zotero/storage/ECQNPT29/Sarrafzadeh - 1990 - Department of electrical engineering and computer .pdf}
}

@inproceedings{sartorCombinatorialLearningTraffic2019,
  title = {Combinatorial {{Learning}} in {{Traffic Management}}},
  booktitle = {Machine {{Learning}}, {{Optimization}}, and {{Data Science}}},
  author = {Sartor, Giorgio and Mannino, Carlo and Bach, Lukas},
  editor = {Nicosia, Giuseppe and Pardalos, Panos and Umeton, Renato and Giuffrida, Giovanni and Sciacca, Vincenzo},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {384--395},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-37599-7_32},
  abstract = {We describe an exact combinatorial learning approach to solve dynamic job-shop scheduling problems arising in traffic management. When a set of vehicles has to be controlled in real-time, a new schedule must be computed whenever a deviation from the current plan is detected, or periodically after a short amount of time. This suggests that each two (or more) consecutive instances will be very similar. We exploit a recently introduced MILP formulation for job-shop scheduling (called path\&cycle) to develop an effective solution algorithm based on delayed row generation. In our re-optimization framework, the algorithm maintains a pool of combinatorial cuts separated during the solution of previous instances, and adapts them to warm start each new instance. In our experiments, this adaptive approach led to a 4-time average speedup over the static approach (where each instance is solved independently) for a critical application in air traffic management.},
  isbn = {978-3-030-37599-7},
  langid = {english},
  keywords = {Job-shop scheduling,Mixed Integer Linear Programming,Re-optimization},
  file = {/home/jeroen/zotero/storage/IKJPBJI2/Sartor et al. - 2019 - Combinatorial Learning in Traffic Management.pdf}
}

@inproceedings{shenFastMethodPrevent2017,
  title = {A {{Fast Method}} to {{Prevent Traffic Blockage}} by {{Signal Control Based}} on {{Reinforcement Learning}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Communication}} and {{Electronic Information Engineering}} ({{CEIE}} 2016)},
  author = {Shen, Mengjia},
  year = {2017},
  publisher = {{Atlantis Press}},
  address = {{Guangzhou, China}},
  doi = {10.2991/ceie-16.2017.36},
  urldate = {2023-01-20},
  isbn = {978-94-6252-312-8},
  langid = {english},
  file = {/home/jeroen/zotero/storage/2QU3WDTM/Shen - 2017 - A Fast Method to Prevent Traffic Blockage by Signa.pdf}
}

@article{shengrenOptimalEnergySystem2023,
  title = {Optimal Energy System Scheduling Using a Constraint-Aware Reinforcement Learning Algorithm},
  author = {Shengren, Hou and Vergara, Pedro P. and Salazar Duque, Edgar Mauricio and Palensky, Peter},
  year = {2023},
  month = oct,
  journal = {International Journal of Electrical Power \& Energy Systems},
  volume = {152},
  pages = {109230},
  issn = {01420615},
  doi = {10.1016/j.ijepes.2023.109230},
  urldate = {2023-09-27},
  abstract = {The massive integration of renewable-based distributed energy resources (DERs) inherently increases the energy system's complexity, especially when it comes to defining its operational schedule. Deep reinforcement learning (DRL) algorithms arise as a promising solution due to their data-driven and model-free features. However, current DRL algorithms fail to enforce rigorous operational constraints (e.g., power balance, ramping up or down constraints) limiting their implementation in real systems. To overcome this, in this paper, a DRL algorithm (namely MIP-DQN) is proposed, capable of strictly enforcing all operational constraints in the action space, ensuring the feasibility of the defined schedule in real-time operation. This is done by leveraging recent optimization advances for deep neural networks (DNNs) that allow their representation as a MIP formulation, enabling further consideration of any action space constraints. Comprehensive numerical simulations show that the proposed algorithm outperforms existing state-of-the-art DRL algorithms, obtaining a lower error when compared with the optimal global solution (upper boundary) obtained after solving a mathematical programming formulation with perfect forecast information; while strictly enforcing all operational constraints (even in unseen test days).},
  langid = {english},
  file = {/home/jeroen/zotero/storage/ZDQYZ7TW/Shengren et al. - 2023 - Optimal energy system scheduling using a constrain.pdf}
}

@inproceedings{shulinModelPredictiveControl2010,
  title = {Model {{Predictive Control}} for Urban Traffic Networks via {{MILP}}},
  booktitle = {Proceedings of the 2010 {{American Control Conference}}},
  author = {{Shu Lin} and De Schutter, B and {Yugeng Xi} and Hellendoorn, H},
  year = {2010},
  month = jun,
  pages = {2272--2277},
  publisher = {{IEEE}},
  address = {{Baltimore, MD}},
  doi = {10.1109/ACC.2010.5530534},
  urldate = {2023-09-14},
  abstract = {Model Predictive Control (MPC) is an advanced control strategy that can easily coordinate urban traffic networks. But, due to the nonlinearity of the traffic model, the optimization problem of the MPC controller will become intractable in practice when the scale of the controlled traffic network grows larger. To solve this problem, the nonlinear traffic model is reformulated into a model with only linear equations and inequalities. Mixed-Integer Linear Programming (MILP) algorithms can efficiently solve the reformulated optimization problem, and guarantee the global optimum at the same time. Moreover, the MILP optimization problem is further relaxed by model reduction and adding upper bound constraints.},
  isbn = {978-1-4244-7427-1 978-1-4244-7426-4 978-1-4244-7425-7},
  langid = {english},
  keywords = {MILP},
  file = {/home/jeroen/zotero/storage/BTB9ZU5I/Shu Lin et al. - 2010 - Model Predictive Control for urban traffic network.pdf}
}

@article{shyalikaReinforcementLearningDynamic2020,
  title = {Reinforcement {{Learning}} in {{Dynamic Task Scheduling}}: {{A Review}}},
  shorttitle = {Reinforcement {{Learning}} in {{Dynamic Task Scheduling}}},
  author = {Shyalika, Chathurangi and Silva, Thushari and Karunananda, Asoka},
  year = {2020},
  month = sep,
  journal = {SN Computer Science},
  volume = {1},
  number = {6},
  pages = {306},
  issn = {2661-8907},
  doi = {10.1007/s42979-020-00326-5},
  urldate = {2023-09-27},
  abstract = {Scheduling is assigning shared resources over time to efficiently complete the tasks over a given period of time. The term is applied separately for tasks and resources correspondingly in task scheduling and resource allocation. Scheduling is a popular topic in operational management and computer science. Effective schedules ensure system efficiency, effective decision making, minimize resource wastage and cost, and enhance overall productivity. It is generally a tedious task to choose the most accurate resources in performing work items and schedules in both computing and business process execution. Especially in real-world dynamic systems where multiple agents involve in scheduling various dynamic tasks is a challenging issue. Reinforcement Learning is an emergent technology which has been able to solve the problem of the optimal task and resource scheduling dynamically. This review paper is about a research study that focused on Reinforcement Learning techniques that have been used for dynamic task scheduling. The paper addresses the results of the study by means of the state-of-the-art on Reinforcement learning techniques used in dynamic task scheduling and a comparative review of those techniques.},
  langid = {english},
  keywords = {Dynamic,Environment uncertainty,Multi-agent,Reinforcement learning,Task scheduling},
  file = {/home/jeroen/zotero/storage/P83QVV5V/Shyalika et al. - 2020 - Reinforcement Learning in Dynamic Task Scheduling.pdf}
}

@article{silverMasteringGameGo2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature24270},
  urldate = {2023-01-10},
  langid = {english},
  file = {/home/jeroen/zotero/storage/EBUZTASC/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf}
}

@article{sommerBidirectionallyCoupledNetwork2011,
  title = {Bidirectionally {{Coupled Network}} and {{Road Traffic Simulation}} for {{Improved IVC Analysis}}},
  shorttitle = {Veins},
  author = {Sommer, C and German, R and Dressler, F},
  year = {2011},
  month = jan,
  journal = {IEEE Transactions on Mobile Computing},
  volume = {10},
  number = {1},
  pages = {3--15},
  issn = {1536-1233},
  doi = {10.1109/TMC.2010.133},
  urldate = {2023-01-20},
  file = {/home/jeroen/zotero/storage/M2QZETM7/Sommer et al. - 2011 - Bidirectionally Coupled Network and Road Traffic S.pdf}
}

@article{sorensenMetaheuristicsMetaphorExposed2015,
  title = {Metaheuristics\textemdash the Metaphor Exposed},
  author = {S{\"o}rensen, Kenneth},
  year = {2015},
  journal = {International Transactions in Operational Research},
  volume = {22},
  number = {1},
  pages = {3--18},
  issn = {1475-3995},
  doi = {10.1111/itor.12001},
  urldate = {2023-10-13},
  abstract = {In recent years, the field of combinatorial optimization has witnessed a true tsunami of ``novel'' metaheuristic methods, most of them based on a metaphor of some natural or man-made process. The behavior of virtually any species of insects, the flow of water, musicians playing together \textendash{} it seems that no idea is too far-fetched to serve as inspiration to launch yet another metaheuristic. In this paper, we will argue that this line of research is threatening to lead the area of metaheuristics away from scientific rigor. We will examine the historical context that gave rise to the increasing use of metaphors as inspiration and justification for the development of new methods, discuss the reasons for the vulnerability of the metaheuristics field to this line of research, and point out its fallacies. At the same time, truly innovative research of high quality is being performed as well. We conclude the paper by discussing some of the properties of this research and by pointing out some of the most promising research avenues for the field of metaheuristics.},
  copyright = {\textcopyright{} 2013 The Authors. International Transactions in Operational Research \textcopyright{} 2013 International Federation of Operational Research Societies Published by John Wiley \& Sons Ltd, 9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main St, Malden, MA02148, USA.},
  langid = {english},
  keywords = {combinatorial optimization,heuristics,metaheuristics,optimization},
  file = {/home/jeroen/zotero/storage/HCB6R6BK/2013 - Wayback Machine.pdf;/home/jeroen/zotero/storage/BIR54TBV/itor.html}
}

@book{stevanovicAdaptiveTrafficControl2010,
  title = {Adaptive {{Traffic Control Systems}}: {{Domestic}} and {{Foreign State}} of {{Practice}}},
  shorttitle = {Adaptive {{Traffic Control Systems}}},
  author = {Stevanovic, Aleksandar and {Transportation Research Board} and {National Cooperative Highway Research Program Synthesis Program} and {Transportation Research Board}},
  year = {2010},
  month = apr,
  pages = {14364},
  publisher = {{National Academies Press}},
  address = {{Washington, D.C.}},
  doi = {10.17226/14364},
  urldate = {2023-01-20},
  isbn = {978-0-309-28039-6},
  langid = {english},
  file = {/home/jeroen/zotero/storage/RU8A7VCB/Stevanovic et al. - 2010 - Adaptive Traffic Control Systems Domestic and For.pdf}
}

@misc{sumorl,
  title = {{{SUMO-RL}}},
  author = {Alegre, Lucas N.},
  year = {2019},
  publisher = {{GitHub}}
}

@article{suttonDynaIntegratedArchitecture1991,
  title = {Dyna, an Integrated Architecture for Learning, Planning, and Reacting},
  author = {Sutton, Richard S.},
  year = {1991},
  month = jul,
  journal = {ACM SIGART Bulletin},
  volume = {2},
  number = {4},
  pages = {160--163},
  issn = {0163-5719},
  doi = {10.1145/122344.122377},
  urldate = {2023-01-10},
  abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/ZUHCWX36/Sutton - 1991 - Dyna, an integrated architecture for learning, pla.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning},
  file = {/home/jeroen/zotero/storage/P6DD4LLQ/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf}
}

@article{sweetDoesTrafficCongestion2011,
  title = {Does {{Traffic Congestion Slow}} the {{Economy}}?},
  author = {Sweet, Matthias},
  year = {2011},
  month = nov,
  journal = {Journal of Planning Literature},
  volume = {26},
  number = {4},
  pages = {391--404},
  issn = {0885-4122, 1552-6593},
  doi = {10.1177/0885412211409754},
  urldate = {2023-01-10},
  abstract = {Does traffic congestion negatively impact the economic growth of metropolitan areas? This article reviews the findings of three research directions addressing this question. First, research on first-order impacts indicates that the economic value of congestion-induced travel delay is tenuous since travelers adapt. Second, research on second-order impacts suggests that congestion slows metropolitan growth, inhibits agglomeration economies, and shapes economic geographies. Third, research on public-sector congestion mitigation policies identifies significant fiscal burdens despite limited success at reducing congestion. In sum, research on individual, business, and public-sector responses to congestion demonstrate a shift from congestion mitigation toward adaptation.},
  langid = {english}
}

@book{szepesvariAlgorithmsReinforcementLearning2019,
  title = {Algorithms for {{Reinforcement Learning}}},
  author = {Szepesv{\'a}ri, Csaba},
  year = {2019},
  series = {Synthesis {{Lectures}} on {{Artificial Intelligence}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-01551-9},
  urldate = {2022-12-09},
  isbn = {978-3-031-00423-0 978-3-031-01551-9},
  langid = {english},
  file = {/home/jeroen/zotero/storage/UK3TFXTN/Szepesvri - 2019 - Algorithms for Reinforcement Learning.pdf}
}

@misc{tangReinforcementLearningInteger2020,
  title = {Reinforcement {{Learning}} for {{Integer Programming}}: {{Learning}} to {{Cut}}},
  shorttitle = {Reinforcement {{Learning}} for {{Integer Programming}}},
  author = {Tang, Yunhao and Agrawal, Shipra and Faenza, Yuri},
  year = {2020},
  month = jul,
  number = {arXiv:1906.04859},
  eprint = {1906.04859},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.04859},
  urldate = {2023-09-25},
  abstract = {Integer programming (IP) is a general optimization framework widely applicable to a variety of unstructured and structured problems arising in, e.g., scheduling, production planning, and graph optimization. As IP models many provably hard to solve problems, modern IP solvers rely on many heuristics. These heuristics are usually human-designed, and naturally prone to suboptimality. The goal of this work is to show that the performance of those solvers can be greatly enhanced using reinforcement learning (RL). In particular, we investigate a specific methodology for solving IPs, known as the Cutting Plane Method. This method is employed as a subroutine by all modern IP solvers. We present a deep RL formulation, network architecture, and algorithms for intelligent adaptive selection of cutting planes (aka cuts). Across a wide range of IP tasks, we show that the trained RL agent significantly outperforms human-designed heuristics, and effectively generalizes to 10X larger instances and across IP problem classes. The trained agent is also demonstrated to benefit the popular downstream application of cutting plane methods in Branch-and-Cut algorithm, which is the backbone of state-of-the-art commercial IP solvers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/ZYWC4E8Z/Tang et al. - 2020 - Reinforcement Learning for Integer Programming Le.pdf;/home/jeroen/zotero/storage/ZHQ44AZ7/1906.html}
}

@misc{tasselReinforcementLearningEnvironment2021,
  title = {A {{Reinforcement Learning Environment For Job-Shop Scheduling}}},
  author = {Tassel, Pierre and Gebser, Martin and Schekotihin, Konstantin},
  year = {2021},
  month = apr,
  number = {arXiv:2104.03760},
  eprint = {2104.03760},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-27},
  abstract = {Scheduling is a fundamental task occurring in various automated systems applications, e.g., optimal schedules for machines on a job shop allow for a reduction of production costs and waste. Nevertheless, finding such schedules is often intractable and cannot be achieved by Combinatorial Optimization Problem (COP) methods within a given time limit. Recent advances of Deep Reinforcement Learning (DRL) in learning complex behavior enable new COP application possibilities. This paper presents an efficient DRL environment for Job-Shop Scheduling -- an important problem in the field. Furthermore, we design a meaningful and compact state representation as well as a novel, simple dense reward function, closely related to the sparse make-span minimization criteria used by COP methods. We demonstrate that our approach significantly outperforms existing DRL methods on classic benchmark instances, coming close to state-of-the-art COP approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/jeroen/zotero/storage/SJQW33NS/Tassel et al. - 2021 - A Reinforcement Learning Environment For Job-Shop .pdf;/home/jeroen/zotero/storage/6JZN4CA5/2104.html}
}

@article{taylorTransferLearningReinforcement,
  title = {Transfer {{Learning}} for {{Reinforcement Learning Domains}}: {{A Survey}}},
  author = {Taylor, Matthew E and Stone, Peter},
  abstract = {The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/2P69864C/Taylor and Stone - Transfer Learning for Reinforcement Learning Domai.pdf}
}

@article{timmermanPlatoonFormingAlgorithms2021,
  title = {Platoon Forming Algorithms for Intelligent Street Intersections},
  author = {Timmerman, R. W. and Boon, M. A. A.},
  year = {2021},
  month = feb,
  journal = {Transportmetrica A: Transport Science},
  volume = {17},
  number = {3},
  pages = {278--307},
  issn = {2324-9935, 2324-9943},
  doi = {10.1080/23249935.2019.1692962},
  urldate = {2023-01-10},
  langid = {english},
  file = {/home/jeroen/zotero/storage/7A98IPE5/Timmerman and Boon - 2021 - Platoon forming algorithms for intelligent street .pdf}
}

@article{trecaGreenWaveCoordinRaetiionnforFcoermTernat,
  title = {Green {{Wave CoordinRaetiionnforFcoermTernat}}{{LceaSrignninagl Control Using Deep}}},
  author = {Tr{\'e}ca, Maxime and Zargayouna, Mahdi and Barth, Dominique and Garbiso, Julian},
  langid = {english},
  file = {/home/jeroen/zotero/storage/PDJIJI8C/Trca et al. - Green Wave CoordinRaetiionnforFcoermTernatLceaSri.pdf}
}

@article{vanleeuwaardenDelayAnalysisFixedCycle2006,
  title = {Delay {{Analysis}} for the {{Fixed-Cycle Traffic-Light Queue}}},
  author = {{van Leeuwaarden}, J. S. H.},
  year = {2006},
  month = may,
  journal = {Transportation Science},
  volume = {40},
  number = {2},
  pages = {189--199},
  issn = {0041-1655, 1526-5447},
  doi = {10.1287/trsc.1050.0125},
  urldate = {2023-01-10},
  abstract = {We consider the fixed-cycle traffic-light (FCTL) queue, where vehicles arrive at an intersection controlled by a traffic light and form a queue. The traffic-light signal alternates between green and red periods, and delayed vehicles are assumed to depart during the green period at equal time intervals.             Most of the research done on the FCTL queue assumes that the vehicles arrive at the intersection according to a Poisson process and focuses on deriving formulas for the mean queue length at the end of green periods and the mean delay. For a class of discrete arrival processes, including the Poisson process, we derive the probability generating function of both the queue length and delay, from which the whole queue length and delay distribution can be obtained. This allows for the evaluation of performance characteristics other than the mean, such as the variance and percentiles of the distribution.             We discuss the numerical procedures that are required to obtain the performance characteristics, and give several numerical examples.},
  langid = {english}
}

@misc{vanrielTransientBehaviorQueues2021,
  title = {Transient {{Behavior}} of {{Queues}} at {{Signalized Traffic Intersections}}},
  author = {{van Riel}, Jeroen},
  year = {2021},
  langid = {english},
  file = {/home/jeroen/zotero/storage/ZC2ENVNC/van Riel - Transient Behavior of Queues at Signalized Traffic.pdf}
}

@misc{vinyalsPointerNetworks2017,
  title = {Pointer {{Networks}}},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  year = {2017},
  month = jan,
  number = {arXiv:1506.03134},
  eprint = {1506.03134},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.03134},
  urldate = {2023-10-17},
  abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,deep learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/YP7JHMXP/Vinyals et al. - 2017 - Pointer Networks.pdf;/home/jeroen/zotero/storage/MS2A84PV/1506.html}
}

@article{vivekanandanReinforcementLearningApproach2023,
  title = {A {{Reinforcement Learning Approach}} for {{Scheduling Problems}} with {{Improved Generalization}} through {{Order Swapping}}},
  author = {Vivekanandan, Deepak and Wirth, Samuel and Karlbauer, Patrick and Klarmann, Noah},
  year = {2023},
  month = jun,
  journal = {Machine Learning and Knowledge Extraction},
  volume = {5},
  number = {2},
  pages = {418--430},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2504-4990},
  doi = {10.3390/make5020025},
  urldate = {2023-10-14},
  abstract = {The scheduling of production resources (such as associating jobs to machines) plays a vital role for the manufacturing industry not only for saving energy, but also for increasing the overall efficiency. Among the different job scheduling problems, the Job Shop Scheduling Problem (JSSP) is addressed in this work. JSSP falls into the category of NP-hard Combinatorial Optimization Problem (COP), in which solving the problem through exhaustive search becomes unfeasible. Simple heuristics such as First-In, First-Out, Largest Processing Time First and metaheuristics such as taboo search are often adopted to solve the problem by truncating the search space. The viability of the methods becomes inefficient for large problem sizes as it is either far from the optimum or time consuming. In recent years, the research towards using Deep Reinforcement Learning (DRL) to solve COPs has gained interest and has shown promising results in terms of solution quality and computational efficiency. In this work, we provide an novel approach to solve the JSSP examining the objectives generalization and solution effectiveness using DRL. In particular, we employ the Proximal Policy Optimization (PPO) algorithm that adopts the policy-gradient paradigm that is found to perform well in the constrained dispatching of jobs. We incorporated a new method called Order Swapping Mechanism (OSM) in the environment to achieve better generalized learning of the problem. The performance of the presented approach is analyzed in depth by using a set of available benchmark instances and comparing our results with the work of other groups.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {generalization,Industry 4.0,Job Shop Scheduling,Markov Decision Process,Production Scheduling,Reinforcement Learning},
  file = {/home/jeroen/zotero/storage/8WVQCT7T/Vivekanandan et al. - 2023 - A Reinforcement Learning Approach for Scheduling P.pdf}
}

@article{wageningen-kessels2015a,
  title = {Genealogy of Traffic Flow Models},
  author = {{Wageningen-Kessels}, F. and Lint, H. and Vuik, K. and Hoogendoorn, S.},
  year = {2015},
  journal = {EURO Journal on Transportation and Logistics},
  volume = {4},
  number = {4},
  pages = {445--473},
  langid = {english}
}

@techreport{websterTrafficSignalSettings1958,
  title = {Traffic Signal Settings},
  author = {Webster, F.V.},
  year = {1958}
}

@inproceedings{weiCoLightLearningNetworklevel2019,
  title = {{{CoLight}}: {{Learning Network-level Cooperation}} for {{Traffic Signal Control}}},
  shorttitle = {{{CoLight}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Wei, Hua and Xu, Nan and Zhang, Huichu and Zheng, Guanjie and Zang, Xinshi and Chen, Chacha and Zhang, Weinan and Zhu, Yanmin and Xu, Kai and Li, Zhenhui},
  year = {2019},
  month = nov,
  eprint = {1905.05717},
  primaryclass = {cs},
  pages = {1913--1922},
  doi = {10.1145/3357384.3357902},
  urldate = {2023-09-23},
  abstract = {Cooperation among the traffic signals enables vehicles to move through intersections more quickly. Conventional transportation approaches implement cooperation by pre-calculating the offsets between two intersections. Such pre-calculated offsets are not suitable for dynamic traffic environments. To enable cooperation of traffic signals, in this paper, we propose a model, CoLight, which uses graph attentional networks to facilitate communication. Specifically, for a target intersection in a network, CoLight can not only incorporate the temporal and spatial influences of neighboring intersections to the target intersection, but also build up index-free modeling of neighboring intersections. To the best of our knowledge, we are the first to use graph attentional networks in the setting of reinforcement learning for traffic signal control and to conduct experiments on the large-scale road network with hundreds of traffic signals. In experiments, we demonstrate that by learning the communication, the proposed model can achieve superior performance against the state-of-the-art methods.},
  archiveprefix = {arxiv},
  keywords = {68Txx,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/jeroen/zotero/storage/72C9U98C/Wei et al. - 2019 - CoLight Learning Network-level Cooperation for Tr.pdf;/home/jeroen/zotero/storage/Z6M9HYCP/1905.html}
}

@article{weiDeepReinforcementLearning2019,
  title = {Deep {{Reinforcement Learning}} for {{Traffic Signal Control}} along {{Arterials}}},
  author = {Wei, Hua and Chen, Chacha and Wu, Kan and Zheng, Guanjie and Yu, Zhengyao and Gayah, Vikash and Li, Zhenhui},
  year = {2019},
  abstract = {Arterial streets serve as the principal undertaker for urban mobility in a typical urban road network. In this paper, we propose a novel decentralized reinforcement learning method for multiintersection traffic signal control on arterial traffic, by applying reinforcement learning control agents in each intersection. While applying individual control to multi-intersection problems faces many challenges, two main adjustments are made to optimize the overall performance: 1) to provide simple yet novel contextual information to individual agents and 2) to train the RL agents in a transfer learning way. We test our method on synthetic data dataset and show that our proposed method outperforms the state-of-theart methods. We also interpret the policies learned by our method, which is the first time that the policy learned by the reinforcement learning control agents is interpreted using the traditional transportation coordination method on the arterial.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/46P599NU/Wei et al. - 2019 - Deep Reinforcement Learning for Traffic Signal Con.pdf}
}

@inproceedings{weiIntelliLightReinforcementLearning2018,
  title = {{{IntelliLight}}: {{A Reinforcement Learning Approach}} for {{Intelligent Traffic Light Control}}},
  shorttitle = {{{IntelliLight}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Wei, Hua and Zheng, Guanjie and Yao, Huaxiu and Li, Zhenhui},
  year = {2018},
  month = jul,
  pages = {2496--2505},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/3219819.3220096},
  urldate = {2023-09-23},
  isbn = {978-1-4503-5552-0},
  langid = {english},
  file = {/home/jeroen/zotero/storage/NYXVAYX8/Wei et al. - 2018 - IntelliLight A Reinforcement Learning Approach fo.pdf}
}

@inproceedings{weiPressLightLearningMax2019,
  title = {{{PressLight}}: {{Learning Max Pressure Control}} to {{Coordinate Traffic Signals}} in {{Arterial Network}}},
  shorttitle = {{{PressLight}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Wei, Hua and Chen, Chacha and Zheng, Guanjie and Wu, Kan and Gayah, Vikash and Xu, Kai and Li, Zhenhui},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {1290--1298},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3292500.3330949},
  urldate = {2023-09-25},
  abstract = {Traffic signal control is essential for transportation efficiency in road networks. It has been a challenging problem because of the complexity in traffic dynamics. Conventional transportation research suffers from the incompetency to adapt to dynamic traffic situations. Recent studies propose to use reinforcement learning (RL) to search for more efficient traffic signal plans. However, most existing RL-based studies design the key elements - reward and state - in a heuristic way. This results in highly sensitive performances and a long learning process. To avoid the heuristic design of RL elements, we propose to connect RL with recent studies in transportation research. Our method is inspired by the state-of-the-art method max pressure (MP) in the transportation field. The reward design of our method is well supported by the theory in MP, which can be proved to be maximizing the throughput of the traffic network, i.e., minimizing the overall network travel time. We also show that our concise state representation can fully support the optimization of the proposed reward function. Through comprehensive experiments, we demonstrate that our method outperforms both conventional transportation approaches and existing learning-based methods.},
  isbn = {978-1-4503-6201-6},
  keywords = {deep reinforcement learning,multi-agent system,traffic signal control},
  file = {/home/jeroen/zotero/storage/RV86D9SB/Wei et al. - 2019 - PressLight Learning Max Pressure Control to Coord.pdf}
}

@article{weisbrodMeasuringEconomicCosts2003,
  title = {Measuring {{Economic Costs}} of {{Urban Traffic Congestion}} to {{Business}}},
  author = {Weisbrod, Glen and Vary, Don and Treyz, George},
  year = {2003},
  month = jan,
  journal = {Transportation Research Record: Journal of the Transportation Research Board},
  volume = {1839},
  number = {1},
  pages = {98--106},
  issn = {0361-1981, 2169-4052},
  doi = {10.3141/1839-10},
  urldate = {2023-01-10},
  abstract = {Key findings are provided from NCHRP Study 2-21, which examined how urban traffic congestion imposes economic costs within metropolitan areas. Specifically, the study applied data from Chicago and Philadelphia to examine how various producers of economic goods and services are sensitive to congestion, through its impact on business costs, productivity, and output levels. The data analysis showed that sensitivity to traffic congestion varies by industry sector and is attributable to differences in each industry sector's mix of required inputs and hence its reliance on access to skilled labor, access to specialized inputs, and access to a large, transportation-based market area. Statistical analysis models were applied with the local data to demonstrate how congestion effectively shrinks business market areas and reduces the "agglomeration economies" of businesses operating in large urban areas, thus raising production costs. Overall, this research illustrates how it is possible to estimate the economic implications of congestion, an approach that may be applied in the future for benefit-cost analysis of urban congestion-reduction strategies or for development of congestion pricing strategies. The analysis also shows how congestion-reduction strategies can induce additional traffic as a result of economic benefits.},
  langid = {english}
}

@misc{weiSurveyTrafficSignal2020,
  title = {A {{Survey}} on {{Traffic Signal Control Methods}}},
  author = {Wei, Hua and Zheng, Guanjie and Gayah, Vikash and Li, Zhenhui},
  year = {2020},
  month = jan,
  number = {arXiv:1904.08117},
  eprint = {1904.08117},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-01-10},
  abstract = {Traffic signal control is an important and challenging real-world problem, which aims to minimize the travel time of vehicles by coordinating their movements at the road intersections. Current traffic signal control systems in use still rely heavily on oversimplified information and rule-based methods, although we now have richer data, more computing power and advanced methods to drive the development of intelligent transportation. With the growing interest in intelligent transportation using machine learning methods like reinforcement learning, this survey covers the widely acknowledged transportation approaches and a comprehensive list of recent literature on reinforcement for traffic signal control. We hope this survey can foster interdisciplinary research on this important topic.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {68Txx,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/8NW67ZJX/Wei et al. - 2020 - A Survey on Traffic Signal Control Methods.pdf}
}

@article{xuLeveragingTransformerModel2022,
  title = {Leveraging {{Transformer Model}} to {{Predict Vehicle Trajectories}} in {{Congested Urban Traffic}}},
  author = {Xu, Yufei and Wang, Yu and Peeta, Srinivas},
  year = {2022},
  month = aug,
  journal = {Transportation Research Record: Journal of the Transportation Research Board},
  pages = {036119812211095},
  issn = {0361-1981, 2169-4052},
  doi = {10.1177/03611981221109594},
  urldate = {2023-01-20},
  abstract = {Accurate vehicle trajectory prediction enables safe, comfortable, and optimal proactive motion planning for connected and autonomous vehicles (CAVs). Because of rapid advances in learning techniques and increasing access to massive amounts of data, deep learning techniques have been applied to predict vehicle trajectories, especially the long short-term memory (LSTM) model. However, the accurate prediction of vehicle trajectories for congested urban traffic remains problematic, as existing LSTM models do not perform well. To address this gap, this paper proposes to leverage an emerging deep learning technique\textemdash transformer\textemdash and utilizes a recently released dataset (pNEUMA) for predicting vehicle trajectories in congested urban traffic. The proposed transformer model uses the self-attention mechanism, which helps to identify dependencies within the model inputs, to systematically determine the impacts of vehicular interactions on the target vehicle's future trajectory. The pNEUMA dataset, which provides drone-based large-scale data of congested urban traffic, is processed to fit a typical trajectory prediction scenario, and used to train the transformer model. Numerical studies are conducted to analyze the effectiveness of the proposed modeling approach. A comparison of the proposed model with representative LSTM models highlights the advantages of leveraging the transformer model characteristics for the vehicle trajectory prediction of congested urban traffic. By contrast, existing LSTM models may suffice for the trajectory prediction of freeway traffic. The results also indicate that, unlike for vehicle trajectory prediction for freeway traffic, a longer time window of inputs does not guarantee better prediction performance for congested urban traffic.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/73KTKYFW/Xu et al. - 2022 - Leveraging Transformer Model to Predict Vehicle Tr.pdf}
}

@article{yauSurveyReinforcementLearning2018,
  title = {A {{Survey}} on {{Reinforcement Learning Models}} and {{Algorithms}} for {{Traffic Signal Control}}},
  author = {Yau, Kok-Lim Alvin and Qadir, Junaid and Khoo, Hooi Ling and Ling, Mee Hong and Komisarczuk, Peter},
  year = {2018},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {50},
  number = {3},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3068287},
  urldate = {2023-09-26},
  abstract = {Traffic congestion has become a vexing and complex issue in many urban areas. Of particular interest are the intersections where traffic bottlenecks are known to occur despite being traditionally signalized. Reinforcement learning (RL), which is an artificial intelligence approach, has been adopted in traffic signal control for monitoring and ameliorating traffic congestion. RL enables autonomous decision makers (e.g., traffic signal controllers) to observe, learn, and select the optimal action (e.g., determining the appropriate traffic phase and its timing) to manage traffic such that system performance is improved. This article reviews various RL models and algorithms applied to traffic signal control in the aspects of the representations of the RL model (i.e., state, action, and reward), performance measures, and complexity to establish a foundation for further investigation in this research field. Open issues are presented toward the end of this article to discover new research areas with the objective to spark new interest in this research field.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/4UJSP6QX/Yau et al. - 2018 - A Survey on Reinforcement Learning Models and Algo.pdf}
}

@inproceedings{zhangCityFlowMultiAgentReinforcement2019,
  title = {{{CityFlow}}: {{A Multi-Agent Reinforcement Learning Environment}} for {{Large Scale City Traffic Scenario}}},
  shorttitle = {{{CityFlow}}},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Zhang, Huichu and Feng, Siyuan and Liu, Chang and Ding, Yaoyao and Zhu, Yichen and Zhou, Zihan and Zhang, Weinan and Yu, Yong and Jin, Haiming and Li, Zhenhui},
  year = {2019},
  month = may,
  eprint = {1905.05217},
  primaryclass = {cs},
  pages = {3620--3624},
  doi = {10.1145/3308558.3314139},
  urldate = {2023-09-25},
  abstract = {Traffic signal control is an emerging application scenario for reinforcement learning. Besides being as an important problem that affects people's daily life in commuting, traffic signal control poses its unique challenges for reinforcement learning in terms of adapting to dynamic traffic environment and coordinating thousands of agents including vehicles and pedestrians. A key factor in the success of modern reinforcement learning relies on a good simulator to generate a large number of data samples for learning. The most commonly used open-source traffic simulator SUMO is, however, not scalable to large road network and large traffic flow, which hinders the study of reinforcement learning on traffic scenarios. This motivates us to create a new traffic simulator CityFlow with fundamentally optimized data structures and efficient algorithms. CityFlow can support flexible definitions for road network and traffic flow based on synthetic and real-world data. It also provides user-friendly interface for reinforcement learning. Most importantly, CityFlow is more than twenty times faster than SUMO and is capable of supporting city-wide traffic simulation with an interactive render for monitoring. Besides traffic signal control, CityFlow could serve as the base for other transportation studies and can create new possibilities to test machine learning methods in the intelligent transportation domain.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/jeroen/zotero/storage/L9EDGC59/Zhang et al. - 2019 - CityFlow A Multi-Agent Reinforcement Learning Env.pdf;/home/jeroen/zotero/storage/VGJXZ99T/1905.html}
}

@misc{zhangLearningDispatchJob2020,
  title = {Learning to {{Dispatch}} for {{Job Shop Scheduling}} via {{Deep Reinforcement Learning}}},
  author = {Zhang, Cong and Song, Wen and Cao, Zhiguang and Zhang, Jie and Tan, Puay Siew and Xu, Chi},
  year = {2020},
  month = oct,
  number = {arXiv:2010.12367},
  eprint = {2010.12367},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.12367},
  urldate = {2023-09-27},
  abstract = {Priority dispatching rule (PDR) is widely used for solving real-world Job-shop scheduling problem (JSSP). However, the design of effective PDRs is a tedious task, requiring a myriad of specialized knowledge and often delivering limited performance. In this paper, we propose to automatically learn PDRs via an end-to-end deep reinforcement learning agent. We exploit the disjunctive graph representation of JSSP, and propose a Graph Neural Network based scheme to embed the states encountered during solving. The resulting policy network is size-agnostic, effectively enabling generalization on large-scale instances. Experiments show that the agent can learn high-quality PDRs from scratch with elementary raw features, and demonstrates strong performance against the best existing PDRs. The learned policies also perform well on much larger instances that are unseen in training.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/X9IDBWNN/Zhang et al. - 2020 - Learning to Dispatch for Job Shop Scheduling via D.pdf;/home/jeroen/zotero/storage/DIPBMFPB/2010.html}
}

@misc{zhengConstrainedUpperConfidence2020,
  title = {Constrained {{Upper Confidence Reinforcement Learning}}},
  author = {Zheng, Liyuan and Ratliff, Lillian J.},
  year = {2020},
  month = jan,
  number = {arXiv:2001.09377},
  eprint = {2001.09377},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-12-13},
  abstract = {Constrained Markov Decision Processes are a class of stochastic decision problems in which the decision maker must select a policy that satisfies auxiliary cost constraints. This paper extends upper confidence reinforcement learning for settings in which the reward function and the constraints, described by cost functions, are unknown a priori but the transition kernel is known. Such a setting is well-motivated by a number of applications including exploration of unknown, potentially unsafe, environments. We present an algorithm C-UCRL and show that it achieves sub-linear regret (\$ O(T\^\{\textbackslash frac\{3\}\{4\}\}\textbackslash sqrt\{\textbackslash log(T/\textbackslash delta)\})\$) with respect to the reward while satisfying the constraints even while learning with probability \$1-\textbackslash delta\$. Illustrative examples are provided.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeroen/zotero/storage/WJGU7JUL/Zheng and Ratliff - 2020 - Constrained Upper Confidence Reinforcement Learnin.pdf}
}

@article{zhengReinforcementLearningBased2022,
  title = {A {{Reinforcement Learning Based Traffic Control Strategy}} in a {{Macroscopic Fundamental Diagram Region}}},
  author = {Zheng, Lingyu and Wu, Bing},
  editor = {Jin, Peter J.},
  year = {2022},
  month = apr,
  journal = {Journal of Advanced Transportation},
  volume = {2022},
  pages = {1--12},
  issn = {2042-3195, 0197-6729},
  doi = {10.1155/2022/5681234},
  urldate = {2023-01-20},
  abstract = {Urban traffic control systems (UTCSs) are deployed to a great number of urban cities despite lacking feedback when adjusting the traffic signals. The development of reinforcement learning (RL) makes it possible to apply feedback to UTCS, and great efforts have been made on RL-based traffic control strategies. However, those studies are regardless of the traffic flow theory of the network and the road users' perspectives on the performance of traffic. This study proposes a multiagent reinforcement learning (MARL) based traffic control strategy, in which each intersection in a macroscopic fundamental diagram (MFD) region was controlled by one agent using the level of services (LOS) and MFD-based parameters as rewards. The proposed MARL strategy was evaluated by simulation in a 3\texttimes 3 grid network compared with pretimed, actuated, and MFD-based traffic control strategies. The evaluation results showed that, at different demand levels, the proposed MARL strategy outperforms the other three traffic control strategies in terms of average intersection queue length and average intersection waiting time to a different extent. Results also showed that the proposed MARL dissipated the congestion faster than the other three control strategies. Results of the Friedman test indicated that the differences in performances between the proposed MARL and other strategies were statistically significant regardless of the demand level. The MFD in the testbed network controlled by the proposed MARL was different from that controlled by the pretimed strategy, especially the MFD scatter plot. It provides insights on considering the traffic flow theory of the network when applying MARL to traffic control strategies.},
  langid = {english},
  file = {/home/jeroen/zotero/storage/2NVJYMTI/Zheng and Wu - 2022 - A Reinforcement Learning Based Traffic Control Str.pdf}
}

@misc{zotero-111,
  type = {Misc}
}
